{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main_new.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c5wyNEJgKRb",
        "outputId": "84b1b1f7-7cf5-4a76-ecb1-0e076ebcd2fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_nWU85wg69-",
        "outputId": "c6990c0c-979d-4426-b05c-ff3d645eb9e8"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/11785/recitation08part02"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/11785/recitation08part02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI7141U9mSJS",
        "outputId": "74a7b047-bb18-4693-b11f-a45b29037bf2"
      },
      "source": [
        "! pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.6/dist-packages (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnfGxMuwpj3A"
      },
      "source": [
        "! pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMrRwUs3qTez",
        "outputId": "a6655f54-add8-475a-8077-38b8efcf599d"
      },
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suKjOXruqbQH"
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVhL2r_2q2JR"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddtv9Hi-q6zp"
      },
      "source": [
        "! kaggle competitions download -c 11-785-fall-20-homework-4-part-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrwPcKHFHP1I",
        "outputId": "34527140-d3a3-40a7-f3b8-1fd5ac3a9d06"
      },
      "source": [
        "! git clone https://github.com/salesforce/awd-lstm-lm.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'awd-lstm-lm'...\n",
            "remote: Enumerating objects: 137, done.\u001b[K\n",
            "remote: Total 137 (delta 0), reused 0 (delta 0), pack-reused 137\u001b[K\n",
            "Receiving objects: 100% (137/137), 57.62 KiB | 366.00 KiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFGLOa4rHkJ6"
      },
      "source": [
        "! cp /content/awd-lstm-lm/weight_drop.py /content/gdrive/MyDrive/11785/recitation08part02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyIeTExxvO-d"
      },
      "source": [
        "# dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQqJW35tvNdi"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset \n",
        "from torch.nn.utils.rnn import *\n",
        "\n",
        "'''\n",
        "Loading all the numpy files containing the utterance information and text information\n",
        "'''\n",
        "def load_data(): \n",
        "    speech_train = np.load('./data/train.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_valid = np.load('./data/dev.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_test = np.load('./data/test.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "    transcript_train = np.load('./data/train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "    transcript_valid = np.load('./data/dev_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "\n",
        "    return speech_train, speech_valid, speech_test, transcript_train, transcript_valid\n",
        "\n",
        "\n",
        "'''\n",
        "Transforms alphabetical input to numerical input, replace each letter by its corresponding \n",
        "index from letter_list\n",
        "'''\n",
        "def transform_letter_to_index(transcript, letter_list):\n",
        "    '''\n",
        "    :param transcript :(N, ) Transcripts are the text input\n",
        "    :param letter_list: Letter list defined above\n",
        "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
        "    '''\n",
        "    letter_to_index = []\n",
        "    for sentence in transcript:\n",
        "        sent=\"\"\n",
        "        for word in sentence:\n",
        "          word = word.decode()\n",
        "          sent+=(word + ' ')\n",
        "        sent = sent[:-1]\n",
        "        lst = []\n",
        "        lst.append(letter_list.index('<sos>'))\n",
        "        for c in sent:\n",
        "          lst.append(letter_list.index(c))\n",
        "        lst.append(letter_list.index('<eos>')) \n",
        "        letter_to_index.append(lst)\n",
        "    return letter_to_index\n",
        "\n",
        "\n",
        "'''\n",
        "Optional, create dictionaries for letter2index and index2letter transformations\n",
        "'''\n",
        "def create_dictionaries(letter_list):\n",
        "    letter2index = dict()\n",
        "    index2letter = dict()\n",
        "    return letter2index, index2letter\n",
        "\n",
        "\n",
        "class Speech2TextDataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for the speech to text data, this may need some tweaking in the\n",
        "    getitem method as your implementation in the collate function may be different from\n",
        "    ours. \n",
        "    '''\n",
        "    def __init__(self, speech, text=None, isTrain=True):\n",
        "        self.speech = speech\n",
        "        self.isTrain = isTrain\n",
        "        if (text is not None):\n",
        "            self.text = text\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.speech.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if (self.isTrain == True):\n",
        "            return torch.tensor(self.speech[index].astype(np.float32)), torch.tensor(self.text[index])\n",
        "        else:\n",
        "            return torch.tensor(self.speech[index].astype(np.float32))\n",
        "\n",
        "\n",
        "def collate_train(batch):\n",
        "    ### Return the padded speech and text data, and the length of utterance and transcript ###\n",
        "    # pass \n",
        "    #print('hi')\n",
        "    #print(len(batch))\n",
        "    data = [torch.Tensor(item[0]) for item in batch]  # just form a list of tensor\n",
        "    lengths_x = torch.tensor([ t.shape[0] for t in data ]) # torch.tensor(\n",
        "    #print('lenx',lengths_x)\n",
        "    target = [item[1][1:] for item in batch] #torch.Tensor(\n",
        "    lengths_y = torch.tensor([ t.shape[0] for t in target ])\n",
        "    #print('leny',lengths_y)\n",
        "    data = pad_sequence(data,batch_first=True) # CCCCHHEEECCKKK BATCH FIRST!!!\n",
        "    target = pad_sequence(target,batch_first=True)\n",
        "    #target = torch.LongTensor(target)\n",
        "    #X_lens = torch.LongTensor([len(seq) for seq in data])\n",
        "    #Y_lens = torch.LongTensor([len(seq) for seq in target])\n",
        "    \n",
        "    return data, target.long(), lengths_x.long(), lengths_y.long() # X_lens, Y_lens\n",
        "\n",
        "\n",
        "def collate_test(batch_data):\n",
        "    ### Return padded speech and length of utterance ###\n",
        "    pass "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iUDFZmtzmmb"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYKhJQsFzl2R"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as utils\n",
        "from embed_regularize import embedded_dropout\n",
        "from locked_dropout import LockedDropout\n",
        "from weight_drop import WeightDrop\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key, value and query from Encoder and decoder.\n",
        "    Below are the set of operations you need to perform for computing attention:\n",
        "        energy = bmm(key, query)\n",
        "        attention = softmax(energy)\n",
        "        context = bmm(attention, value)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, query, key, value, lens):\n",
        "        '''\n",
        "        :param query :(batch_size, hidden_size) Query is the output of LSTMCell from Decoder\n",
        "        :param keys: (batch_size, max_len, encoder_size) Key Projection from Encoder\n",
        "        :param values: (batch_size, max_len, encoder_size) Value Projection from Encoder\n",
        "        :return context: (batch_size, encoder_size) Attended Context\n",
        "        :return attention_mask: (batch_size, max_len) Attention mask that can be plotted  \n",
        "        : lens - (batch_size,)\n",
        "        '''\n",
        "        # Compute (batch_size, max_len) attention logits. \"bmm\" stands for \"batch matrix multiplication\".\n",
        "        # Input shape of bmm:  (batch_szie, max_len, hidden_size), (batch_size, hidden_size, 1) \n",
        "        # Output shape of bmm: (batch_size, max_len, 1)\n",
        "\n",
        "        #ENERGY = (batch_size x maxlen x encoder_size) x (batch_size, hidden_size , 1)\n",
        "\n",
        "        attention = torch.bmm(key, query.unsqueeze(2)).squeeze(2)        # batch_size x maxlen\n",
        "        \n",
        "        # Create an (batch_size, max_len) boolean mask for all padding positions\n",
        "        # Make use of broadcasting: (1, max_len), (batch_size, 1) -> (batch_size, max_len)\n",
        "        mask = torch.arange(key.size(1)).unsqueeze(0) >= lens.unsqueeze(1)  #(batch_size, max_len) where lens are original seq lens\n",
        "        mask = mask.to(DEVICE)\n",
        "        # Set attention logits at padding positions to negative infinity.\n",
        "        attention.masked_fill_(mask, -1e9)                  ##(batch_size, max_len)\n",
        "        \n",
        "        # Take softmax over the \"source length\" dimension.\n",
        "        attention = nn.functional.softmax(attention, dim=1) #.to(DEVICE)       # batch_size x maxlen\n",
        "        # context = torch.bmm(attention, value.unsqueeze(2)).squeeze(2) ### CHECK!\n",
        "        # Compute attention-weighted sum of context vectors\n",
        "        # Input shape of bmm: (batch_size, 1, max_len), (batch_size, max_len, hidden_size) \n",
        "        # Output shape of bmm: (batch_size, 1, hidden_size)\n",
        "        context = torch.bmm(attention.unsqueeze(1), value).squeeze(1)    ## batch_size x encoder_size\n",
        "        \n",
        "        # attention vectors are returned for visualization\n",
        "        return  context, attention         #out, attention\n",
        "\n",
        "\n",
        "class pBLSTM(nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
        "    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n",
        "    and inferior results even after extensive training.\n",
        "    The major reason is inability of AttendAndSpell operation to extract relevant information\n",
        "    from a large number of input steps.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        # CCHHEECCKKK BATCH FIRST !!!!!!!\n",
        "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True,batch_first=True)\n",
        "        self.blstm = WeightDrop(self.blstm, ['weight_hh_l0'], dropout= 0.2)\n",
        "        self.lockdrop = LockedDropout()\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x :(N, T) input to the pBLSTM\n",
        "        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM \n",
        "        '''\n",
        "        \n",
        "        x, lens_unpacked = utils.rnn.pad_packed_sequence(x,batch_first= True)\n",
        "        x = self.lockdrop(x,dropout=0.2)\n",
        "        N = x.size(0)\n",
        "        T = x.size(1)\n",
        "        feat = x.size(2)\n",
        "        if T%2 != 0:\n",
        "          x = x[:,:-1,:]\n",
        "        '''x = x.permute(1,0,2)\n",
        "        x = x.reshape(N,int(T/2),feat*2)\n",
        "        X = x.permute(1,0,2)'''\n",
        "        #x = x.contiguous().reshape(N,int(T//2),feat*2)\n",
        "        x = x.contiguous().view(N,int(T//2),feat*2)\n",
        "        input_x = utils.rnn.pack_padded_sequence(x, lengths=lens_unpacked//2, batch_first= True, enforce_sorted=False)\n",
        "        \n",
        "        output,hidden = self.blstm(input_x)\n",
        "        return output,hidden\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key and value.\n",
        "    Key and value are nothing but simple projections of the output from pBLSTM network.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, value_size=256,key_size=256):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True).cuda()\n",
        "        self.lstm = WeightDrop(self.lstm, ['weight_hh_l0'], dropout= 0.2)\n",
        "        self.lockdrop = LockedDropout()\n",
        "        ### Add code to define the blocks of pBLSTMs! ###\n",
        "\n",
        "        self.pLSTM_layer1 = pBLSTM(hidden_dim*4,hidden_dim).cuda()\n",
        "        self.pLSTM_layer2 = pBLSTM(hidden_dim*4,hidden_dim).cuda() \n",
        "        self.pLSTM_layer3 = pBLSTM(hidden_dim*4,hidden_dim).cuda()\n",
        "\n",
        "        self.key_network = nn.Linear(hidden_dim*2, value_size).cuda()\n",
        "        self.value_network = nn.Linear(hidden_dim*2, key_size).cuda()\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "      # BATCH FIRST TRUE !!!!\n",
        "        rnn_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first= True, enforce_sorted=False)\n",
        "        outputs, _ = self.lstm(rnn_inp)\n",
        "\n",
        "        ### Use the outputs and pass it through the pBLSTM blocks! ###\n",
        "\n",
        "        output, _ = self.pLSTM_layer1(outputs)\n",
        "        output, _ = self.pLSTM_layer2(output)\n",
        "        outputs, _ = self.pLSTM_layer3(output)\n",
        "\n",
        "        # BATCH FIRST TRUE !!!!\n",
        "        linear_input, lengths = utils.rnn.pad_packed_sequence(outputs,batch_first= True)\n",
        "        linear_input = self.lockdrop(linear_input,dropout=0.2)\n",
        "        keys = self.key_network(linear_input)\n",
        "        value = self.value_network(linear_input)\n",
        "\n",
        "        return keys, value, lengths\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, \n",
        "    thus we use LSTMCell instead of LSLTM here.\n",
        "    The output from the second LSTMCell can be used as query here for attention module.\n",
        "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, hidden_dim, value_size=256, key_size=256, isAttended=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
        "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
        "#embedded_dropout\n",
        "        self.isAttended = isAttended\n",
        "        if (isAttended == True):\n",
        "            self.attention = Attention()\n",
        "# HHHARDDDD CCCOOODDDDDEEEDDDD\n",
        "        self.character_prob = nn.Linear(256, vocab_size)\n",
        "        self.character_prob.weight = self.embedding.weight\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        #self.fc2 = nn.Linear(256, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(256, 256)\n",
        "        #self.fc4 = nn.Linear(config.vocab_size + config.vocab_size, config.vocab_size)\n",
        "        self.init_weights()\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.character_prob.bias.data.zero_()\n",
        "        self.character_prob.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, epoch, key, values, lens, text=None, isTrain=True):\n",
        "        '''\n",
        "        :param key :(T, N, key_size) Output of the Encoder Key projection layer.  (Batch x max len. x encoder_size)\n",
        "        :param values: (T, N, value_size) Output of the Encoder Value projection layer.  (Batch x max len x encoder_size)\n",
        "        :param text: (N, text_len) Batch input of text with text_length.  (batch_size x text_len). \n",
        "        :param isTrain: Train or eval mode\n",
        "        :return predictions: Returns the character perdiction probability \n",
        "        '''\n",
        "        num_comb = 100\n",
        "        batch_size = key.shape[0]     #key.shape[1]\n",
        "        sos = torch.Tensor(np.array([33]*batch_size)).unsqueeze(1)\n",
        "        sos = sos.to(DEVICE).long()\n",
        "        text = torch.cat((sos,text),1)\n",
        "        text = text.to(DEVICE)\n",
        "        if (isTrain == True):\n",
        "            max_len =  text.shape[1]\n",
        "            embeddings = self.embedding(text) # y values\n",
        "        else:\n",
        "            max_len = 600 \n",
        "\n",
        "        predictions = []\n",
        "        hidden_states = [None, None]\n",
        "        hid_dim = values.size(2) # same as encoder_size\n",
        "        context = values[:,0,:].reshape(values.size(0),values.size(2)) # initialise context with 0. torch.zeros(batch_size, hid_dim).to(DEVICE)\n",
        "        prediction = (torch.ones(batch_size, 1)*33).to(DEVICE)  #.  torch.zeros(batch_size,1).to(DEVICE)\n",
        "        all_attentions = []\n",
        "        if epoch < 5:\n",
        "            tf_rate = 0.9\n",
        "        elif epoch >=5 and epoch < 10:\n",
        "            tf_rate = 0.85\n",
        "        elif epoch >=10 and epoch < 15:\n",
        "            tf_rate = 0.75\n",
        "        elif epoch >=15 and epoch < 20:\n",
        "            tf_rate = 0.7\n",
        "        elif epoch >=20 and epoch < 25:\n",
        "            tf_rate = 0.65\n",
        "        elif epoch >=25 and epoch < 30:\n",
        "            tf_rate = 0.6\n",
        "        else:\n",
        "            tf_rate = 0.55\n",
        "        for i in range(max_len-1): \n",
        "            # * Implement Gumble noise and teacher forcing techniques \n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
        "            # * If you haven't implemented attention yet, then you may want to check the index and break \n",
        "            #   out of the loop so you do not get index out of range errors. \n",
        "            \n",
        "            if (isTrain):\n",
        "                # teacher forcing\n",
        "                p = np.random.random()\n",
        "                if p < tf_rate:\n",
        "                    char_embed = embeddings[:,i,:]    \n",
        "                else: \n",
        "                    char_embed = self.embedding(prediction.argmax(dim=-1))  \n",
        "            else:\n",
        "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "\n",
        "            inp = torch.cat([char_embed, context], dim=1) # inp = batch x (hidden dim + hidden dim)\n",
        "            hidden_states[0] = self.lstm1(inp, hidden_states[0]) # h = batch x hidden & c = batch x hidden\n",
        "\n",
        "            inp_2 = hidden_states[0][0]\n",
        "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1]) \n",
        "\n",
        "            ### Compute attention from the output of the second LSTM Cell ###\n",
        "            output = hidden_states[1][0]    # query. output = batch x hidden\n",
        "            context,attention = self.attention(output,key, values,lens) #batch_size x encoder_size\n",
        "            all_attentions.append(attention.detach())\n",
        "            fc_output = self.relu1(self.bn1(self.fc1(output)))\n",
        "            context_out = self.relu2(self.bn2(self.fc3(context)))\n",
        "            context_out = context_out.to(DEVICE)\n",
        "            #print('shape check',torch.cat([fc_output, context_out], dim=1).shape)\n",
        "            prediction = self.character_prob(torch.cat([fc_output, context_out], dim=1)) # s, c # batch x (enc_size + hidden) pred = b x vocab\n",
        "            predictions.append(prediction.unsqueeze(1)) # b x 1 x vocab\n",
        "        all_attentions = torch.stack(all_attentions, dim=1)\n",
        "\n",
        "        return torch.cat(predictions, dim=1) , all_attentions  # b x maxlen x vocab\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size, hidden_dim, value_size=256, key_size=256, isAttended=False):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim)\n",
        "\n",
        "    def forward(self, epoch, speech_input, speech_len, text_input=None, isTrain=True):\n",
        "        key, value, lens = self.encoder(speech_input, speech_len)\n",
        "        if (isTrain == True):\n",
        "            predictions, att = self.decoder(epoch, key, value, lens, text_input)\n",
        "        else:\n",
        "            predictions, att = self.decoder(epoch, key, value, lens, text=text_input, isTrain=False)\n",
        "        return predictions, att\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfy2d0tVy653"
      },
      "source": [
        "# util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz2CpvbNy6lL"
      },
      "source": [
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_attn_flow(attn_mask, path):\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(attn_mask, cmap=\"hot\")\n",
        "    plt.show()\n",
        "    fig.savefig(path)\n",
        "    #plt.imsave(path, fig, cmap='hot')\n",
        "    plt.close()\n",
        "    return \n",
        "\n",
        "def plot_grad_flow(named_parameters, path):\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            if(p is not None):\n",
        "                layers.append(n)\n",
        "                ave_grads.append(p.grad.abs().mean())\n",
        "                max_grads.append(p.grad.abs().max())\n",
        "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    #plt.tight_layout()\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    plt.show()\n",
        "    plt.savefig(path)\n",
        "    return plt, max_grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c38Ut_HKLZjf"
      },
      "source": [
        "# train test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcwM8_kYLZCb"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "from Levenshtein import distance\n",
        "### Add Your Other Necessary Imports Here! ###\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "LETTER_LIST = ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
        "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']\n",
        "def train(model, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    start = time.time()\n",
        "\n",
        "    # 1) Iterate through your loader\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        #model.train()\n",
        "        X,Y,len_x, len_y=batch # x = B X maxlen X 40 Y = B X maxlen\n",
        "        print('i',i)\n",
        "        x = X.to(device) #Variable(X.type(Tensor)).cuda() #X.cuda() #to(device) #Variable(X.type(Tensor)).cuda() # X.long().cuda() #Variable(X.type(Tensor)).cuda()\n",
        "        #print(x.device)\n",
        "        \n",
        "        y = Y.to(device) # Variable(Y.type(Tensor)).cuda() #Y.cuda() #.to(device) #Variable(Y.type(Tensor)).cuda() # Y.long().cuda() #Variable(Y.type(Tensor)).cuda()\n",
        "        #print(y.device)\n",
        "        #len_x=len_x.to(device) \n",
        "        #len_y=len_y.to(device) \n",
        "        # torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "        # 2) Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion\n",
        "        \n",
        "            # 3) Set the inputs to the device.\n",
        "\n",
        "            # 4) Pass your inputs, and length of speech into the model.\n",
        "        optimizer.zero_grad()\n",
        "        pred, att = model(epoch, x, len_x, text_input=y, isTrain=True)\n",
        "        # for each text sequence mask = batch x maxlen   \n",
        "        \n",
        "        mask = torch.arange(y.size(1)).unsqueeze(0) >= len_y.unsqueeze(1)\n",
        "        mask = mask.to(device)\n",
        "        #pred.masked_fill_(mask, 0) # batch x maxlen x vocab\n",
        "        \n",
        "            # 5) Generate a mask based on the lengths of the text to create a masked loss. \n",
        "            # 5.1) Ensure the mask is on the device and is the correct shape.\n",
        "\n",
        "            # 6) If necessary, reshape your predictions and origianl text input \n",
        "            # 6.1) Use .contiguous() if you need to. \n",
        "\n",
        "            # 7) Use the criterion to get the loss.\n",
        "        #loss = 0\n",
        "        #for i in range(y.size(0)):\n",
        "            #index_ignore = torch.arange(len_y[i,:],y.size(1))\n",
        "            #loss + = criterion(pred[i,:,:].contiguous(), y[i,:].long(), )\n",
        "        loss = criterion(pred.contiguous().view(-1, pred.size(2)), y.long().view(-1)) # batch x maxlen\n",
        "        \n",
        "            # 8) Use the mask to calculate a masked loss.           ----> ???\n",
        "        loss.masked_fill_(mask.view(-1),0)\n",
        "            # 9) Run the backward pass on the masked loss.\n",
        "        loss =  loss.mean()\n",
        "        loss.backward()\n",
        "            # 10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)          \n",
        "            # 11) Take a step with your optimizer\n",
        "        optimizer.step()\n",
        "        if i == 5:\n",
        "            att = att[0, :len_y[0] - 1, :len_x[0]].cpu().numpy()\n",
        "            plot_attn_flow(att, path='/content/gdrive/MyDrive/11785/recitation08part02/exp2/attn_weight/attention'+str(epoch+1)+'.jpg')\n",
        "        \n",
        "        \n",
        "\n",
        "            # 12) Normalize the masked loss\n",
        "        #loss /= loss.sum()\n",
        "        print(\"epoch\", epoch+1)\n",
        "        print(' batch Loss train:', loss.item())\n",
        "\n",
        "            # 13) Optionally print the training loss after every N batches\n",
        "    \n",
        "    print('total epoch Loss train:', loss.mean())\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "def val(model, val_loader):\n",
        "    ### Write your val code here! ###\n",
        "    \n",
        "    model.eval()\n",
        "    for i, batch in enumerate(val_loader):\n",
        "        speech_input,text_input, speech_len, text_lens = batch\n",
        "        speech_input = speech_input.to(device)\n",
        "        text_input = text_input.to(device)\n",
        "        #speech_len = speech_len.to(device)\n",
        "        text_lens = text_lens.to(device)\n",
        "\n",
        "        #print(text_input.shape)\n",
        "        maxlen = text_input.shape[1]\n",
        "\n",
        "        pred, _ = model(0,speech_input, speech_len, text_input=text_input, isTrain=False)\n",
        "        edit_dist=0\n",
        "        \n",
        "        #characters = ''\n",
        "        for j in range(text_input.shape[0]):\n",
        "        # For the i-th sample in the batch, get the best output\n",
        "          seq = pred[j,:,:].argmax(dim=-1)\n",
        "          bestsound=''\n",
        "          ystring=''\n",
        "          for k in range(maxlen):\n",
        "            if text_input[j][k].item() == 34 or text_input[j][k].item() == 0:\n",
        "              break\n",
        "            else:\n",
        "              ystring += LETTER_LIST[text_input[j][k].item()] \n",
        "          for k in seq:\n",
        "              if k == 34 or k == 0:\n",
        "                  break\n",
        "              else:\n",
        "                  bestsound += LETTER_LIST[k]\n",
        "          #print('pred',bestsound)\n",
        "          \n",
        "          #print('label',ystring)\n",
        "          \n",
        "          edit_dist+=distance(ystring, bestsound)\n",
        "        print('dist = ',edit_dist/text_input.shape[0])\n",
        "\n",
        "    \n",
        "\n",
        "def test(model, test_loader):\n",
        "    ### Write your test code here! ###\n",
        "    model.eval()\n",
        "    predict = []\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        speech_input, speech_len = batch\n",
        "        speech_input = speech_input.to(device)\n",
        "        \n",
        "        #speech_len = speech_len.to(device)\n",
        "        \n",
        "\n",
        "        #print(text_input.shape)\n",
        "        \n",
        "\n",
        "        pred, _ = model(0,speech_input, speech_len, text_input=None, isTrain=False)\n",
        "        \n",
        "        \n",
        "        #characters = ''\n",
        "        for j in range(speech_input.shape[0]):\n",
        "        # For the i-th sample in the batch, get the best output\n",
        "          print(j)\n",
        "          seq = pred[j,:,:].argmax(dim=-1)\n",
        "          bestsound=''\n",
        "          for k in seq:\n",
        "              if k == 34 or k == 0:\n",
        "                  break\n",
        "              else:\n",
        "                  bestsound += LETTER_LIST[k]\n",
        "          predict.append(bestsound)\n",
        "          #print('pred',bestsound)\n",
        "          #print()\n",
        "          #print('label',ystring)\n",
        "          #print()\n",
        "          \n",
        "    return predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHkp9WXTiqPK"
      },
      "source": [
        "# load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htzHrAZYvRo9"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jrQbSsfNglF5",
        "outputId": "556ae9b1-c616-4e5e-a153-60576b0afd85"
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.nn.utils.rnn import *\n",
        "#from models import Seq2Seq\n",
        "#from train_test import train, test\n",
        "#from dataloader import load_data, collate_train, collate_test, transform_letter_to_index, Speech2TextDataset\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "LETTER_LIST = ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
        "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']\n",
        "\n",
        "\n",
        "model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim = 512)\n",
        "model = model.to(DEVICE)\n",
        "#model.load_state_dict(torch.load('/content/gdrive/MyDrive/11785/recitation08part02/hw4model_epoch_25'))\n",
        "#torch.save(model.state_dict(),'hw4model_epoch12.pth')\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "nepochs = 30\n",
        "batch_size = 64 if DEVICE == 'cuda' else 1\n",
        "\n",
        "speech_train, speech_valid, speech_test, transcript_train, transcript_valid = load_data()\n",
        "print(speech_train[0].shape)\n",
        "character_text_train = transform_letter_to_index(transcript_train, LETTER_LIST)\n",
        "character_text_valid = transform_letter_to_index(transcript_valid, LETTER_LIST)\n",
        "\n",
        "train_dataset = Speech2TextDataset(speech_train, character_text_train)\n",
        "\n",
        "val_dataset = Speech2TextDataset(speech_valid, character_text_valid)\n",
        "\n",
        "test_dataset = Speech2TextDataset(speech_test, None, False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_train)\n",
        "print(len(train_loader))\n",
        "#print(train_dataset.shape)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_train)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_test)\n",
        "\n",
        "for epoch in range(nepochs):\n",
        "    train(model, train_loader, criterion, optimizer, epoch)\n",
        "    #val(model, val_loader)\n",
        "    torch.save(model.state_dict(),\"./exp2/hw4model_epoch_{}\".format(epoch+1))\n",
        "#test(model, test_loader, epoch)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying weight drop of 0.2 to weight_hh_l0\n",
            "Applying weight drop of 0.2 to weight_hh_l0\n",
            "Applying weight drop of 0.2 to weight_hh_l0\n",
            "Applying weight drop of 0.2 to weight_hh_l0\n",
            "(1067, 40)\n",
            "446\n",
            "i 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:585: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:775.)\n",
            "  self.num_layers, self.dropout, self.training, self.bidirectional)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 31\n",
            " batch Loss train: 0.07918063551187515\n",
            "i 1\n",
            "epoch 31\n",
            " batch Loss train: 0.07648749649524689\n",
            "i 2\n",
            "epoch 31\n",
            " batch Loss train: 0.06444495171308517\n",
            "i 3\n",
            "epoch 31\n",
            " batch Loss train: 0.07763517647981644\n",
            "i 4\n",
            "epoch 31\n",
            " batch Loss train: 0.08324217796325684\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 31\n",
            " batch Loss train: 0.12833814322948456\n",
            "i 6\n",
            "epoch 31\n",
            " batch Loss train: 0.11012420058250427\n",
            "i 7\n",
            "epoch 31\n",
            " batch Loss train: 0.07828918844461441\n",
            "i 8\n",
            "epoch 31\n",
            " batch Loss train: 0.10164443403482437\n",
            "i 9\n",
            "epoch 31\n",
            " batch Loss train: 0.10428349673748016\n",
            "i 10\n",
            "epoch 31\n",
            " batch Loss train: 0.09883514791727066\n",
            "i 11\n",
            "epoch 31\n",
            " batch Loss train: 0.10816346853971481\n",
            "i 12\n",
            "epoch 31\n",
            " batch Loss train: 0.08052564412355423\n",
            "i 13\n",
            "epoch 31\n",
            " batch Loss train: 0.1081700250506401\n",
            "i 14\n",
            "epoch 31\n",
            " batch Loss train: 0.06366433203220367\n",
            "i 15\n",
            "epoch 31\n",
            " batch Loss train: 0.08887051790952682\n",
            "i 16\n",
            "epoch 31\n",
            " batch Loss train: 0.06581945717334747\n",
            "i 17\n",
            "epoch 31\n",
            " batch Loss train: 0.09720596671104431\n",
            "i 18\n",
            "epoch 31\n",
            " batch Loss train: 0.08290782570838928\n",
            "i 19\n",
            "epoch 31\n",
            " batch Loss train: 0.10261914134025574\n",
            "i 20\n",
            "epoch 31\n",
            " batch Loss train: 0.0850779339671135\n",
            "i 21\n",
            "epoch 31\n",
            " batch Loss train: 0.08650362491607666\n",
            "i 22\n",
            "epoch 31\n",
            " batch Loss train: 0.08559397608041763\n",
            "i 23\n",
            "epoch 31\n",
            " batch Loss train: 0.09157309681177139\n",
            "i 24\n",
            "epoch 31\n",
            " batch Loss train: 0.08681290596723557\n",
            "i 25\n",
            "epoch 31\n",
            " batch Loss train: 0.06701735407114029\n",
            "i 26\n",
            "epoch 31\n",
            " batch Loss train: 0.07733243703842163\n",
            "i 27\n",
            "epoch 31\n",
            " batch Loss train: 0.09710247814655304\n",
            "i 28\n",
            "epoch 31\n",
            " batch Loss train: 0.08159879595041275\n",
            "i 29\n",
            "epoch 31\n",
            " batch Loss train: 0.08887181431055069\n",
            "i 30\n",
            "epoch 31\n",
            " batch Loss train: 0.12418999522924423\n",
            "i 31\n",
            "epoch 31\n",
            " batch Loss train: 0.11290144920349121\n",
            "i 32\n",
            "epoch 31\n",
            " batch Loss train: 0.10698837041854858\n",
            "i 33\n",
            "epoch 31\n",
            " batch Loss train: 0.07327516376972198\n",
            "i 34\n",
            "epoch 31\n",
            " batch Loss train: 0.08638428151607513\n",
            "i 35\n",
            "epoch 31\n",
            " batch Loss train: 0.08640290051698685\n",
            "i 36\n",
            "epoch 31\n",
            " batch Loss train: 0.10791055858135223\n",
            "i 37\n",
            "epoch 31\n",
            " batch Loss train: 0.08894471824169159\n",
            "i 38\n",
            "epoch 31\n",
            " batch Loss train: 0.13615316152572632\n",
            "i 39\n",
            "epoch 31\n",
            " batch Loss train: 0.0898481085896492\n",
            "i 40\n",
            "epoch 31\n",
            " batch Loss train: 0.08575572818517685\n",
            "i 41\n",
            "epoch 31\n",
            " batch Loss train: 0.08941870927810669\n",
            "i 42\n",
            "epoch 31\n",
            " batch Loss train: 0.08490452170372009\n",
            "i 43\n",
            "epoch 31\n",
            " batch Loss train: 0.08637967705726624\n",
            "i 44\n",
            "epoch 31\n",
            " batch Loss train: 0.13225746154785156\n",
            "i 45\n",
            "epoch 31\n",
            " batch Loss train: 0.0794006884098053\n",
            "i 46\n",
            "epoch 31\n",
            " batch Loss train: 0.06136048212647438\n",
            "i 47\n",
            "epoch 31\n",
            " batch Loss train: 0.07477524876594543\n",
            "i 48\n",
            "epoch 31\n",
            " batch Loss train: 0.08062323182821274\n",
            "i 49\n",
            "epoch 31\n",
            " batch Loss train: 0.06528384983539581\n",
            "i 50\n",
            "epoch 31\n",
            " batch Loss train: 0.06473895162343979\n",
            "i 51\n",
            "epoch 31\n",
            " batch Loss train: 0.10180488973855972\n",
            "i 52\n",
            "epoch 31\n",
            " batch Loss train: 0.10706593841314316\n",
            "i 53\n",
            "epoch 31\n",
            " batch Loss train: 0.09259871393442154\n",
            "i 54\n",
            "epoch 31\n",
            " batch Loss train: 0.07703572511672974\n",
            "i 55\n",
            "epoch 31\n",
            " batch Loss train: 0.10567698627710342\n",
            "i 56\n",
            "epoch 31\n",
            " batch Loss train: 0.0760047659277916\n",
            "i 57\n",
            "epoch 31\n",
            " batch Loss train: 0.0792379155755043\n",
            "i 58\n",
            "epoch 31\n",
            " batch Loss train: 0.07303068041801453\n",
            "i 59\n",
            "epoch 31\n",
            " batch Loss train: 0.11196696758270264\n",
            "i 60\n",
            "epoch 31\n",
            " batch Loss train: 0.07271753251552582\n",
            "i 61\n",
            "epoch 31\n",
            " batch Loss train: 0.07029182463884354\n",
            "i 62\n",
            "epoch 31\n",
            " batch Loss train: 0.0780428946018219\n",
            "i 63\n",
            "epoch 31\n",
            " batch Loss train: 0.0735381692647934\n",
            "i 64\n",
            "epoch 31\n",
            " batch Loss train: 0.1070035845041275\n",
            "i 65\n",
            "epoch 31\n",
            " batch Loss train: 0.09398814290761948\n",
            "i 66\n",
            "epoch 31\n",
            " batch Loss train: 0.07738354802131653\n",
            "i 67\n",
            "epoch 31\n",
            " batch Loss train: 0.09635770320892334\n",
            "i 68\n",
            "epoch 31\n",
            " batch Loss train: 0.0599580779671669\n",
            "i 69\n",
            "epoch 31\n",
            " batch Loss train: 0.06916545331478119\n",
            "i 70\n",
            "epoch 31\n",
            " batch Loss train: 0.06691213697195053\n",
            "i 71\n",
            "epoch 31\n",
            " batch Loss train: 0.09766712039709091\n",
            "i 72\n",
            "epoch 31\n",
            " batch Loss train: 0.070314422249794\n",
            "i 73\n",
            "epoch 31\n",
            " batch Loss train: 0.07799726724624634\n",
            "i 74\n",
            "epoch 31\n",
            " batch Loss train: 0.08773371577262878\n",
            "i 75\n",
            "epoch 31\n",
            " batch Loss train: 0.10057327896356583\n",
            "i 76\n",
            "epoch 31\n",
            " batch Loss train: 0.08581086993217468\n",
            "i 77\n",
            "epoch 31\n",
            " batch Loss train: 0.08869360387325287\n",
            "i 78\n",
            "epoch 31\n",
            " batch Loss train: 0.06816184520721436\n",
            "i 79\n",
            "epoch 31\n",
            " batch Loss train: 0.08897252380847931\n",
            "i 80\n",
            "epoch 31\n",
            " batch Loss train: 0.0749494656920433\n",
            "i 81\n",
            "epoch 31\n",
            " batch Loss train: 0.09859475493431091\n",
            "i 82\n",
            "epoch 31\n",
            " batch Loss train: 0.09149493277072906\n",
            "i 83\n",
            "epoch 31\n",
            " batch Loss train: 0.09418279677629471\n",
            "i 84\n",
            "epoch 31\n",
            " batch Loss train: 0.09492997825145721\n",
            "i 85\n",
            "epoch 31\n",
            " batch Loss train: 0.07096952199935913\n",
            "i 86\n",
            "epoch 31\n",
            " batch Loss train: 0.06936077773571014\n",
            "i 87\n",
            "epoch 31\n",
            " batch Loss train: 0.0758991539478302\n",
            "i 88\n",
            "epoch 31\n",
            " batch Loss train: 0.1143765076994896\n",
            "i 89\n",
            "epoch 31\n",
            " batch Loss train: 0.10324172675609589\n",
            "i 90\n",
            "epoch 31\n",
            " batch Loss train: 0.06680548936128616\n",
            "i 91\n",
            "epoch 31\n",
            " batch Loss train: 0.06360692530870438\n",
            "i 92\n",
            "epoch 31\n",
            " batch Loss train: 0.07747585326433182\n",
            "i 93\n",
            "epoch 31\n",
            " batch Loss train: 0.10824430733919144\n",
            "i 94\n",
            "epoch 31\n",
            " batch Loss train: 0.10724455118179321\n",
            "i 95\n",
            "epoch 31\n",
            " batch Loss train: 0.08514466136693954\n",
            "i 96\n",
            "epoch 31\n",
            " batch Loss train: 0.05722636356949806\n",
            "i 97\n",
            "epoch 31\n",
            " batch Loss train: 0.08868594467639923\n",
            "i 98\n",
            "epoch 31\n",
            " batch Loss train: 0.08604544401168823\n",
            "i 99\n",
            "epoch 31\n",
            " batch Loss train: 0.08860138058662415\n",
            "i 100\n",
            "epoch 31\n",
            " batch Loss train: 0.09667408466339111\n",
            "i 101\n",
            "epoch 31\n",
            " batch Loss train: 0.10727252811193466\n",
            "i 102\n",
            "epoch 31\n",
            " batch Loss train: 0.09386930614709854\n",
            "i 103\n",
            "epoch 31\n",
            " batch Loss train: 0.12951074540615082\n",
            "i 104\n",
            "epoch 31\n",
            " batch Loss train: 0.07531819492578506\n",
            "i 105\n",
            "epoch 31\n",
            " batch Loss train: 0.08737477660179138\n",
            "i 106\n",
            "epoch 31\n",
            " batch Loss train: 0.06685935705900192\n",
            "i 107\n",
            "epoch 31\n",
            " batch Loss train: 0.08148534595966339\n",
            "i 108\n",
            "epoch 31\n",
            " batch Loss train: 0.09479108452796936\n",
            "i 109\n",
            "epoch 31\n",
            " batch Loss train: 0.09229348599910736\n",
            "i 110\n",
            "epoch 31\n",
            " batch Loss train: 0.08590471744537354\n",
            "i 111\n",
            "epoch 31\n",
            " batch Loss train: 0.08242049813270569\n",
            "i 112\n",
            "epoch 31\n",
            " batch Loss train: 0.08653877675533295\n",
            "i 113\n",
            "epoch 31\n",
            " batch Loss train: 0.10144340991973877\n",
            "i 114\n",
            "epoch 31\n",
            " batch Loss train: 0.07950887829065323\n",
            "i 115\n",
            "epoch 31\n",
            " batch Loss train: 0.09990283101797104\n",
            "i 116\n",
            "epoch 31\n",
            " batch Loss train: 0.10710503160953522\n",
            "i 117\n",
            "epoch 31\n",
            " batch Loss train: 0.10506925731897354\n",
            "i 118\n",
            "epoch 31\n",
            " batch Loss train: 0.08742137253284454\n",
            "i 119\n",
            "epoch 31\n",
            " batch Loss train: 0.08745159953832626\n",
            "i 120\n",
            "epoch 31\n",
            " batch Loss train: 0.09016870707273483\n",
            "i 121\n",
            "epoch 31\n",
            " batch Loss train: 0.10373762995004654\n",
            "i 122\n",
            "epoch 31\n",
            " batch Loss train: 0.0851280614733696\n",
            "i 123\n",
            "epoch 31\n",
            " batch Loss train: 0.09882847964763641\n",
            "i 124\n",
            "epoch 31\n",
            " batch Loss train: 0.10696275532245636\n",
            "i 125\n",
            "epoch 31\n",
            " batch Loss train: 0.09146574139595032\n",
            "i 126\n",
            "epoch 31\n",
            " batch Loss train: 0.08616267144680023\n",
            "i 127\n",
            "epoch 31\n",
            " batch Loss train: 0.09486912190914154\n",
            "i 128\n",
            "epoch 31\n",
            " batch Loss train: 0.09661238640546799\n",
            "i 129\n",
            "epoch 31\n",
            " batch Loss train: 0.08750487118959427\n",
            "i 130\n",
            "epoch 31\n",
            " batch Loss train: 0.09686989337205887\n",
            "i 131\n",
            "epoch 31\n",
            " batch Loss train: 0.08078707009553909\n",
            "i 132\n",
            "epoch 31\n",
            " batch Loss train: 0.07153188437223434\n",
            "i 133\n",
            "epoch 31\n",
            " batch Loss train: 0.0828457772731781\n",
            "i 134\n",
            "epoch 31\n",
            " batch Loss train: 0.0924038365483284\n",
            "i 135\n",
            "epoch 31\n",
            " batch Loss train: 0.0821664035320282\n",
            "i 136\n",
            "epoch 31\n",
            " batch Loss train: 0.09110264480113983\n",
            "i 137\n",
            "epoch 31\n",
            " batch Loss train: 0.09373964369297028\n",
            "i 138\n",
            "epoch 31\n",
            " batch Loss train: 0.0779668465256691\n",
            "i 139\n",
            "epoch 31\n",
            " batch Loss train: 0.08941050618886948\n",
            "i 140\n",
            "epoch 31\n",
            " batch Loss train: 0.10187694430351257\n",
            "i 141\n",
            "epoch 31\n",
            " batch Loss train: 0.09021604061126709\n",
            "i 142\n",
            "epoch 31\n",
            " batch Loss train: 0.0893179178237915\n",
            "i 143\n",
            "epoch 31\n",
            " batch Loss train: 0.06992102414369583\n",
            "i 144\n",
            "epoch 31\n",
            " batch Loss train: 0.0706145390868187\n",
            "i 145\n",
            "epoch 31\n",
            " batch Loss train: 0.08611135184764862\n",
            "i 146\n",
            "epoch 31\n",
            " batch Loss train: 0.08725347369909286\n",
            "i 147\n",
            "epoch 31\n",
            " batch Loss train: 0.10098538547754288\n",
            "i 148\n",
            "epoch 31\n",
            " batch Loss train: 0.11323289573192596\n",
            "i 149\n",
            "epoch 31\n",
            " batch Loss train: 0.12049231678247452\n",
            "i 150\n",
            "epoch 31\n",
            " batch Loss train: 0.1137518510222435\n",
            "i 151\n",
            "epoch 31\n",
            " batch Loss train: 0.1068962961435318\n",
            "i 152\n",
            "epoch 31\n",
            " batch Loss train: 0.10565323382616043\n",
            "i 153\n",
            "epoch 31\n",
            " batch Loss train: 0.08981344103813171\n",
            "i 154\n",
            "epoch 31\n",
            " batch Loss train: 0.0703575387597084\n",
            "i 155\n",
            "epoch 31\n",
            " batch Loss train: 0.08876167237758636\n",
            "i 156\n",
            "epoch 31\n",
            " batch Loss train: 0.08591470122337341\n",
            "i 157\n",
            "epoch 31\n",
            " batch Loss train: 0.13573795557022095\n",
            "i 158\n",
            "epoch 31\n",
            " batch Loss train: 0.09712830185890198\n",
            "i 159\n",
            "epoch 31\n",
            " batch Loss train: 0.08933524042367935\n",
            "i 160\n",
            "epoch 31\n",
            " batch Loss train: 0.06558149307966232\n",
            "i 161\n",
            "epoch 31\n",
            " batch Loss train: 0.09429065138101578\n",
            "i 162\n",
            "epoch 31\n",
            " batch Loss train: 0.12089250981807709\n",
            "i 163\n",
            "epoch 31\n",
            " batch Loss train: 0.07266271114349365\n",
            "i 164\n",
            "epoch 31\n",
            " batch Loss train: 0.08668863028287888\n",
            "i 165\n",
            "epoch 31\n",
            " batch Loss train: 0.12501244246959686\n",
            "i 166\n",
            "epoch 31\n",
            " batch Loss train: 0.06621149927377701\n",
            "i 167\n",
            "epoch 31\n",
            " batch Loss train: 0.09098505973815918\n",
            "i 168\n",
            "epoch 31\n",
            " batch Loss train: 0.0761619433760643\n",
            "i 169\n",
            "epoch 31\n",
            " batch Loss train: 0.08704640716314316\n",
            "i 170\n",
            "epoch 31\n",
            " batch Loss train: 0.10714849084615707\n",
            "i 171\n",
            "epoch 31\n",
            " batch Loss train: 0.07744866609573364\n",
            "i 172\n",
            "epoch 31\n",
            " batch Loss train: 0.09162099659442902\n",
            "i 173\n",
            "epoch 31\n",
            " batch Loss train: 0.09661790728569031\n",
            "i 174\n",
            "epoch 31\n",
            " batch Loss train: 0.10005299746990204\n",
            "i 175\n",
            "epoch 31\n",
            " batch Loss train: 0.11006154865026474\n",
            "i 176\n",
            "epoch 31\n",
            " batch Loss train: 0.10740972310304642\n",
            "i 177\n",
            "epoch 31\n",
            " batch Loss train: 0.10493095964193344\n",
            "i 178\n",
            "epoch 31\n",
            " batch Loss train: 0.08661762624979019\n",
            "i 179\n",
            "epoch 31\n",
            " batch Loss train: 0.12778674066066742\n",
            "i 180\n",
            "epoch 31\n",
            " batch Loss train: 0.09943772852420807\n",
            "i 181\n",
            "epoch 31\n",
            " batch Loss train: 0.08157865703105927\n",
            "i 182\n",
            "epoch 31\n",
            " batch Loss train: 0.06733057647943497\n",
            "i 183\n",
            "epoch 31\n",
            " batch Loss train: 0.08239086717367172\n",
            "i 184\n",
            "epoch 31\n",
            " batch Loss train: 0.07756184041500092\n",
            "i 185\n",
            "epoch 31\n",
            " batch Loss train: 0.09057319909334183\n",
            "i 186\n",
            "epoch 31\n",
            " batch Loss train: 0.08692051470279694\n",
            "i 187\n",
            "epoch 31\n",
            " batch Loss train: 0.09253348410129547\n",
            "i 188\n",
            "epoch 31\n",
            " batch Loss train: 0.08341602236032486\n",
            "i 189\n",
            "epoch 31\n",
            " batch Loss train: 0.07076676189899445\n",
            "i 190\n",
            "epoch 31\n",
            " batch Loss train: 0.09636325389146805\n",
            "i 191\n",
            "epoch 31\n",
            " batch Loss train: 0.09584073722362518\n",
            "i 192\n",
            "epoch 31\n",
            " batch Loss train: 0.08517513424158096\n",
            "i 193\n",
            "epoch 31\n",
            " batch Loss train: 0.087958924472332\n",
            "i 194\n",
            "epoch 31\n",
            " batch Loss train: 0.09304093569517136\n",
            "i 195\n",
            "epoch 31\n",
            " batch Loss train: 0.10206108540296555\n",
            "i 196\n",
            "epoch 31\n",
            " batch Loss train: 0.07259483635425568\n",
            "i 197\n",
            "epoch 31\n",
            " batch Loss train: 0.07674360275268555\n",
            "i 198\n",
            "epoch 31\n",
            " batch Loss train: 0.08202584832906723\n",
            "i 199\n",
            "epoch 31\n",
            " batch Loss train: 0.1072968915104866\n",
            "i 200\n",
            "epoch 31\n",
            " batch Loss train: 0.08330564200878143\n",
            "i 201\n",
            "epoch 31\n",
            " batch Loss train: 0.08498062938451767\n",
            "i 202\n",
            "epoch 31\n",
            " batch Loss train: 0.11964541673660278\n",
            "i 203\n",
            "epoch 31\n",
            " batch Loss train: 0.08460341393947601\n",
            "i 204\n",
            "epoch 31\n",
            " batch Loss train: 0.10543212294578552\n",
            "i 205\n",
            "epoch 31\n",
            " batch Loss train: 0.10169115662574768\n",
            "i 206\n",
            "epoch 31\n",
            " batch Loss train: 0.1310623586177826\n",
            "i 207\n",
            "epoch 31\n",
            " batch Loss train: 0.08593284338712692\n",
            "i 208\n",
            "epoch 31\n",
            " batch Loss train: 0.090414859354496\n",
            "i 209\n",
            "epoch 31\n",
            " batch Loss train: 0.10002840310335159\n",
            "i 210\n",
            "epoch 31\n",
            " batch Loss train: 0.09170510619878769\n",
            "i 211\n",
            "epoch 31\n",
            " batch Loss train: 0.12793989479541779\n",
            "i 212\n",
            "epoch 31\n",
            " batch Loss train: 0.08527981489896774\n",
            "i 213\n",
            "epoch 31\n",
            " batch Loss train: 0.06966139376163483\n",
            "i 214\n",
            "epoch 31\n",
            " batch Loss train: 0.08139672875404358\n",
            "i 215\n",
            "epoch 31\n",
            " batch Loss train: 0.08181910216808319\n",
            "i 216\n",
            "epoch 31\n",
            " batch Loss train: 0.06897372752428055\n",
            "i 217\n",
            "epoch 31\n",
            " batch Loss train: 0.07095752656459808\n",
            "i 218\n",
            "epoch 31\n",
            " batch Loss train: 0.09132466465234756\n",
            "i 219\n",
            "epoch 31\n",
            " batch Loss train: 0.08317159116268158\n",
            "i 220\n",
            "epoch 31\n",
            " batch Loss train: 0.09668620675802231\n",
            "i 221\n",
            "epoch 31\n",
            " batch Loss train: 0.09893219918012619\n",
            "i 222\n",
            "epoch 31\n",
            " batch Loss train: 0.09121431410312653\n",
            "i 223\n",
            "epoch 31\n",
            " batch Loss train: 0.12259890139102936\n",
            "i 224\n",
            "epoch 31\n",
            " batch Loss train: 0.09439167380332947\n",
            "i 225\n",
            "epoch 31\n",
            " batch Loss train: 0.10047516226768494\n",
            "i 226\n",
            "epoch 31\n",
            " batch Loss train: 0.08389479666948318\n",
            "i 227\n",
            "epoch 31\n",
            " batch Loss train: 0.08831725269556046\n",
            "i 228\n",
            "epoch 31\n",
            " batch Loss train: 0.10019290447235107\n",
            "i 229\n",
            "epoch 31\n",
            " batch Loss train: 0.11299129575490952\n",
            "i 230\n",
            "epoch 31\n",
            " batch Loss train: 0.08632641285657883\n",
            "i 231\n",
            "epoch 31\n",
            " batch Loss train: 0.07102879136800766\n",
            "i 232\n",
            "epoch 31\n",
            " batch Loss train: 0.08260884881019592\n",
            "i 233\n",
            "epoch 31\n",
            " batch Loss train: 0.08037060499191284\n",
            "i 234\n",
            "epoch 31\n",
            " batch Loss train: 0.0659090131521225\n",
            "i 235\n",
            "epoch 31\n",
            " batch Loss train: 0.08667086064815521\n",
            "i 236\n",
            "epoch 31\n",
            " batch Loss train: 0.07284753769636154\n",
            "i 237\n",
            "epoch 31\n",
            " batch Loss train: 0.07002700120210648\n",
            "i 238\n",
            "epoch 31\n",
            " batch Loss train: 0.08368337154388428\n",
            "i 239\n",
            "epoch 31\n",
            " batch Loss train: 0.08965708315372467\n",
            "i 240\n",
            "epoch 31\n",
            " batch Loss train: 0.0848342552781105\n",
            "i 241\n",
            "epoch 31\n",
            " batch Loss train: 0.07961735129356384\n",
            "i 242\n",
            "epoch 31\n",
            " batch Loss train: 0.09337186068296432\n",
            "i 243\n",
            "epoch 31\n",
            " batch Loss train: 0.07528410106897354\n",
            "i 244\n",
            "epoch 31\n",
            " batch Loss train: 0.10182026028633118\n",
            "i 245\n",
            "epoch 31\n",
            " batch Loss train: 0.08876755088567734\n",
            "i 246\n",
            "epoch 31\n",
            " batch Loss train: 0.09884519129991531\n",
            "i 247\n",
            "epoch 31\n",
            " batch Loss train: 0.09055299311876297\n",
            "i 248\n",
            "epoch 31\n",
            " batch Loss train: 0.09355457127094269\n",
            "i 249\n",
            "epoch 31\n",
            " batch Loss train: 0.07937231659889221\n",
            "i 250\n",
            "epoch 31\n",
            " batch Loss train: 0.0952880010008812\n",
            "i 251\n",
            "epoch 31\n",
            " batch Loss train: 0.08751249313354492\n",
            "i 252\n",
            "epoch 31\n",
            " batch Loss train: 0.08508889377117157\n",
            "i 253\n",
            "epoch 31\n",
            " batch Loss train: 0.08638739585876465\n",
            "i 254\n",
            "epoch 31\n",
            " batch Loss train: 0.09006314724683762\n",
            "i 255\n",
            "epoch 31\n",
            " batch Loss train: 0.08237933367490768\n",
            "i 256\n",
            "epoch 31\n",
            " batch Loss train: 0.09588693827390671\n",
            "i 257\n",
            "epoch 31\n",
            " batch Loss train: 0.08365748077630997\n",
            "i 258\n",
            "epoch 31\n",
            " batch Loss train: 0.0877777636051178\n",
            "i 259\n",
            "epoch 31\n",
            " batch Loss train: 0.09611954540014267\n",
            "i 260\n",
            "epoch 31\n",
            " batch Loss train: 0.10375607013702393\n",
            "i 261\n",
            "epoch 31\n",
            " batch Loss train: 0.08546184003353119\n",
            "i 262\n",
            "epoch 31\n",
            " batch Loss train: 0.09950128197669983\n",
            "i 263\n",
            "epoch 31\n",
            " batch Loss train: 0.08320972323417664\n",
            "i 264\n",
            "epoch 31\n",
            " batch Loss train: 0.09439309686422348\n",
            "i 265\n",
            "epoch 31\n",
            " batch Loss train: 0.11254873871803284\n",
            "i 266\n",
            "epoch 31\n",
            " batch Loss train: 0.10667727887630463\n",
            "i 267\n",
            "epoch 31\n",
            " batch Loss train: 0.10450855642557144\n",
            "i 268\n",
            "epoch 31\n",
            " batch Loss train: 0.09818080067634583\n",
            "i 269\n",
            "epoch 31\n",
            " batch Loss train: 0.12121907621622086\n",
            "i 270\n",
            "epoch 31\n",
            " batch Loss train: 0.12359365820884705\n",
            "i 271\n",
            "epoch 31\n",
            " batch Loss train: 0.06978923082351685\n",
            "i 272\n",
            "epoch 31\n",
            " batch Loss train: 0.08856131881475449\n",
            "i 273\n",
            "epoch 31\n",
            " batch Loss train: 0.1092899739742279\n",
            "i 274\n",
            "epoch 31\n",
            " batch Loss train: 0.09116648882627487\n",
            "i 275\n",
            "epoch 31\n",
            " batch Loss train: 0.11254827678203583\n",
            "i 276\n",
            "epoch 31\n",
            " batch Loss train: 0.08351671695709229\n",
            "i 277\n",
            "epoch 31\n",
            " batch Loss train: 0.11839865148067474\n",
            "i 278\n",
            "epoch 31\n",
            " batch Loss train: 0.08931029587984085\n",
            "i 279\n",
            "epoch 31\n",
            " batch Loss train: 0.09788907319307327\n",
            "i 280\n",
            "epoch 31\n",
            " batch Loss train: 0.10468510538339615\n",
            "i 281\n",
            "epoch 31\n",
            " batch Loss train: 0.1116529032588005\n",
            "i 282\n",
            "epoch 31\n",
            " batch Loss train: 0.11572758853435516\n",
            "i 283\n",
            "epoch 31\n",
            " batch Loss train: 0.10081575810909271\n",
            "i 284\n",
            "epoch 31\n",
            " batch Loss train: 0.0927785187959671\n",
            "i 285\n",
            "epoch 31\n",
            " batch Loss train: 0.0766233578324318\n",
            "i 286\n",
            "epoch 31\n",
            " batch Loss train: 0.09295453876256943\n",
            "i 287\n",
            "epoch 31\n",
            " batch Loss train: 0.09141086041927338\n",
            "i 288\n",
            "epoch 31\n",
            " batch Loss train: 0.08529888093471527\n",
            "i 289\n",
            "epoch 31\n",
            " batch Loss train: 0.07231177389621735\n",
            "i 290\n",
            "epoch 31\n",
            " batch Loss train: 0.10394204407930374\n",
            "i 291\n",
            "epoch 31\n",
            " batch Loss train: 0.09557497501373291\n",
            "i 292\n",
            "epoch 31\n",
            " batch Loss train: 0.09848187863826752\n",
            "i 293\n",
            "epoch 31\n",
            " batch Loss train: 0.09390097856521606\n",
            "i 294\n",
            "epoch 31\n",
            " batch Loss train: 0.10572606325149536\n",
            "i 295\n",
            "epoch 31\n",
            " batch Loss train: 0.09892695397138596\n",
            "i 296\n",
            "epoch 31\n",
            " batch Loss train: 0.08718754351139069\n",
            "i 297\n",
            "epoch 31\n",
            " batch Loss train: 0.08373827487230301\n",
            "i 298\n",
            "epoch 31\n",
            " batch Loss train: 0.09367216378450394\n",
            "i 299\n",
            "epoch 31\n",
            " batch Loss train: 0.10063537955284119\n",
            "i 300\n",
            "epoch 31\n",
            " batch Loss train: 0.08944650739431381\n",
            "i 301\n",
            "epoch 31\n",
            " batch Loss train: 0.09213075041770935\n",
            "i 302\n",
            "epoch 31\n",
            " batch Loss train: 0.08344648778438568\n",
            "i 303\n",
            "epoch 31\n",
            " batch Loss train: 0.10284548252820969\n",
            "i 304\n",
            "epoch 31\n",
            " batch Loss train: 0.08697321265935898\n",
            "i 305\n",
            "epoch 31\n",
            " batch Loss train: 0.09132219851016998\n",
            "i 306\n",
            "epoch 31\n",
            " batch Loss train: 0.07984859496355057\n",
            "i 307\n",
            "epoch 31\n",
            " batch Loss train: 0.09877809882164001\n",
            "i 308\n",
            "epoch 31\n",
            " batch Loss train: 0.09564739465713501\n",
            "i 309\n",
            "epoch 31\n",
            " batch Loss train: 0.07803522050380707\n",
            "i 310\n",
            "epoch 31\n",
            " batch Loss train: 0.08461174368858337\n",
            "i 311\n",
            "epoch 31\n",
            " batch Loss train: 0.08419303596019745\n",
            "i 312\n",
            "epoch 31\n",
            " batch Loss train: 0.07214322686195374\n",
            "i 313\n",
            "epoch 31\n",
            " batch Loss train: 0.0871855691075325\n",
            "i 314\n",
            "epoch 31\n",
            " batch Loss train: 0.08689817786216736\n",
            "i 315\n",
            "epoch 31\n",
            " batch Loss train: 0.09326354414224625\n",
            "i 316\n",
            "epoch 31\n",
            " batch Loss train: 0.06789755076169968\n",
            "i 317\n",
            "epoch 31\n",
            " batch Loss train: 0.07707846164703369\n",
            "i 318\n",
            "epoch 31\n",
            " batch Loss train: 0.08238653838634491\n",
            "i 319\n",
            "epoch 31\n",
            " batch Loss train: 0.12646201252937317\n",
            "i 320\n",
            "epoch 31\n",
            " batch Loss train: 0.09170378744602203\n",
            "i 321\n",
            "epoch 31\n",
            " batch Loss train: 0.09386922419071198\n",
            "i 322\n",
            "epoch 31\n",
            " batch Loss train: 0.09103310853242874\n",
            "i 323\n",
            "epoch 31\n",
            " batch Loss train: 0.08552701771259308\n",
            "i 324\n",
            "epoch 31\n",
            " batch Loss train: 0.10633811354637146\n",
            "i 325\n",
            "epoch 31\n",
            " batch Loss train: 0.14853250980377197\n",
            "i 326\n",
            "epoch 31\n",
            " batch Loss train: 0.08874525129795074\n",
            "i 327\n",
            "epoch 31\n",
            " batch Loss train: 0.09350086003541946\n",
            "i 328\n",
            "epoch 31\n",
            " batch Loss train: 0.11419757455587387\n",
            "i 329\n",
            "epoch 31\n",
            " batch Loss train: 0.10177328437566757\n",
            "i 330\n",
            "epoch 31\n",
            " batch Loss train: 0.09777434170246124\n",
            "i 331\n",
            "epoch 31\n",
            " batch Loss train: 0.11526628583669662\n",
            "i 332\n",
            "epoch 31\n",
            " batch Loss train: 0.10413701832294464\n",
            "i 333\n",
            "epoch 31\n",
            " batch Loss train: 0.08601099997758865\n",
            "i 334\n",
            "epoch 31\n",
            " batch Loss train: 0.09437737613916397\n",
            "i 335\n",
            "epoch 31\n",
            " batch Loss train: 0.08696712553501129\n",
            "i 336\n",
            "epoch 31\n",
            " batch Loss train: 0.11285331845283508\n",
            "i 337\n",
            "epoch 31\n",
            " batch Loss train: 0.09462852776050568\n",
            "i 338\n",
            "epoch 31\n",
            " batch Loss train: 0.07880730926990509\n",
            "i 339\n",
            "epoch 31\n",
            " batch Loss train: 0.08741176873445511\n",
            "i 340\n",
            "epoch 31\n",
            " batch Loss train: 0.12628403306007385\n",
            "i 341\n",
            "epoch 31\n",
            " batch Loss train: 0.08591552078723907\n",
            "i 342\n",
            "epoch 31\n",
            " batch Loss train: 0.1033664420247078\n",
            "i 343\n",
            "epoch 31\n",
            " batch Loss train: 0.09541241079568863\n",
            "i 344\n",
            "epoch 31\n",
            " batch Loss train: 0.07351613789796829\n",
            "i 345\n",
            "epoch 31\n",
            " batch Loss train: 0.1431490033864975\n",
            "i 346\n",
            "epoch 31\n",
            " batch Loss train: 0.10085365176200867\n",
            "i 347\n",
            "epoch 31\n",
            " batch Loss train: 0.1161736324429512\n",
            "i 348\n",
            "epoch 31\n",
            " batch Loss train: 0.08040613681077957\n",
            "i 349\n",
            "epoch 31\n",
            " batch Loss train: 0.1232868954539299\n",
            "i 350\n",
            "epoch 31\n",
            " batch Loss train: 0.07856042683124542\n",
            "i 351\n",
            "epoch 31\n",
            " batch Loss train: 0.12064030766487122\n",
            "i 352\n",
            "epoch 31\n",
            " batch Loss train: 0.10334979742765427\n",
            "i 353\n",
            "epoch 31\n",
            " batch Loss train: 0.11375521868467331\n",
            "i 354\n",
            "epoch 31\n",
            " batch Loss train: 0.11253774166107178\n",
            "i 355\n",
            "epoch 31\n",
            " batch Loss train: 0.08308408409357071\n",
            "i 356\n",
            "epoch 31\n",
            " batch Loss train: 0.08734283596277237\n",
            "i 357\n",
            "epoch 31\n",
            " batch Loss train: 0.11267870664596558\n",
            "i 358\n",
            "epoch 31\n",
            " batch Loss train: 0.11587357521057129\n",
            "i 359\n",
            "epoch 31\n",
            " batch Loss train: 0.1034223884344101\n",
            "i 360\n",
            "epoch 31\n",
            " batch Loss train: 0.09247848391532898\n",
            "i 361\n",
            "epoch 31\n",
            " batch Loss train: 0.10217256098985672\n",
            "i 362\n",
            "epoch 31\n",
            " batch Loss train: 0.09123557806015015\n",
            "i 363\n",
            "epoch 31\n",
            " batch Loss train: 0.080928735435009\n",
            "i 364\n",
            "epoch 31\n",
            " batch Loss train: 0.08255064487457275\n",
            "i 365\n",
            "epoch 31\n",
            " batch Loss train: 0.0996226817369461\n",
            "i 366\n",
            "epoch 31\n",
            " batch Loss train: 0.09473138302564621\n",
            "i 367\n",
            "epoch 31\n",
            " batch Loss train: 0.08855050057172775\n",
            "i 368\n",
            "epoch 31\n",
            " batch Loss train: 0.07894852012395859\n",
            "i 369\n",
            "epoch 31\n",
            " batch Loss train: 0.09160648286342621\n",
            "i 370\n",
            "epoch 31\n",
            " batch Loss train: 0.11373283714056015\n",
            "i 371\n",
            "epoch 31\n",
            " batch Loss train: 0.0854165181517601\n",
            "i 372\n",
            "epoch 31\n",
            " batch Loss train: 0.09346000850200653\n",
            "i 373\n",
            "epoch 31\n",
            " batch Loss train: 0.08023539185523987\n",
            "i 374\n",
            "epoch 31\n",
            " batch Loss train: 0.10792982578277588\n",
            "i 375\n",
            "epoch 31\n",
            " batch Loss train: 0.09786035120487213\n",
            "i 376\n",
            "epoch 31\n",
            " batch Loss train: 0.10217848420143127\n",
            "i 377\n",
            "epoch 31\n",
            " batch Loss train: 0.10426982492208481\n",
            "i 378\n",
            "epoch 31\n",
            " batch Loss train: 0.09995073825120926\n",
            "i 379\n",
            "epoch 31\n",
            " batch Loss train: 0.10184474289417267\n",
            "i 380\n",
            "epoch 31\n",
            " batch Loss train: 0.09248422831296921\n",
            "i 381\n",
            "epoch 31\n",
            " batch Loss train: 0.09248729050159454\n",
            "i 382\n",
            "epoch 31\n",
            " batch Loss train: 0.1052386611700058\n",
            "i 383\n",
            "epoch 31\n",
            " batch Loss train: 0.08922477066516876\n",
            "i 384\n",
            "epoch 31\n",
            " batch Loss train: 0.0916554406285286\n",
            "i 385\n",
            "epoch 31\n",
            " batch Loss train: 0.10466412454843521\n",
            "i 386\n",
            "epoch 31\n",
            " batch Loss train: 0.11088258773088455\n",
            "i 387\n",
            "epoch 31\n",
            " batch Loss train: 0.11710479110479355\n",
            "i 388\n",
            "epoch 31\n",
            " batch Loss train: 0.11315187066793442\n",
            "i 389\n",
            "epoch 31\n",
            " batch Loss train: 0.10656892508268356\n",
            "i 390\n",
            "epoch 31\n",
            " batch Loss train: 0.07409647852182388\n",
            "i 391\n",
            "epoch 31\n",
            " batch Loss train: 0.10692223906517029\n",
            "i 392\n",
            "epoch 31\n",
            " batch Loss train: 0.11057072132825851\n",
            "i 393\n",
            "epoch 31\n",
            " batch Loss train: 0.08375159651041031\n",
            "i 394\n",
            "epoch 31\n",
            " batch Loss train: 0.10113313049077988\n",
            "i 395\n",
            "epoch 31\n",
            " batch Loss train: 0.10228049755096436\n",
            "i 396\n",
            "epoch 31\n",
            " batch Loss train: 0.08333726227283478\n",
            "i 397\n",
            "epoch 31\n",
            " batch Loss train: 0.11255882680416107\n",
            "i 398\n",
            "epoch 31\n",
            " batch Loss train: 0.07724988460540771\n",
            "i 399\n",
            "epoch 31\n",
            " batch Loss train: 0.09533006697893143\n",
            "i 400\n",
            "epoch 31\n",
            " batch Loss train: 0.1214052215218544\n",
            "i 401\n",
            "epoch 31\n",
            " batch Loss train: 0.07161707431077957\n",
            "i 402\n",
            "epoch 31\n",
            " batch Loss train: 0.13253284990787506\n",
            "i 403\n",
            "epoch 31\n",
            " batch Loss train: 0.10768455266952515\n",
            "i 404\n",
            "epoch 31\n",
            " batch Loss train: 0.08587591350078583\n",
            "i 405\n",
            "epoch 31\n",
            " batch Loss train: 0.11534333974123001\n",
            "i 406\n",
            "epoch 31\n",
            " batch Loss train: 0.10117467492818832\n",
            "i 407\n",
            "epoch 31\n",
            " batch Loss train: 0.1233932375907898\n",
            "i 408\n",
            "epoch 31\n",
            " batch Loss train: 0.12893134355545044\n",
            "i 409\n",
            "epoch 31\n",
            " batch Loss train: 0.08958316594362259\n",
            "i 410\n",
            "epoch 31\n",
            " batch Loss train: 0.08874312788248062\n",
            "i 411\n",
            "epoch 31\n",
            " batch Loss train: 0.10431940853595734\n",
            "i 412\n",
            "epoch 31\n",
            " batch Loss train: 0.06757624447345734\n",
            "i 413\n",
            "epoch 31\n",
            " batch Loss train: 0.07852263748645782\n",
            "i 414\n",
            "epoch 31\n",
            " batch Loss train: 0.08161921799182892\n",
            "i 415\n",
            "epoch 31\n",
            " batch Loss train: 0.10448722541332245\n",
            "i 416\n",
            "epoch 31\n",
            " batch Loss train: 0.10966898500919342\n",
            "i 417\n",
            "epoch 31\n",
            " batch Loss train: 0.0829755961894989\n",
            "i 418\n",
            "epoch 31\n",
            " batch Loss train: 0.0962863638997078\n",
            "i 419\n",
            "epoch 31\n",
            " batch Loss train: 0.0900251567363739\n",
            "i 420\n",
            "epoch 31\n",
            " batch Loss train: 0.08461743593215942\n",
            "i 421\n",
            "epoch 31\n",
            " batch Loss train: 0.10189799219369888\n",
            "i 422\n",
            "epoch 31\n",
            " batch Loss train: 0.08226629346609116\n",
            "i 423\n",
            "epoch 31\n",
            " batch Loss train: 0.11587942391633987\n",
            "i 424\n",
            "epoch 31\n",
            " batch Loss train: 0.07957188040018082\n",
            "i 425\n",
            "epoch 31\n",
            " batch Loss train: 0.07982449233531952\n",
            "i 426\n",
            "epoch 31\n",
            " batch Loss train: 0.12007425725460052\n",
            "i 427\n",
            "epoch 31\n",
            " batch Loss train: 0.10515867918729782\n",
            "i 428\n",
            "epoch 31\n",
            " batch Loss train: 0.0896039828658104\n",
            "i 429\n",
            "epoch 31\n",
            " batch Loss train: 0.08254965394735336\n",
            "i 430\n",
            "epoch 31\n",
            " batch Loss train: 0.0968068391084671\n",
            "i 431\n",
            "epoch 31\n",
            " batch Loss train: 0.1528395712375641\n",
            "i 432\n",
            "epoch 31\n",
            " batch Loss train: 0.09144233167171478\n",
            "i 433\n",
            "epoch 31\n",
            " batch Loss train: 0.14495056867599487\n",
            "i 434\n",
            "epoch 31\n",
            " batch Loss train: 0.08520570397377014\n",
            "i 435\n",
            "epoch 31\n",
            " batch Loss train: 0.1061568483710289\n",
            "i 436\n",
            "epoch 31\n",
            " batch Loss train: 0.11071278154850006\n",
            "i 437\n",
            "epoch 31\n",
            " batch Loss train: 0.08841338008642197\n",
            "i 438\n",
            "epoch 31\n",
            " batch Loss train: 0.08687108755111694\n",
            "i 439\n",
            "epoch 31\n",
            " batch Loss train: 0.10275254398584366\n",
            "i 440\n",
            "epoch 31\n",
            " batch Loss train: 0.09470998495817184\n",
            "i 441\n",
            "epoch 31\n",
            " batch Loss train: 0.08585239201784134\n",
            "i 442\n",
            "epoch 31\n",
            " batch Loss train: 0.10769984871149063\n",
            "i 443\n",
            "epoch 31\n",
            " batch Loss train: 0.10351236164569855\n",
            "i 444\n",
            "epoch 31\n",
            " batch Loss train: 0.08833587914705276\n",
            "i 445\n",
            "epoch 31\n",
            " batch Loss train: 0.10577397048473358\n",
            "total epoch Loss train: tensor(0.1058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 32\n",
            " batch Loss train: 0.099749855697155\n",
            "i 1\n",
            "epoch 32\n",
            " batch Loss train: 0.05907207354903221\n",
            "i 2\n",
            "epoch 32\n",
            " batch Loss train: 0.07040480524301529\n",
            "i 3\n",
            "epoch 32\n",
            " batch Loss train: 0.064728744328022\n",
            "i 4\n",
            "epoch 32\n",
            " batch Loss train: 0.07528309524059296\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 32\n",
            " batch Loss train: 0.06021897494792938\n",
            "i 6\n",
            "epoch 32\n",
            " batch Loss train: 0.09735576063394547\n",
            "i 7\n",
            "epoch 32\n",
            " batch Loss train: 0.10926084220409393\n",
            "i 8\n",
            "epoch 32\n",
            " batch Loss train: 0.06905844807624817\n",
            "i 9\n",
            "epoch 32\n",
            " batch Loss train: 0.06475061923265457\n",
            "i 10\n",
            "epoch 32\n",
            " batch Loss train: 0.09215962886810303\n",
            "i 11\n",
            "epoch 32\n",
            " batch Loss train: 0.08417818695306778\n",
            "i 12\n",
            "epoch 32\n",
            " batch Loss train: 0.07244747877120972\n",
            "i 13\n",
            "epoch 32\n",
            " batch Loss train: 0.05919743701815605\n",
            "i 14\n",
            "epoch 32\n",
            " batch Loss train: 0.07234242558479309\n",
            "i 15\n",
            "epoch 32\n",
            " batch Loss train: 0.0831424742937088\n",
            "i 16\n",
            "epoch 32\n",
            " batch Loss train: 0.11475033313035965\n",
            "i 17\n",
            "epoch 32\n",
            " batch Loss train: 0.08875316381454468\n",
            "i 18\n",
            "epoch 32\n",
            " batch Loss train: 0.09924650192260742\n",
            "i 19\n",
            "epoch 32\n",
            " batch Loss train: 0.10162286460399628\n",
            "i 20\n",
            "epoch 32\n",
            " batch Loss train: 0.08493021130561829\n",
            "i 21\n",
            "epoch 32\n",
            " batch Loss train: 0.0905766487121582\n",
            "i 22\n",
            "epoch 32\n",
            " batch Loss train: 0.10401349514722824\n",
            "i 23\n",
            "epoch 32\n",
            " batch Loss train: 0.0719422921538353\n",
            "i 24\n",
            "epoch 32\n",
            " batch Loss train: 0.09832581132650375\n",
            "i 25\n",
            "epoch 32\n",
            " batch Loss train: 0.0810106173157692\n",
            "i 26\n",
            "epoch 32\n",
            " batch Loss train: 0.08872143179178238\n",
            "i 27\n",
            "epoch 32\n",
            " batch Loss train: 0.07986126840114594\n",
            "i 28\n",
            "epoch 32\n",
            " batch Loss train: 0.07449357956647873\n",
            "i 29\n",
            "epoch 32\n",
            " batch Loss train: 0.1313740462064743\n",
            "i 30\n",
            "epoch 32\n",
            " batch Loss train: 0.06471087038516998\n",
            "i 31\n",
            "epoch 32\n",
            " batch Loss train: 0.07199215143918991\n",
            "i 32\n",
            "epoch 32\n",
            " batch Loss train: 0.0703955888748169\n",
            "i 33\n",
            "epoch 32\n",
            " batch Loss train: 0.06975007057189941\n",
            "i 34\n",
            "epoch 32\n",
            " batch Loss train: 0.08489478379487991\n",
            "i 35\n",
            "epoch 32\n",
            " batch Loss train: 0.09473168104887009\n",
            "i 36\n",
            "epoch 32\n",
            " batch Loss train: 0.05041339993476868\n",
            "i 37\n",
            "epoch 32\n",
            " batch Loss train: 0.09375228732824326\n",
            "i 38\n",
            "epoch 32\n",
            " batch Loss train: 0.09053917974233627\n",
            "i 39\n",
            "epoch 32\n",
            " batch Loss train: 0.07816455513238907\n",
            "i 40\n",
            "epoch 32\n",
            " batch Loss train: 0.08833994716405869\n",
            "i 41\n",
            "epoch 32\n",
            " batch Loss train: 0.07975554466247559\n",
            "i 42\n",
            "epoch 32\n",
            " batch Loss train: 0.05635395273566246\n",
            "i 43\n",
            "epoch 32\n",
            " batch Loss train: 0.09153114259243011\n",
            "i 44\n",
            "epoch 32\n",
            " batch Loss train: 0.07255807518959045\n",
            "i 45\n",
            "epoch 32\n",
            " batch Loss train: 0.0889093279838562\n",
            "i 46\n",
            "epoch 32\n",
            " batch Loss train: 0.08878063410520554\n",
            "i 47\n",
            "epoch 32\n",
            " batch Loss train: 0.0655457079410553\n",
            "i 48\n",
            "epoch 32\n",
            " batch Loss train: 0.08110425621271133\n",
            "i 49\n",
            "epoch 32\n",
            " batch Loss train: 0.07928621768951416\n",
            "i 50\n",
            "epoch 32\n",
            " batch Loss train: 0.06862886995077133\n",
            "i 51\n",
            "epoch 32\n",
            " batch Loss train: 0.0888238251209259\n",
            "i 52\n",
            "epoch 32\n",
            " batch Loss train: 0.06611976027488708\n",
            "i 53\n",
            "epoch 32\n",
            " batch Loss train: 0.0866461843252182\n",
            "i 54\n",
            "epoch 32\n",
            " batch Loss train: 0.07528263330459595\n",
            "i 55\n",
            "epoch 32\n",
            " batch Loss train: 0.06203429400920868\n",
            "i 56\n",
            "epoch 32\n",
            " batch Loss train: 0.06756249070167542\n",
            "i 57\n",
            "epoch 32\n",
            " batch Loss train: 0.06970309466123581\n",
            "i 58\n",
            "epoch 32\n",
            " batch Loss train: 0.07538734376430511\n",
            "i 59\n",
            "epoch 32\n",
            " batch Loss train: 0.08282942324876785\n",
            "i 60\n",
            "epoch 32\n",
            " batch Loss train: 0.07947719097137451\n",
            "i 61\n",
            "epoch 32\n",
            " batch Loss train: 0.06610318273305893\n",
            "i 62\n",
            "epoch 32\n",
            " batch Loss train: 0.07147548347711563\n",
            "i 63\n",
            "epoch 32\n",
            " batch Loss train: 0.09622523188591003\n",
            "i 64\n",
            "epoch 32\n",
            " batch Loss train: 0.08912041783332825\n",
            "i 65\n",
            "epoch 32\n",
            " batch Loss train: 0.11937770992517471\n",
            "i 66\n",
            "epoch 32\n",
            " batch Loss train: 0.05707839876413345\n",
            "i 67\n",
            "epoch 32\n",
            " batch Loss train: 0.061144184321165085\n",
            "i 68\n",
            "epoch 32\n",
            " batch Loss train: 0.07720111310482025\n",
            "i 69\n",
            "epoch 32\n",
            " batch Loss train: 0.07721071690320969\n",
            "i 70\n",
            "epoch 32\n",
            " batch Loss train: 0.08222392201423645\n",
            "i 71\n",
            "epoch 32\n",
            " batch Loss train: 0.10297515243291855\n",
            "i 72\n",
            "epoch 32\n",
            " batch Loss train: 0.11279046535491943\n",
            "i 73\n",
            "epoch 32\n",
            " batch Loss train: 0.07716400176286697\n",
            "i 74\n",
            "epoch 32\n",
            " batch Loss train: 0.09601063281297684\n",
            "i 75\n",
            "epoch 32\n",
            " batch Loss train: 0.08256744593381882\n",
            "i 76\n",
            "epoch 32\n",
            " batch Loss train: 0.08162538707256317\n",
            "i 77\n",
            "epoch 32\n",
            " batch Loss train: 0.09829491376876831\n",
            "i 78\n",
            "epoch 32\n",
            " batch Loss train: 0.08227220177650452\n",
            "i 79\n",
            "epoch 32\n",
            " batch Loss train: 0.09348400682210922\n",
            "i 80\n",
            "epoch 32\n",
            " batch Loss train: 0.06487295031547546\n",
            "i 81\n",
            "epoch 32\n",
            " batch Loss train: 0.0976441502571106\n",
            "i 82\n",
            "epoch 32\n",
            " batch Loss train: 0.06442370265722275\n",
            "i 83\n",
            "epoch 32\n",
            " batch Loss train: 0.09741766005754471\n",
            "i 84\n",
            "epoch 32\n",
            " batch Loss train: 0.10285335779190063\n",
            "i 85\n",
            "epoch 32\n",
            " batch Loss train: 0.08982648700475693\n",
            "i 86\n",
            "epoch 32\n",
            " batch Loss train: 0.0804186463356018\n",
            "i 87\n",
            "epoch 32\n",
            " batch Loss train: 0.07674624770879745\n",
            "i 88\n",
            "epoch 32\n",
            " batch Loss train: 0.07283656299114227\n",
            "i 89\n",
            "epoch 32\n",
            " batch Loss train: 0.0853802040219307\n",
            "i 90\n",
            "epoch 32\n",
            " batch Loss train: 0.08080229163169861\n",
            "i 91\n",
            "epoch 32\n",
            " batch Loss train: 0.10078444331884384\n",
            "i 92\n",
            "epoch 32\n",
            " batch Loss train: 0.08784537762403488\n",
            "i 93\n",
            "epoch 32\n",
            " batch Loss train: 0.07330511510372162\n",
            "i 94\n",
            "epoch 32\n",
            " batch Loss train: 0.08640815317630768\n",
            "i 95\n",
            "epoch 32\n",
            " batch Loss train: 0.06474601477384567\n",
            "i 96\n",
            "epoch 32\n",
            " batch Loss train: 0.08565274626016617\n",
            "i 97\n",
            "epoch 32\n",
            " batch Loss train: 0.10157331079244614\n",
            "i 98\n",
            "epoch 32\n",
            " batch Loss train: 0.0665944293141365\n",
            "i 99\n",
            "epoch 32\n",
            " batch Loss train: 0.07240326702594757\n",
            "i 100\n",
            "epoch 32\n",
            " batch Loss train: 0.08778070658445358\n",
            "i 101\n",
            "epoch 32\n",
            " batch Loss train: 0.1021101251244545\n",
            "i 102\n",
            "epoch 32\n",
            " batch Loss train: 0.0650172233581543\n",
            "i 103\n",
            "epoch 32\n",
            " batch Loss train: 0.089210145175457\n",
            "i 104\n",
            "epoch 32\n",
            " batch Loss train: 0.0965256467461586\n",
            "i 105\n",
            "epoch 32\n",
            " batch Loss train: 0.12956452369689941\n",
            "i 106\n",
            "epoch 32\n",
            " batch Loss train: 0.08255751430988312\n",
            "i 107\n",
            "epoch 32\n",
            " batch Loss train: 0.07518887519836426\n",
            "i 108\n",
            "epoch 32\n",
            " batch Loss train: 0.07501979172229767\n",
            "i 109\n",
            "epoch 32\n",
            " batch Loss train: 0.08633165806531906\n",
            "i 110\n",
            "epoch 32\n",
            " batch Loss train: 0.06711786985397339\n",
            "i 111\n",
            "epoch 32\n",
            " batch Loss train: 0.10540927201509476\n",
            "i 112\n",
            "epoch 32\n",
            " batch Loss train: 0.09205223619937897\n",
            "i 113\n",
            "epoch 32\n",
            " batch Loss train: 0.08452845364809036\n",
            "i 114\n",
            "epoch 32\n",
            " batch Loss train: 0.09056153893470764\n",
            "i 115\n",
            "epoch 32\n",
            " batch Loss train: 0.0765688568353653\n",
            "i 116\n",
            "epoch 32\n",
            " batch Loss train: 0.08527553826570511\n",
            "i 117\n",
            "epoch 32\n",
            " batch Loss train: 0.07968307286500931\n",
            "i 118\n",
            "epoch 32\n",
            " batch Loss train: 0.08421195298433304\n",
            "i 119\n",
            "epoch 32\n",
            " batch Loss train: 0.08225671947002411\n",
            "i 120\n",
            "epoch 32\n",
            " batch Loss train: 0.14295002818107605\n",
            "i 121\n",
            "epoch 32\n",
            " batch Loss train: 0.09850654006004333\n",
            "i 122\n",
            "epoch 32\n",
            " batch Loss train: 0.10898011177778244\n",
            "i 123\n",
            "epoch 32\n",
            " batch Loss train: 0.10614104568958282\n",
            "i 124\n",
            "epoch 32\n",
            " batch Loss train: 0.07113441824913025\n",
            "i 125\n",
            "epoch 32\n",
            " batch Loss train: 0.0829973816871643\n",
            "i 126\n",
            "epoch 32\n",
            " batch Loss train: 0.07197239249944687\n",
            "i 127\n",
            "epoch 32\n",
            " batch Loss train: 0.09285492449998856\n",
            "i 128\n",
            "epoch 32\n",
            " batch Loss train: 0.07345147430896759\n",
            "i 129\n",
            "epoch 32\n",
            " batch Loss train: 0.11555872112512589\n",
            "i 130\n",
            "epoch 32\n",
            " batch Loss train: 0.09779296070337296\n",
            "i 131\n",
            "epoch 32\n",
            " batch Loss train: 0.07041115313768387\n",
            "i 132\n",
            "epoch 32\n",
            " batch Loss train: 0.07805761694908142\n",
            "i 133\n",
            "epoch 32\n",
            " batch Loss train: 0.06536082923412323\n",
            "i 134\n",
            "epoch 32\n",
            " batch Loss train: 0.07604434341192245\n",
            "i 135\n",
            "epoch 32\n",
            " batch Loss train: 0.06562749296426773\n",
            "i 136\n",
            "epoch 32\n",
            " batch Loss train: 0.08217006176710129\n",
            "i 137\n",
            "epoch 32\n",
            " batch Loss train: 0.12025342136621475\n",
            "i 138\n",
            "epoch 32\n",
            " batch Loss train: 0.07905419170856476\n",
            "i 139\n",
            "epoch 32\n",
            " batch Loss train: 0.06860760599374771\n",
            "i 140\n",
            "epoch 32\n",
            " batch Loss train: 0.06299275159835815\n",
            "i 141\n",
            "epoch 32\n",
            " batch Loss train: 0.07913190126419067\n",
            "i 142\n",
            "epoch 32\n",
            " batch Loss train: 0.08727877587080002\n",
            "i 143\n",
            "epoch 32\n",
            " batch Loss train: 0.07411060482263565\n",
            "i 144\n",
            "epoch 32\n",
            " batch Loss train: 0.09686798602342606\n",
            "i 145\n",
            "epoch 32\n",
            " batch Loss train: 0.0647653266787529\n",
            "i 146\n",
            "epoch 32\n",
            " batch Loss train: 0.09552189707756042\n",
            "i 147\n",
            "epoch 32\n",
            " batch Loss train: 0.08279752731323242\n",
            "i 148\n",
            "epoch 32\n",
            " batch Loss train: 0.08331739157438278\n",
            "i 149\n",
            "epoch 32\n",
            " batch Loss train: 0.0817294716835022\n",
            "i 150\n",
            "epoch 32\n",
            " batch Loss train: 0.09455212205648422\n",
            "i 151\n",
            "epoch 32\n",
            " batch Loss train: 0.1067846342921257\n",
            "i 152\n",
            "epoch 32\n",
            " batch Loss train: 0.07144469767808914\n",
            "i 153\n",
            "epoch 32\n",
            " batch Loss train: 0.09978089481592178\n",
            "i 154\n",
            "epoch 32\n",
            " batch Loss train: 0.0717897117137909\n",
            "i 155\n",
            "epoch 32\n",
            " batch Loss train: 0.09304869920015335\n",
            "i 156\n",
            "epoch 32\n",
            " batch Loss train: 0.06891639530658722\n",
            "i 157\n",
            "epoch 32\n",
            " batch Loss train: 0.07339262962341309\n",
            "i 158\n",
            "epoch 32\n",
            " batch Loss train: 0.08146752417087555\n",
            "i 159\n",
            "epoch 32\n",
            " batch Loss train: 0.10040708631277084\n",
            "i 160\n",
            "epoch 32\n",
            " batch Loss train: 0.09221121668815613\n",
            "i 161\n",
            "epoch 32\n",
            " batch Loss train: 0.08257852494716644\n",
            "i 162\n",
            "epoch 32\n",
            " batch Loss train: 0.09398198127746582\n",
            "i 163\n",
            "epoch 32\n",
            " batch Loss train: 0.08543609082698822\n",
            "i 164\n",
            "epoch 32\n",
            " batch Loss train: 0.07485134899616241\n",
            "i 165\n",
            "epoch 32\n",
            " batch Loss train: 0.09411878138780594\n",
            "i 166\n",
            "epoch 32\n",
            " batch Loss train: 0.08769950270652771\n",
            "i 167\n",
            "epoch 32\n",
            " batch Loss train: 0.10877703875303268\n",
            "i 168\n",
            "epoch 32\n",
            " batch Loss train: 0.11285658180713654\n",
            "i 169\n",
            "epoch 32\n",
            " batch Loss train: 0.06188816577196121\n",
            "i 170\n",
            "epoch 32\n",
            " batch Loss train: 0.06294728070497513\n",
            "i 171\n",
            "epoch 32\n",
            " batch Loss train: 0.08076643943786621\n",
            "i 172\n",
            "epoch 32\n",
            " batch Loss train: 0.0825270488858223\n",
            "i 173\n",
            "epoch 32\n",
            " batch Loss train: 0.07473780959844589\n",
            "i 174\n",
            "epoch 32\n",
            " batch Loss train: 0.10531962662935257\n",
            "i 175\n",
            "epoch 32\n",
            " batch Loss train: 0.07810167968273163\n",
            "i 176\n",
            "epoch 32\n",
            " batch Loss train: 0.09296638518571854\n",
            "i 177\n",
            "epoch 32\n",
            " batch Loss train: 0.10004845261573792\n",
            "i 178\n",
            "epoch 32\n",
            " batch Loss train: 0.09808321297168732\n",
            "i 179\n",
            "epoch 32\n",
            " batch Loss train: 0.07489091902971268\n",
            "i 180\n",
            "epoch 32\n",
            " batch Loss train: 0.09908275306224823\n",
            "i 181\n",
            "epoch 32\n",
            " batch Loss train: 0.06739749014377594\n",
            "i 182\n",
            "epoch 32\n",
            " batch Loss train: 0.08457192778587341\n",
            "i 183\n",
            "epoch 32\n",
            " batch Loss train: 0.09065219014883041\n",
            "i 184\n",
            "epoch 32\n",
            " batch Loss train: 0.09536803513765335\n",
            "i 185\n",
            "epoch 32\n",
            " batch Loss train: 0.0753152072429657\n",
            "i 186\n",
            "epoch 32\n",
            " batch Loss train: 0.07533103972673416\n",
            "i 187\n",
            "epoch 32\n",
            " batch Loss train: 0.06811501830816269\n",
            "i 188\n",
            "epoch 32\n",
            " batch Loss train: 0.10360287874937057\n",
            "i 189\n",
            "epoch 32\n",
            " batch Loss train: 0.08590962737798691\n",
            "i 190\n",
            "epoch 32\n",
            " batch Loss train: 0.08530969172716141\n",
            "i 191\n",
            "epoch 32\n",
            " batch Loss train: 0.08181317895650864\n",
            "i 192\n",
            "epoch 32\n",
            " batch Loss train: 0.088070347905159\n",
            "i 193\n",
            "epoch 32\n",
            " batch Loss train: 0.0749380961060524\n",
            "i 194\n",
            "epoch 32\n",
            " batch Loss train: 0.08572429418563843\n",
            "i 195\n",
            "epoch 32\n",
            " batch Loss train: 0.07745175063610077\n",
            "i 196\n",
            "epoch 32\n",
            " batch Loss train: 0.09236536920070648\n",
            "i 197\n",
            "epoch 32\n",
            " batch Loss train: 0.10190797597169876\n",
            "i 198\n",
            "epoch 32\n",
            " batch Loss train: 0.09091085195541382\n",
            "i 199\n",
            "epoch 32\n",
            " batch Loss train: 0.08069941401481628\n",
            "i 200\n",
            "epoch 32\n",
            " batch Loss train: 0.08868317306041718\n",
            "i 201\n",
            "epoch 32\n",
            " batch Loss train: 0.08350454270839691\n",
            "i 202\n",
            "epoch 32\n",
            " batch Loss train: 0.07289934903383255\n",
            "i 203\n",
            "epoch 32\n",
            " batch Loss train: 0.07485299557447433\n",
            "i 204\n",
            "epoch 32\n",
            " batch Loss train: 0.10228589177131653\n",
            "i 205\n",
            "epoch 32\n",
            " batch Loss train: 0.08951324969530106\n",
            "i 206\n",
            "epoch 32\n",
            " batch Loss train: 0.08170625567436218\n",
            "i 207\n",
            "epoch 32\n",
            " batch Loss train: 0.06728750467300415\n",
            "i 208\n",
            "epoch 32\n",
            " batch Loss train: 0.09485649317502975\n",
            "i 209\n",
            "epoch 32\n",
            " batch Loss train: 0.10408788174390793\n",
            "i 210\n",
            "epoch 32\n",
            " batch Loss train: 0.07786037027835846\n",
            "i 211\n",
            "epoch 32\n",
            " batch Loss train: 0.0779958963394165\n",
            "i 212\n",
            "epoch 32\n",
            " batch Loss train: 0.11647871136665344\n",
            "i 213\n",
            "epoch 32\n",
            " batch Loss train: 0.07401332259178162\n",
            "i 214\n",
            "epoch 32\n",
            " batch Loss train: 0.08858053386211395\n",
            "i 215\n",
            "epoch 32\n",
            " batch Loss train: 0.08932085335254669\n",
            "i 216\n",
            "epoch 32\n",
            " batch Loss train: 0.07615497708320618\n",
            "i 217\n",
            "epoch 32\n",
            " batch Loss train: 0.080318383872509\n",
            "i 218\n",
            "epoch 32\n",
            " batch Loss train: 0.08207860589027405\n",
            "i 219\n",
            "epoch 32\n",
            " batch Loss train: 0.0737568736076355\n",
            "i 220\n",
            "epoch 32\n",
            " batch Loss train: 0.08777651190757751\n",
            "i 221\n",
            "epoch 32\n",
            " batch Loss train: 0.11063393950462341\n",
            "i 222\n",
            "epoch 32\n",
            " batch Loss train: 0.09792175889015198\n",
            "i 223\n",
            "epoch 32\n",
            " batch Loss train: 0.10827287286520004\n",
            "i 224\n",
            "epoch 32\n",
            " batch Loss train: 0.09679979830980301\n",
            "i 225\n",
            "epoch 32\n",
            " batch Loss train: 0.12015512585639954\n",
            "i 226\n",
            "epoch 32\n",
            " batch Loss train: 0.0942702516913414\n",
            "i 227\n",
            "epoch 32\n",
            " batch Loss train: 0.07439480721950531\n",
            "i 228\n",
            "epoch 32\n",
            " batch Loss train: 0.10020922124385834\n",
            "i 229\n",
            "epoch 32\n",
            " batch Loss train: 0.08383481204509735\n",
            "i 230\n",
            "epoch 32\n",
            " batch Loss train: 0.08436430245637894\n",
            "i 231\n",
            "epoch 32\n",
            " batch Loss train: 0.09666360169649124\n",
            "i 232\n",
            "epoch 32\n",
            " batch Loss train: 0.08842422813177109\n",
            "i 233\n",
            "epoch 32\n",
            " batch Loss train: 0.09797497093677521\n",
            "i 234\n",
            "epoch 32\n",
            " batch Loss train: 0.1126820370554924\n",
            "i 235\n",
            "epoch 32\n",
            " batch Loss train: 0.10827188193798065\n",
            "i 236\n",
            "epoch 32\n",
            " batch Loss train: 0.09206349402666092\n",
            "i 237\n",
            "epoch 32\n",
            " batch Loss train: 0.09638668596744537\n",
            "i 238\n",
            "epoch 32\n",
            " batch Loss train: 0.07380124926567078\n",
            "i 239\n",
            "epoch 32\n",
            " batch Loss train: 0.09273688495159149\n",
            "i 240\n",
            "epoch 32\n",
            " batch Loss train: 0.09326096624135971\n",
            "i 241\n",
            "epoch 32\n",
            " batch Loss train: 0.09312032908201218\n",
            "i 242\n",
            "epoch 32\n",
            " batch Loss train: 0.10242225974798203\n",
            "i 243\n",
            "epoch 32\n",
            " batch Loss train: 0.0996587797999382\n",
            "i 244\n",
            "epoch 32\n",
            " batch Loss train: 0.10383101552724838\n",
            "i 245\n",
            "epoch 32\n",
            " batch Loss train: 0.07417584955692291\n",
            "i 246\n",
            "epoch 32\n",
            " batch Loss train: 0.12000390142202377\n",
            "i 247\n",
            "epoch 32\n",
            " batch Loss train: 0.09953607618808746\n",
            "i 248\n",
            "epoch 32\n",
            " batch Loss train: 0.07488453388214111\n",
            "i 249\n",
            "epoch 32\n",
            " batch Loss train: 0.1292552798986435\n",
            "i 250\n",
            "epoch 32\n",
            " batch Loss train: 0.12076061218976974\n",
            "i 251\n",
            "epoch 32\n",
            " batch Loss train: 0.06351375579833984\n",
            "i 252\n",
            "epoch 32\n",
            " batch Loss train: 0.12012845277786255\n",
            "i 253\n",
            "epoch 32\n",
            " batch Loss train: 0.09002905339002609\n",
            "i 254\n",
            "epoch 32\n",
            " batch Loss train: 0.10235075652599335\n",
            "i 255\n",
            "epoch 32\n",
            " batch Loss train: 0.08163957297801971\n",
            "i 256\n",
            "epoch 32\n",
            " batch Loss train: 0.0760188177227974\n",
            "i 257\n",
            "epoch 32\n",
            " batch Loss train: 0.10484948754310608\n",
            "i 258\n",
            "epoch 32\n",
            " batch Loss train: 0.06358412653207779\n",
            "i 259\n",
            "epoch 32\n",
            " batch Loss train: 0.09620903432369232\n",
            "i 260\n",
            "epoch 32\n",
            " batch Loss train: 0.08526542782783508\n",
            "i 261\n",
            "epoch 32\n",
            " batch Loss train: 0.10545752197504044\n",
            "i 262\n",
            "epoch 32\n",
            " batch Loss train: 0.08429516851902008\n",
            "i 263\n",
            "epoch 32\n",
            " batch Loss train: 0.07142911106348038\n",
            "i 264\n",
            "epoch 32\n",
            " batch Loss train: 0.14183460175991058\n",
            "i 265\n",
            "epoch 32\n",
            " batch Loss train: 0.09345387667417526\n",
            "i 266\n",
            "epoch 32\n",
            " batch Loss train: 0.09511248022317886\n",
            "i 267\n",
            "epoch 32\n",
            " batch Loss train: 0.08586015552282333\n",
            "i 268\n",
            "epoch 32\n",
            " batch Loss train: 0.09149223566055298\n",
            "i 269\n",
            "epoch 32\n",
            " batch Loss train: 0.0737299919128418\n",
            "i 270\n",
            "epoch 32\n",
            " batch Loss train: 0.09621547162532806\n",
            "i 271\n",
            "epoch 32\n",
            " batch Loss train: 0.08443606644868851\n",
            "i 272\n",
            "epoch 32\n",
            " batch Loss train: 0.10288619250059128\n",
            "i 273\n",
            "epoch 32\n",
            " batch Loss train: 0.10103140771389008\n",
            "i 274\n",
            "epoch 32\n",
            " batch Loss train: 0.09496712684631348\n",
            "i 275\n",
            "epoch 32\n",
            " batch Loss train: 0.10224422812461853\n",
            "i 276\n",
            "epoch 32\n",
            " batch Loss train: 0.07582546770572662\n",
            "i 277\n",
            "epoch 32\n",
            " batch Loss train: 0.08971501141786575\n",
            "i 278\n",
            "epoch 32\n",
            " batch Loss train: 0.0920310765504837\n",
            "i 279\n",
            "epoch 32\n",
            " batch Loss train: 0.12041100114583969\n",
            "i 280\n",
            "epoch 32\n",
            " batch Loss train: 0.071318618953228\n",
            "i 281\n",
            "epoch 32\n",
            " batch Loss train: 0.10104119777679443\n",
            "i 282\n",
            "epoch 32\n",
            " batch Loss train: 0.07889293134212494\n",
            "i 283\n",
            "epoch 32\n",
            " batch Loss train: 0.10601464658975601\n",
            "i 284\n",
            "epoch 32\n",
            " batch Loss train: 0.0698409155011177\n",
            "i 285\n",
            "epoch 32\n",
            " batch Loss train: 0.1369347721338272\n",
            "i 286\n",
            "epoch 32\n",
            " batch Loss train: 0.13948029279708862\n",
            "i 287\n",
            "epoch 32\n",
            " batch Loss train: 0.08420227468013763\n",
            "i 288\n",
            "epoch 32\n",
            " batch Loss train: 0.09675972163677216\n",
            "i 289\n",
            "epoch 32\n",
            " batch Loss train: 0.0901675596833229\n",
            "i 290\n",
            "epoch 32\n",
            " batch Loss train: 0.0724136233329773\n",
            "i 291\n",
            "epoch 32\n",
            " batch Loss train: 0.07712540775537491\n",
            "i 292\n",
            "epoch 32\n",
            " batch Loss train: 0.07862438261508942\n",
            "i 293\n",
            "epoch 32\n",
            " batch Loss train: 0.09232420474290848\n",
            "i 294\n",
            "epoch 32\n",
            " batch Loss train: 0.08846448361873627\n",
            "i 295\n",
            "epoch 32\n",
            " batch Loss train: 0.09767688810825348\n",
            "i 296\n",
            "epoch 32\n",
            " batch Loss train: 0.08022081851959229\n",
            "i 297\n",
            "epoch 32\n",
            " batch Loss train: 0.12158801406621933\n",
            "i 298\n",
            "epoch 32\n",
            " batch Loss train: 0.08808861672878265\n",
            "i 299\n",
            "epoch 32\n",
            " batch Loss train: 0.09579151123762131\n",
            "i 300\n",
            "epoch 32\n",
            " batch Loss train: 0.07185790687799454\n",
            "i 301\n",
            "epoch 32\n",
            " batch Loss train: 0.10643497854471207\n",
            "i 302\n",
            "epoch 32\n",
            " batch Loss train: 0.07476593554019928\n",
            "i 303\n",
            "epoch 32\n",
            " batch Loss train: 0.09675107896327972\n",
            "i 304\n",
            "epoch 32\n",
            " batch Loss train: 0.0792248398065567\n",
            "i 305\n",
            "epoch 32\n",
            " batch Loss train: 0.09772521257400513\n",
            "i 306\n",
            "epoch 32\n",
            " batch Loss train: 0.07348345220088959\n",
            "i 307\n",
            "epoch 32\n",
            " batch Loss train: 0.08167741447687149\n",
            "i 308\n",
            "epoch 32\n",
            " batch Loss train: 0.09912300109863281\n",
            "i 309\n",
            "epoch 32\n",
            " batch Loss train: 0.09313030540943146\n",
            "i 310\n",
            "epoch 32\n",
            " batch Loss train: 0.10122232884168625\n",
            "i 311\n",
            "epoch 32\n",
            " batch Loss train: 0.07625456899404526\n",
            "i 312\n",
            "epoch 32\n",
            " batch Loss train: 0.12596875429153442\n",
            "i 313\n",
            "epoch 32\n",
            " batch Loss train: 0.10017837584018707\n",
            "i 314\n",
            "epoch 32\n",
            " batch Loss train: 0.10099944472312927\n",
            "i 315\n",
            "epoch 32\n",
            " batch Loss train: 0.08530709892511368\n",
            "i 316\n",
            "epoch 32\n",
            " batch Loss train: 0.07916934043169022\n",
            "i 317\n",
            "epoch 32\n",
            " batch Loss train: 0.1204218715429306\n",
            "i 318\n",
            "epoch 32\n",
            " batch Loss train: 0.08226706832647324\n",
            "i 319\n",
            "epoch 32\n",
            " batch Loss train: 0.11286767572164536\n",
            "i 320\n",
            "epoch 32\n",
            " batch Loss train: 0.09481699764728546\n",
            "i 321\n",
            "epoch 32\n",
            " batch Loss train: 0.131462961435318\n",
            "i 322\n",
            "epoch 32\n",
            " batch Loss train: 0.10979066789150238\n",
            "i 323\n",
            "epoch 32\n",
            " batch Loss train: 0.1009884923696518\n",
            "i 324\n",
            "epoch 32\n",
            " batch Loss train: 0.09493441879749298\n",
            "i 325\n",
            "epoch 32\n",
            " batch Loss train: 0.09943634271621704\n",
            "i 326\n",
            "epoch 32\n",
            " batch Loss train: 0.1025201603770256\n",
            "i 327\n",
            "epoch 32\n",
            " batch Loss train: 0.09120834618806839\n",
            "i 328\n",
            "epoch 32\n",
            " batch Loss train: 0.07172790169715881\n",
            "i 329\n",
            "epoch 32\n",
            " batch Loss train: 0.09020894765853882\n",
            "i 330\n",
            "epoch 32\n",
            " batch Loss train: 0.09018216282129288\n",
            "i 331\n",
            "epoch 32\n",
            " batch Loss train: 0.08499789983034134\n",
            "i 332\n",
            "epoch 32\n",
            " batch Loss train: 0.08450203388929367\n",
            "i 333\n",
            "epoch 32\n",
            " batch Loss train: 0.12415716797113419\n",
            "i 334\n",
            "epoch 32\n",
            " batch Loss train: 0.08693195134401321\n",
            "i 335\n",
            "epoch 32\n",
            " batch Loss train: 0.10402413457632065\n",
            "i 336\n",
            "epoch 32\n",
            " batch Loss train: 0.08969699591398239\n",
            "i 337\n",
            "epoch 32\n",
            " batch Loss train: 0.11091968417167664\n",
            "i 338\n",
            "epoch 32\n",
            " batch Loss train: 0.08984958380460739\n",
            "i 339\n",
            "epoch 32\n",
            " batch Loss train: 0.09269711375236511\n",
            "i 340\n",
            "epoch 32\n",
            " batch Loss train: 0.07010259479284286\n",
            "i 341\n",
            "epoch 32\n",
            " batch Loss train: 0.08068768680095673\n",
            "i 342\n",
            "epoch 32\n",
            " batch Loss train: 0.07352588325738907\n",
            "i 343\n",
            "epoch 32\n",
            " batch Loss train: 0.08992581814527512\n",
            "i 344\n",
            "epoch 32\n",
            " batch Loss train: 0.06284531950950623\n",
            "i 345\n",
            "epoch 32\n",
            " batch Loss train: 0.08518951386213303\n",
            "i 346\n",
            "epoch 32\n",
            " batch Loss train: 0.10752367973327637\n",
            "i 347\n",
            "epoch 32\n",
            " batch Loss train: 0.10645206272602081\n",
            "i 348\n",
            "epoch 32\n",
            " batch Loss train: 0.10185367614030838\n",
            "i 349\n",
            "epoch 32\n",
            " batch Loss train: 0.10498026758432388\n",
            "i 350\n",
            "epoch 32\n",
            " batch Loss train: 0.08648648113012314\n",
            "i 351\n",
            "epoch 32\n",
            " batch Loss train: 0.09362391382455826\n",
            "i 352\n",
            "epoch 32\n",
            " batch Loss train: 0.08558440953493118\n",
            "i 353\n",
            "epoch 32\n",
            " batch Loss train: 0.08458419144153595\n",
            "i 354\n",
            "epoch 32\n",
            " batch Loss train: 0.09083854407072067\n",
            "i 355\n",
            "epoch 32\n",
            " batch Loss train: 0.06362231075763702\n",
            "i 356\n",
            "epoch 32\n",
            " batch Loss train: 0.09380488842725754\n",
            "i 357\n",
            "epoch 32\n",
            " batch Loss train: 0.10118382424116135\n",
            "i 358\n",
            "epoch 32\n",
            " batch Loss train: 0.08909589052200317\n",
            "i 359\n",
            "epoch 32\n",
            " batch Loss train: 0.06529652327299118\n",
            "i 360\n",
            "epoch 32\n",
            " batch Loss train: 0.08770765364170074\n",
            "i 361\n",
            "epoch 32\n",
            " batch Loss train: 0.08967942744493484\n",
            "i 362\n",
            "epoch 32\n",
            " batch Loss train: 0.09142383188009262\n",
            "i 363\n",
            "epoch 32\n",
            " batch Loss train: 0.09559614211320877\n",
            "i 364\n",
            "epoch 32\n",
            " batch Loss train: 0.07662124186754227\n",
            "i 365\n",
            "epoch 32\n",
            " batch Loss train: 0.1156395971775055\n",
            "i 366\n",
            "epoch 32\n",
            " batch Loss train: 0.08557992428541183\n",
            "i 367\n",
            "epoch 32\n",
            " batch Loss train: 0.0994177833199501\n",
            "i 368\n",
            "epoch 32\n",
            " batch Loss train: 0.10585468262434006\n",
            "i 369\n",
            "epoch 32\n",
            " batch Loss train: 0.09220997989177704\n",
            "i 370\n",
            "epoch 32\n",
            " batch Loss train: 0.08841431885957718\n",
            "i 371\n",
            "epoch 32\n",
            " batch Loss train: 0.09084893018007278\n",
            "i 372\n",
            "epoch 32\n",
            " batch Loss train: 0.07821304351091385\n",
            "i 373\n",
            "epoch 32\n",
            " batch Loss train: 0.07057243585586548\n",
            "i 374\n",
            "epoch 32\n",
            " batch Loss train: 0.08950173109769821\n",
            "i 375\n",
            "epoch 32\n",
            " batch Loss train: 0.1128108873963356\n",
            "i 376\n",
            "epoch 32\n",
            " batch Loss train: 0.07643112540245056\n",
            "i 377\n",
            "epoch 32\n",
            " batch Loss train: 0.11266927421092987\n",
            "i 378\n",
            "epoch 32\n",
            " batch Loss train: 0.0975337028503418\n",
            "i 379\n",
            "epoch 32\n",
            " batch Loss train: 0.09504128992557526\n",
            "i 380\n",
            "epoch 32\n",
            " batch Loss train: 0.09019917994737625\n",
            "i 381\n",
            "epoch 32\n",
            " batch Loss train: 0.07502157241106033\n",
            "i 382\n",
            "epoch 32\n",
            " batch Loss train: 0.14056740701198578\n",
            "i 383\n",
            "epoch 32\n",
            " batch Loss train: 0.09552168846130371\n",
            "i 384\n",
            "epoch 32\n",
            " batch Loss train: 0.11541134119033813\n",
            "i 385\n",
            "epoch 32\n",
            " batch Loss train: 0.09203258156776428\n",
            "i 386\n",
            "epoch 32\n",
            " batch Loss train: 0.09409868717193604\n",
            "i 387\n",
            "epoch 32\n",
            " batch Loss train: 0.13168667256832123\n",
            "i 388\n",
            "epoch 32\n",
            " batch Loss train: 0.09575732797384262\n",
            "i 389\n",
            "epoch 32\n",
            " batch Loss train: 0.10207612812519073\n",
            "i 390\n",
            "epoch 32\n",
            " batch Loss train: 0.0817619115114212\n",
            "i 391\n",
            "epoch 32\n",
            " batch Loss train: 0.08552537858486176\n",
            "i 392\n",
            "epoch 32\n",
            " batch Loss train: 0.12231352925300598\n",
            "i 393\n",
            "epoch 32\n",
            " batch Loss train: 0.0777793601155281\n",
            "i 394\n",
            "epoch 32\n",
            " batch Loss train: 0.11776097863912582\n",
            "i 395\n",
            "epoch 32\n",
            " batch Loss train: 0.09146995097398758\n",
            "i 396\n",
            "epoch 32\n",
            " batch Loss train: 0.10442006587982178\n",
            "i 397\n",
            "epoch 32\n",
            " batch Loss train: 0.08583825081586838\n",
            "i 398\n",
            "epoch 32\n",
            " batch Loss train: 0.11368419975042343\n",
            "i 399\n",
            "epoch 32\n",
            " batch Loss train: 0.09405961632728577\n",
            "i 400\n",
            "epoch 32\n",
            " batch Loss train: 0.09085395932197571\n",
            "i 401\n",
            "epoch 32\n",
            " batch Loss train: 0.1267797201871872\n",
            "i 402\n",
            "epoch 32\n",
            " batch Loss train: 0.12474202364683151\n",
            "i 403\n",
            "epoch 32\n",
            " batch Loss train: 0.08321636915206909\n",
            "i 404\n",
            "epoch 32\n",
            " batch Loss train: 0.09862654656171799\n",
            "i 405\n",
            "epoch 32\n",
            " batch Loss train: 0.1024618148803711\n",
            "i 406\n",
            "epoch 32\n",
            " batch Loss train: 0.09980061650276184\n",
            "i 407\n",
            "epoch 32\n",
            " batch Loss train: 0.09683772921562195\n",
            "i 408\n",
            "epoch 32\n",
            " batch Loss train: 0.08687952160835266\n",
            "i 409\n",
            "epoch 32\n",
            " batch Loss train: 0.09095574915409088\n",
            "i 410\n",
            "epoch 32\n",
            " batch Loss train: 0.10057216137647629\n",
            "i 411\n",
            "epoch 32\n",
            " batch Loss train: 0.1374504566192627\n",
            "i 412\n",
            "epoch 32\n",
            " batch Loss train: 0.14124923944473267\n",
            "i 413\n",
            "epoch 32\n",
            " batch Loss train: 0.13884851336479187\n",
            "i 414\n",
            "epoch 32\n",
            " batch Loss train: 0.11889161169528961\n",
            "i 415\n",
            "epoch 32\n",
            " batch Loss train: 0.07247031480073929\n",
            "i 416\n",
            "epoch 32\n",
            " batch Loss train: 0.07706133276224136\n",
            "i 417\n",
            "epoch 32\n",
            " batch Loss train: 0.09914904087781906\n",
            "i 418\n",
            "epoch 32\n",
            " batch Loss train: 0.11489470303058624\n",
            "i 419\n",
            "epoch 32\n",
            " batch Loss train: 0.14257287979125977\n",
            "i 420\n",
            "epoch 32\n",
            " batch Loss train: 0.09608355164527893\n",
            "i 421\n",
            "epoch 32\n",
            " batch Loss train: 0.08501099050045013\n",
            "i 422\n",
            "epoch 32\n",
            " batch Loss train: 0.1026424914598465\n",
            "i 423\n",
            "epoch 32\n",
            " batch Loss train: 0.11652714759111404\n",
            "i 424\n",
            "epoch 32\n",
            " batch Loss train: 0.08624037355184555\n",
            "i 425\n",
            "epoch 32\n",
            " batch Loss train: 0.12040676176548004\n",
            "i 426\n",
            "epoch 32\n",
            " batch Loss train: 0.080448217689991\n",
            "i 427\n",
            "epoch 32\n",
            " batch Loss train: 0.09455596655607224\n",
            "i 428\n",
            "epoch 32\n",
            " batch Loss train: 0.08228468894958496\n",
            "i 429\n",
            "epoch 32\n",
            " batch Loss train: 0.09560185670852661\n",
            "i 430\n",
            "epoch 32\n",
            " batch Loss train: 0.07913323491811752\n",
            "i 431\n",
            "epoch 32\n",
            " batch Loss train: 0.07719796895980835\n",
            "i 432\n",
            "epoch 32\n",
            " batch Loss train: 0.07564858347177505\n",
            "i 433\n",
            "epoch 32\n",
            " batch Loss train: 0.11078538000583649\n",
            "i 434\n",
            "epoch 32\n",
            " batch Loss train: 0.0908154845237732\n",
            "i 435\n",
            "epoch 32\n",
            " batch Loss train: 0.10740776360034943\n",
            "i 436\n",
            "epoch 32\n",
            " batch Loss train: 0.0910506397485733\n",
            "i 437\n",
            "epoch 32\n",
            " batch Loss train: 0.10910961776971817\n",
            "i 438\n",
            "epoch 32\n",
            " batch Loss train: 0.11856289207935333\n",
            "i 439\n",
            "epoch 32\n",
            " batch Loss train: 0.11332173645496368\n",
            "i 440\n",
            "epoch 32\n",
            " batch Loss train: 0.09568651765584946\n",
            "i 441\n",
            "epoch 32\n",
            " batch Loss train: 0.07185184210538864\n",
            "i 442\n",
            "epoch 32\n",
            " batch Loss train: 0.1037570983171463\n",
            "i 443\n",
            "epoch 32\n",
            " batch Loss train: 0.10849292576313019\n",
            "i 444\n",
            "epoch 32\n",
            " batch Loss train: 0.09102651476860046\n",
            "i 445\n",
            "epoch 32\n",
            " batch Loss train: 0.10407233983278275\n",
            "total epoch Loss train: tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 33\n",
            " batch Loss train: 0.06717199832201004\n",
            "i 1\n",
            "epoch 33\n",
            " batch Loss train: 0.06759019196033478\n",
            "i 2\n",
            "epoch 33\n",
            " batch Loss train: 0.05883698910474777\n",
            "i 3\n",
            "epoch 33\n",
            " batch Loss train: 0.08244083821773529\n",
            "i 4\n",
            "epoch 33\n",
            " batch Loss train: 0.08043181896209717\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 33\n",
            " batch Loss train: 0.07035703212022781\n",
            "i 6\n",
            "epoch 33\n",
            " batch Loss train: 0.09517643600702286\n",
            "i 7\n",
            "epoch 33\n",
            " batch Loss train: 0.06125008687376976\n",
            "i 8\n",
            "epoch 33\n",
            " batch Loss train: 0.11154302209615707\n",
            "i 9\n",
            "epoch 33\n",
            " batch Loss train: 0.05982832983136177\n",
            "i 10\n",
            "epoch 33\n",
            " batch Loss train: 0.06329385936260223\n",
            "i 11\n",
            "epoch 33\n",
            " batch Loss train: 0.09048189967870712\n",
            "i 12\n",
            "epoch 33\n",
            " batch Loss train: 0.10175181180238724\n",
            "i 13\n",
            "epoch 33\n",
            " batch Loss train: 0.08373771607875824\n",
            "i 14\n",
            "epoch 33\n",
            " batch Loss train: 0.06867142021656036\n",
            "i 15\n",
            "epoch 33\n",
            " batch Loss train: 0.06980077922344208\n",
            "i 16\n",
            "epoch 33\n",
            " batch Loss train: 0.07705359905958176\n",
            "i 17\n",
            "epoch 33\n",
            " batch Loss train: 0.07749844342470169\n",
            "i 18\n",
            "epoch 33\n",
            " batch Loss train: 0.1000826507806778\n",
            "i 19\n",
            "epoch 33\n",
            " batch Loss train: 0.08914860337972641\n",
            "i 20\n",
            "epoch 33\n",
            " batch Loss train: 0.0997583270072937\n",
            "i 21\n",
            "epoch 33\n",
            " batch Loss train: 0.08659034222364426\n",
            "i 22\n",
            "epoch 33\n",
            " batch Loss train: 0.08600091189146042\n",
            "i 23\n",
            "epoch 33\n",
            " batch Loss train: 0.0907941609621048\n",
            "i 24\n",
            "epoch 33\n",
            " batch Loss train: 0.09195747971534729\n",
            "i 25\n",
            "epoch 33\n",
            " batch Loss train: 0.06460922211408615\n",
            "i 26\n",
            "epoch 33\n",
            " batch Loss train: 0.06749926507472992\n",
            "i 27\n",
            "epoch 33\n",
            " batch Loss train: 0.0673309713602066\n",
            "i 28\n",
            "epoch 33\n",
            " batch Loss train: 0.08312039822340012\n",
            "i 29\n",
            "epoch 33\n",
            " batch Loss train: 0.0770101323723793\n",
            "i 30\n",
            "epoch 33\n",
            " batch Loss train: 0.060750674456357956\n",
            "i 31\n",
            "epoch 33\n",
            " batch Loss train: 0.11166290938854218\n",
            "i 32\n",
            "epoch 33\n",
            " batch Loss train: 0.08489972352981567\n",
            "i 33\n",
            "epoch 33\n",
            " batch Loss train: 0.08269504457712173\n",
            "i 34\n",
            "epoch 33\n",
            " batch Loss train: 0.08178950846195221\n",
            "i 35\n",
            "epoch 33\n",
            " batch Loss train: 0.08327840268611908\n",
            "i 36\n",
            "epoch 33\n",
            " batch Loss train: 0.05311805009841919\n",
            "i 37\n",
            "epoch 33\n",
            " batch Loss train: 0.06433682888746262\n",
            "i 38\n",
            "epoch 33\n",
            " batch Loss train: 0.06979110091924667\n",
            "i 39\n",
            "epoch 33\n",
            " batch Loss train: 0.07458344846963882\n",
            "i 40\n",
            "epoch 33\n",
            " batch Loss train: 0.06459519267082214\n",
            "i 41\n",
            "epoch 33\n",
            " batch Loss train: 0.11618608236312866\n",
            "i 42\n",
            "epoch 33\n",
            " batch Loss train: 0.07432809472084045\n",
            "i 43\n",
            "epoch 33\n",
            " batch Loss train: 0.08821061253547668\n",
            "i 44\n",
            "epoch 33\n",
            " batch Loss train: 0.07872182875871658\n",
            "i 45\n",
            "epoch 33\n",
            " batch Loss train: 0.10425868630409241\n",
            "i 46\n",
            "epoch 33\n",
            " batch Loss train: 0.08392155170440674\n",
            "i 47\n",
            "epoch 33\n",
            " batch Loss train: 0.06166395917534828\n",
            "i 48\n",
            "epoch 33\n",
            " batch Loss train: 0.10585203766822815\n",
            "i 49\n",
            "epoch 33\n",
            " batch Loss train: 0.06832975894212723\n",
            "i 50\n",
            "epoch 33\n",
            " batch Loss train: 0.08427413552999496\n",
            "i 51\n",
            "epoch 33\n",
            " batch Loss train: 0.0880795419216156\n",
            "i 52\n",
            "epoch 33\n",
            " batch Loss train: 0.07864086329936981\n",
            "i 53\n",
            "epoch 33\n",
            " batch Loss train: 0.07791022211313248\n",
            "i 54\n",
            "epoch 33\n",
            " batch Loss train: 0.09772662818431854\n",
            "i 55\n",
            "epoch 33\n",
            " batch Loss train: 0.09064661711454391\n",
            "i 56\n",
            "epoch 33\n",
            " batch Loss train: 0.11529333889484406\n",
            "i 57\n",
            "epoch 33\n",
            " batch Loss train: 0.05919381603598595\n",
            "i 58\n",
            "epoch 33\n",
            " batch Loss train: 0.07529003918170929\n",
            "i 59\n",
            "epoch 33\n",
            " batch Loss train: 0.10893324762582779\n",
            "i 60\n",
            "epoch 33\n",
            " batch Loss train: 0.09595220535993576\n",
            "i 61\n",
            "epoch 33\n",
            " batch Loss train: 0.0793440192937851\n",
            "i 62\n",
            "epoch 33\n",
            " batch Loss train: 0.07326318323612213\n",
            "i 63\n",
            "epoch 33\n",
            " batch Loss train: 0.07904279232025146\n",
            "i 64\n",
            "epoch 33\n",
            " batch Loss train: 0.06724757701158524\n",
            "i 65\n",
            "epoch 33\n",
            " batch Loss train: 0.0868082270026207\n",
            "i 66\n",
            "epoch 33\n",
            " batch Loss train: 0.06433477997779846\n",
            "i 67\n",
            "epoch 33\n",
            " batch Loss train: 0.087267205119133\n",
            "i 68\n",
            "epoch 33\n",
            " batch Loss train: 0.07794082909822464\n",
            "i 69\n",
            "epoch 33\n",
            " batch Loss train: 0.060429833829402924\n",
            "i 70\n",
            "epoch 33\n",
            " batch Loss train: 0.0630892962217331\n",
            "i 71\n",
            "epoch 33\n",
            " batch Loss train: 0.07182075083255768\n",
            "i 72\n",
            "epoch 33\n",
            " batch Loss train: 0.08057492226362228\n",
            "i 73\n",
            "epoch 33\n",
            " batch Loss train: 0.0841832086443901\n",
            "i 74\n",
            "epoch 33\n",
            " batch Loss train: 0.0802839919924736\n",
            "i 75\n",
            "epoch 33\n",
            " batch Loss train: 0.09322968870401382\n",
            "i 76\n",
            "epoch 33\n",
            " batch Loss train: 0.056700270622968674\n",
            "i 77\n",
            "epoch 33\n",
            " batch Loss train: 0.09511156380176544\n",
            "i 78\n",
            "epoch 33\n",
            " batch Loss train: 0.09052640199661255\n",
            "i 79\n",
            "epoch 33\n",
            " batch Loss train: 0.08491255342960358\n",
            "i 80\n",
            "epoch 33\n",
            " batch Loss train: 0.07617345452308655\n",
            "i 81\n",
            "epoch 33\n",
            " batch Loss train: 0.09430358558893204\n",
            "i 82\n",
            "epoch 33\n",
            " batch Loss train: 0.059259746223688126\n",
            "i 83\n",
            "epoch 33\n",
            " batch Loss train: 0.08074504137039185\n",
            "i 84\n",
            "epoch 33\n",
            " batch Loss train: 0.08852247148752213\n",
            "i 85\n",
            "epoch 33\n",
            " batch Loss train: 0.09973070025444031\n",
            "i 86\n",
            "epoch 33\n",
            " batch Loss train: 0.1007208377122879\n",
            "i 87\n",
            "epoch 33\n",
            " batch Loss train: 0.07277772575616837\n",
            "i 88\n",
            "epoch 33\n",
            " batch Loss train: 0.08396206796169281\n",
            "i 89\n",
            "epoch 33\n",
            " batch Loss train: 0.09237129986286163\n",
            "i 90\n",
            "epoch 33\n",
            " batch Loss train: 0.09085612744092941\n",
            "i 91\n",
            "epoch 33\n",
            " batch Loss train: 0.060210756957530975\n",
            "i 92\n",
            "epoch 33\n",
            " batch Loss train: 0.09049152582883835\n",
            "i 93\n",
            "epoch 33\n",
            " batch Loss train: 0.10689227283000946\n",
            "i 94\n",
            "epoch 33\n",
            " batch Loss train: 0.0850626602768898\n",
            "i 95\n",
            "epoch 33\n",
            " batch Loss train: 0.09046412259340286\n",
            "i 96\n",
            "epoch 33\n",
            " batch Loss train: 0.08327175676822662\n",
            "i 97\n",
            "epoch 33\n",
            " batch Loss train: 0.08491738885641098\n",
            "i 98\n",
            "epoch 33\n",
            " batch Loss train: 0.05779310688376427\n",
            "i 99\n",
            "epoch 33\n",
            " batch Loss train: 0.09641118347644806\n",
            "i 100\n",
            "epoch 33\n",
            " batch Loss train: 0.07025782018899918\n",
            "i 101\n",
            "epoch 33\n",
            " batch Loss train: 0.11047224700450897\n",
            "i 102\n",
            "epoch 33\n",
            " batch Loss train: 0.08089552819728851\n",
            "i 103\n",
            "epoch 33\n",
            " batch Loss train: 0.086323082447052\n",
            "i 104\n",
            "epoch 33\n",
            " batch Loss train: 0.0896042212843895\n",
            "i 105\n",
            "epoch 33\n",
            " batch Loss train: 0.0747721865773201\n",
            "i 106\n",
            "epoch 33\n",
            " batch Loss train: 0.0729084312915802\n",
            "i 107\n",
            "epoch 33\n",
            " batch Loss train: 0.07936796545982361\n",
            "i 108\n",
            "epoch 33\n",
            " batch Loss train: 0.06650856137275696\n",
            "i 109\n",
            "epoch 33\n",
            " batch Loss train: 0.06901829689741135\n",
            "i 110\n",
            "epoch 33\n",
            " batch Loss train: 0.07844313979148865\n",
            "i 111\n",
            "epoch 33\n",
            " batch Loss train: 0.06565500795841217\n",
            "i 112\n",
            "epoch 33\n",
            " batch Loss train: 0.05546731874346733\n",
            "i 113\n",
            "epoch 33\n",
            " batch Loss train: 0.11988917738199234\n",
            "i 114\n",
            "epoch 33\n",
            " batch Loss train: 0.08755398541688919\n",
            "i 115\n",
            "epoch 33\n",
            " batch Loss train: 0.06519337743520737\n",
            "i 116\n",
            "epoch 33\n",
            " batch Loss train: 0.10306122899055481\n",
            "i 117\n",
            "epoch 33\n",
            " batch Loss train: 0.09436264634132385\n",
            "i 118\n",
            "epoch 33\n",
            " batch Loss train: 0.06539811939001083\n",
            "i 119\n",
            "epoch 33\n",
            " batch Loss train: 0.07628238201141357\n",
            "i 120\n",
            "epoch 33\n",
            " batch Loss train: 0.07643939554691315\n",
            "i 121\n",
            "epoch 33\n",
            " batch Loss train: 0.06597085297107697\n",
            "i 122\n",
            "epoch 33\n",
            " batch Loss train: 0.08429692685604095\n",
            "i 123\n",
            "epoch 33\n",
            " batch Loss train: 0.07524143159389496\n",
            "i 124\n",
            "epoch 33\n",
            " batch Loss train: 0.07214442640542984\n",
            "i 125\n",
            "epoch 33\n",
            " batch Loss train: 0.06360352039337158\n",
            "i 126\n",
            "epoch 33\n",
            " batch Loss train: 0.07602234929800034\n",
            "i 127\n",
            "epoch 33\n",
            " batch Loss train: 0.0715828537940979\n",
            "i 128\n",
            "epoch 33\n",
            " batch Loss train: 0.08189785480499268\n",
            "i 129\n",
            "epoch 33\n",
            " batch Loss train: 0.09291509538888931\n",
            "i 130\n",
            "epoch 33\n",
            " batch Loss train: 0.06128527969121933\n",
            "i 131\n",
            "epoch 33\n",
            " batch Loss train: 0.0658048689365387\n",
            "i 132\n",
            "epoch 33\n",
            " batch Loss train: 0.09679585695266724\n",
            "i 133\n",
            "epoch 33\n",
            " batch Loss train: 0.08815310895442963\n",
            "i 134\n",
            "epoch 33\n",
            " batch Loss train: 0.08175584673881531\n",
            "i 135\n",
            "epoch 33\n",
            " batch Loss train: 0.07457729429006577\n",
            "i 136\n",
            "epoch 33\n",
            " batch Loss train: 0.06311699002981186\n",
            "i 137\n",
            "epoch 33\n",
            " batch Loss train: 0.07876650243997574\n",
            "i 138\n",
            "epoch 33\n",
            " batch Loss train: 0.06712330877780914\n",
            "i 139\n",
            "epoch 33\n",
            " batch Loss train: 0.096208855509758\n",
            "i 140\n",
            "epoch 33\n",
            " batch Loss train: 0.07401355355978012\n",
            "i 141\n",
            "epoch 33\n",
            " batch Loss train: 0.08505254238843918\n",
            "i 142\n",
            "epoch 33\n",
            " batch Loss train: 0.06819865852594376\n",
            "i 143\n",
            "epoch 33\n",
            " batch Loss train: 0.05917287617921829\n",
            "i 144\n",
            "epoch 33\n",
            " batch Loss train: 0.10039957612752914\n",
            "i 145\n",
            "epoch 33\n",
            " batch Loss train: 0.08873119205236435\n",
            "i 146\n",
            "epoch 33\n",
            " batch Loss train: 0.07335171848535538\n",
            "i 147\n",
            "epoch 33\n",
            " batch Loss train: 0.09980249404907227\n",
            "i 148\n",
            "epoch 33\n",
            " batch Loss train: 0.08362662047147751\n",
            "i 149\n",
            "epoch 33\n",
            " batch Loss train: 0.06441861391067505\n",
            "i 150\n",
            "epoch 33\n",
            " batch Loss train: 0.06676705181598663\n",
            "i 151\n",
            "epoch 33\n",
            " batch Loss train: 0.07838178426027298\n",
            "i 152\n",
            "epoch 33\n",
            " batch Loss train: 0.07341597229242325\n",
            "i 153\n",
            "epoch 33\n",
            " batch Loss train: 0.07602602988481522\n",
            "i 154\n",
            "epoch 33\n",
            " batch Loss train: 0.07632835954427719\n",
            "i 155\n",
            "epoch 33\n",
            " batch Loss train: 0.09907714277505875\n",
            "i 156\n",
            "epoch 33\n",
            " batch Loss train: 0.07036640495061874\n",
            "i 157\n",
            "epoch 33\n",
            " batch Loss train: 0.06838686764240265\n",
            "i 158\n",
            "epoch 33\n",
            " batch Loss train: 0.1156001091003418\n",
            "i 159\n",
            "epoch 33\n",
            " batch Loss train: 0.07115476578474045\n",
            "i 160\n",
            "epoch 33\n",
            " batch Loss train: 0.06111612915992737\n",
            "i 161\n",
            "epoch 33\n",
            " batch Loss train: 0.11093774437904358\n",
            "i 162\n",
            "epoch 33\n",
            " batch Loss train: 0.07337980717420578\n",
            "i 163\n",
            "epoch 33\n",
            " batch Loss train: 0.08991678804159164\n",
            "i 164\n",
            "epoch 33\n",
            " batch Loss train: 0.10767802596092224\n",
            "i 165\n",
            "epoch 33\n",
            " batch Loss train: 0.09036629647016525\n",
            "i 166\n",
            "epoch 33\n",
            " batch Loss train: 0.07181001454591751\n",
            "i 167\n",
            "epoch 33\n",
            " batch Loss train: 0.06606435030698776\n",
            "i 168\n",
            "epoch 33\n",
            " batch Loss train: 0.13991859555244446\n",
            "i 169\n",
            "epoch 33\n",
            " batch Loss train: 0.06961452960968018\n",
            "i 170\n",
            "epoch 33\n",
            " batch Loss train: 0.0836949348449707\n",
            "i 171\n",
            "epoch 33\n",
            " batch Loss train: 0.13270309567451477\n",
            "i 172\n",
            "epoch 33\n",
            " batch Loss train: 0.07676801085472107\n",
            "i 173\n",
            "epoch 33\n",
            " batch Loss train: 0.08050765842199326\n",
            "i 174\n",
            "epoch 33\n",
            " batch Loss train: 0.09414321929216385\n",
            "i 175\n",
            "epoch 33\n",
            " batch Loss train: 0.12228365987539291\n",
            "i 176\n",
            "epoch 33\n",
            " batch Loss train: 0.09984203428030014\n",
            "i 177\n",
            "epoch 33\n",
            " batch Loss train: 0.10296543687582016\n",
            "i 178\n",
            "epoch 33\n",
            " batch Loss train: 0.08281110227108002\n",
            "i 179\n",
            "epoch 33\n",
            " batch Loss train: 0.08570943027734756\n",
            "i 180\n",
            "epoch 33\n",
            " batch Loss train: 0.08872977644205093\n",
            "i 181\n",
            "epoch 33\n",
            " batch Loss train: 0.08011650294065475\n",
            "i 182\n",
            "epoch 33\n",
            " batch Loss train: 0.08355246484279633\n",
            "i 183\n",
            "epoch 33\n",
            " batch Loss train: 0.059388190507888794\n",
            "i 184\n",
            "epoch 33\n",
            " batch Loss train: 0.06816723197698593\n",
            "i 185\n",
            "epoch 33\n",
            " batch Loss train: 0.09613198041915894\n",
            "i 186\n",
            "epoch 33\n",
            " batch Loss train: 0.08357930183410645\n",
            "i 187\n",
            "epoch 33\n",
            " batch Loss train: 0.10585017502307892\n",
            "i 188\n",
            "epoch 33\n",
            " batch Loss train: 0.07598063349723816\n",
            "i 189\n",
            "epoch 33\n",
            " batch Loss train: 0.07743822783231735\n",
            "i 190\n",
            "epoch 33\n",
            " batch Loss train: 0.096391461789608\n",
            "i 191\n",
            "epoch 33\n",
            " batch Loss train: 0.07315340638160706\n",
            "i 192\n",
            "epoch 33\n",
            " batch Loss train: 0.09992048144340515\n",
            "i 193\n",
            "epoch 33\n",
            " batch Loss train: 0.09261354058980942\n",
            "i 194\n",
            "epoch 33\n",
            " batch Loss train: 0.09342870116233826\n",
            "i 195\n",
            "epoch 33\n",
            " batch Loss train: 0.13441914319992065\n",
            "i 196\n",
            "epoch 33\n",
            " batch Loss train: 0.0878305658698082\n",
            "i 197\n",
            "epoch 33\n",
            " batch Loss train: 0.06961073726415634\n",
            "i 198\n",
            "epoch 33\n",
            " batch Loss train: 0.09658940136432648\n",
            "i 199\n",
            "epoch 33\n",
            " batch Loss train: 0.053262386471033096\n",
            "i 200\n",
            "epoch 33\n",
            " batch Loss train: 0.09559591859579086\n",
            "i 201\n",
            "epoch 33\n",
            " batch Loss train: 0.10933714359998703\n",
            "i 202\n",
            "epoch 33\n",
            " batch Loss train: 0.10523118823766708\n",
            "i 203\n",
            "epoch 33\n",
            " batch Loss train: 0.07927806675434113\n",
            "i 204\n",
            "epoch 33\n",
            " batch Loss train: 0.08888747543096542\n",
            "i 205\n",
            "epoch 33\n",
            " batch Loss train: 0.10069314390420914\n",
            "i 206\n",
            "epoch 33\n",
            " batch Loss train: 0.09832416474819183\n",
            "i 207\n",
            "epoch 33\n",
            " batch Loss train: 0.07773186266422272\n",
            "i 208\n",
            "epoch 33\n",
            " batch Loss train: 0.06482485681772232\n",
            "i 209\n",
            "epoch 33\n",
            " batch Loss train: 0.08645402640104294\n",
            "i 210\n",
            "epoch 33\n",
            " batch Loss train: 0.08109448105096817\n",
            "i 211\n",
            "epoch 33\n",
            " batch Loss train: 0.07645025104284286\n",
            "i 212\n",
            "epoch 33\n",
            " batch Loss train: 0.06691687554121017\n",
            "i 213\n",
            "epoch 33\n",
            " batch Loss train: 0.11567576974630356\n",
            "i 214\n",
            "epoch 33\n",
            " batch Loss train: 0.1093321219086647\n",
            "i 215\n",
            "epoch 33\n",
            " batch Loss train: 0.07486579567193985\n",
            "i 216\n",
            "epoch 33\n",
            " batch Loss train: 0.0710623636841774\n",
            "i 217\n",
            "epoch 33\n",
            " batch Loss train: 0.0773218423128128\n",
            "i 218\n",
            "epoch 33\n",
            " batch Loss train: 0.07629048824310303\n",
            "i 219\n",
            "epoch 33\n",
            " batch Loss train: 0.09840951859951019\n",
            "i 220\n",
            "epoch 33\n",
            " batch Loss train: 0.07050659507513046\n",
            "i 221\n",
            "epoch 33\n",
            " batch Loss train: 0.07772679626941681\n",
            "i 222\n",
            "epoch 33\n",
            " batch Loss train: 0.09694063663482666\n",
            "i 223\n",
            "epoch 33\n",
            " batch Loss train: 0.08497952669858932\n",
            "i 224\n",
            "epoch 33\n",
            " batch Loss train: 0.09870882332324982\n",
            "i 225\n",
            "epoch 33\n",
            " batch Loss train: 0.08435549587011337\n",
            "i 226\n",
            "epoch 33\n",
            " batch Loss train: 0.09772251546382904\n",
            "i 227\n",
            "epoch 33\n",
            " batch Loss train: 0.11084688454866409\n",
            "i 228\n",
            "epoch 33\n",
            " batch Loss train: 0.10606783628463745\n",
            "i 229\n",
            "epoch 33\n",
            " batch Loss train: 0.12292473763227463\n",
            "i 230\n",
            "epoch 33\n",
            " batch Loss train: 0.0875805914402008\n",
            "i 231\n",
            "epoch 33\n",
            " batch Loss train: 0.0761338397860527\n",
            "i 232\n",
            "epoch 33\n",
            " batch Loss train: 0.08595946431159973\n",
            "i 233\n",
            "epoch 33\n",
            " batch Loss train: 0.06273481249809265\n",
            "i 234\n",
            "epoch 33\n",
            " batch Loss train: 0.08722085505723953\n",
            "i 235\n",
            "epoch 33\n",
            " batch Loss train: 0.07095833122730255\n",
            "i 236\n",
            "epoch 33\n",
            " batch Loss train: 0.10067898035049438\n",
            "i 237\n",
            "epoch 33\n",
            " batch Loss train: 0.10670296102762222\n",
            "i 238\n",
            "epoch 33\n",
            " batch Loss train: 0.07941465079784393\n",
            "i 239\n",
            "epoch 33\n",
            " batch Loss train: 0.09728890657424927\n",
            "i 240\n",
            "epoch 33\n",
            " batch Loss train: 0.07253925502300262\n",
            "i 241\n",
            "epoch 33\n",
            " batch Loss train: 0.07726722955703735\n",
            "i 242\n",
            "epoch 33\n",
            " batch Loss train: 0.11826495081186295\n",
            "i 243\n",
            "epoch 33\n",
            " batch Loss train: 0.08690650016069412\n",
            "i 244\n",
            "epoch 33\n",
            " batch Loss train: 0.07382669299840927\n",
            "i 245\n",
            "epoch 33\n",
            " batch Loss train: 0.08223365247249603\n",
            "i 246\n",
            "epoch 33\n",
            " batch Loss train: 0.08028893917798996\n",
            "i 247\n",
            "epoch 33\n",
            " batch Loss train: 0.08834280073642731\n",
            "i 248\n",
            "epoch 33\n",
            " batch Loss train: 0.08953168988227844\n",
            "i 249\n",
            "epoch 33\n",
            " batch Loss train: 0.10619378089904785\n",
            "i 250\n",
            "epoch 33\n",
            " batch Loss train: 0.06940163671970367\n",
            "i 251\n",
            "epoch 33\n",
            " batch Loss train: 0.09629634022712708\n",
            "i 252\n",
            "epoch 33\n",
            " batch Loss train: 0.11499318480491638\n",
            "i 253\n",
            "epoch 33\n",
            " batch Loss train: 0.07359278947114944\n",
            "i 254\n",
            "epoch 33\n",
            " batch Loss train: 0.0843581035733223\n",
            "i 255\n",
            "epoch 33\n",
            " batch Loss train: 0.09759887307882309\n",
            "i 256\n",
            "epoch 33\n",
            " batch Loss train: 0.09379015117883682\n",
            "i 257\n",
            "epoch 33\n",
            " batch Loss train: 0.11155951023101807\n",
            "i 258\n",
            "epoch 33\n",
            " batch Loss train: 0.07172433286905289\n",
            "i 259\n",
            "epoch 33\n",
            " batch Loss train: 0.09193265438079834\n",
            "i 260\n",
            "epoch 33\n",
            " batch Loss train: 0.07009319216012955\n",
            "i 261\n",
            "epoch 33\n",
            " batch Loss train: 0.08462370187044144\n",
            "i 262\n",
            "epoch 33\n",
            " batch Loss train: 0.11578842252492905\n",
            "i 263\n",
            "epoch 33\n",
            " batch Loss train: 0.09869126975536346\n",
            "i 264\n",
            "epoch 33\n",
            " batch Loss train: 0.07372065633535385\n",
            "i 265\n",
            "epoch 33\n",
            " batch Loss train: 0.07967803627252579\n",
            "i 266\n",
            "epoch 33\n",
            " batch Loss train: 0.06231626123189926\n",
            "i 267\n",
            "epoch 33\n",
            " batch Loss train: 0.08297205716371536\n",
            "i 268\n",
            "epoch 33\n",
            " batch Loss train: 0.0701732411980629\n",
            "i 269\n",
            "epoch 33\n",
            " batch Loss train: 0.09040270000696182\n",
            "i 270\n",
            "epoch 33\n",
            " batch Loss train: 0.0731046199798584\n",
            "i 271\n",
            "epoch 33\n",
            " batch Loss train: 0.08309394866228104\n",
            "i 272\n",
            "epoch 33\n",
            " batch Loss train: 0.09561737626791\n",
            "i 273\n",
            "epoch 33\n",
            " batch Loss train: 0.10678957402706146\n",
            "i 274\n",
            "epoch 33\n",
            " batch Loss train: 0.09431386739015579\n",
            "i 275\n",
            "epoch 33\n",
            " batch Loss train: 0.09399279952049255\n",
            "i 276\n",
            "epoch 33\n",
            " batch Loss train: 0.07240262627601624\n",
            "i 277\n",
            "epoch 33\n",
            " batch Loss train: 0.09861750900745392\n",
            "i 278\n",
            "epoch 33\n",
            " batch Loss train: 0.08484502881765366\n",
            "i 279\n",
            "epoch 33\n",
            " batch Loss train: 0.09254392236471176\n",
            "i 280\n",
            "epoch 33\n",
            " batch Loss train: 0.0682862177491188\n",
            "i 281\n",
            "epoch 33\n",
            " batch Loss train: 0.07442493736743927\n",
            "i 282\n",
            "epoch 33\n",
            " batch Loss train: 0.06176194176077843\n",
            "i 283\n",
            "epoch 33\n",
            " batch Loss train: 0.08622496575117111\n",
            "i 284\n",
            "epoch 33\n",
            " batch Loss train: 0.07299759238958359\n",
            "i 285\n",
            "epoch 33\n",
            " batch Loss train: 0.09141113609075546\n",
            "i 286\n",
            "epoch 33\n",
            " batch Loss train: 0.07158088684082031\n",
            "i 287\n",
            "epoch 33\n",
            " batch Loss train: 0.10371959954500198\n",
            "i 288\n",
            "epoch 33\n",
            " batch Loss train: 0.09907266497612\n",
            "i 289\n",
            "epoch 33\n",
            " batch Loss train: 0.08240663260221481\n",
            "i 290\n",
            "epoch 33\n",
            " batch Loss train: 0.08929881453514099\n",
            "i 291\n",
            "epoch 33\n",
            " batch Loss train: 0.09749799966812134\n",
            "i 292\n",
            "epoch 33\n",
            " batch Loss train: 0.0562240295112133\n",
            "i 293\n",
            "epoch 33\n",
            " batch Loss train: 0.10733313113451004\n",
            "i 294\n",
            "epoch 33\n",
            " batch Loss train: 0.07240533828735352\n",
            "i 295\n",
            "epoch 33\n",
            " batch Loss train: 0.07048971205949783\n",
            "i 296\n",
            "epoch 33\n",
            " batch Loss train: 0.06949086487293243\n",
            "i 297\n",
            "epoch 33\n",
            " batch Loss train: 0.07328469306230545\n",
            "i 298\n",
            "epoch 33\n",
            " batch Loss train: 0.08402163535356522\n",
            "i 299\n",
            "epoch 33\n",
            " batch Loss train: 0.08198951184749603\n",
            "i 300\n",
            "epoch 33\n",
            " batch Loss train: 0.09522852301597595\n",
            "i 301\n",
            "epoch 33\n",
            " batch Loss train: 0.07266800105571747\n",
            "i 302\n",
            "epoch 33\n",
            " batch Loss train: 0.08016876131296158\n",
            "i 303\n",
            "epoch 33\n",
            " batch Loss train: 0.07798614352941513\n",
            "i 304\n",
            "epoch 33\n",
            " batch Loss train: 0.11784082651138306\n",
            "i 305\n",
            "epoch 33\n",
            " batch Loss train: 0.07581669092178345\n",
            "i 306\n",
            "epoch 33\n",
            " batch Loss train: 0.08473450690507889\n",
            "i 307\n",
            "epoch 33\n",
            " batch Loss train: 0.0709376111626625\n",
            "i 308\n",
            "epoch 33\n",
            " batch Loss train: 0.11069153994321823\n",
            "i 309\n",
            "epoch 33\n",
            " batch Loss train: 0.10956692695617676\n",
            "i 310\n",
            "epoch 33\n",
            " batch Loss train: 0.10462899506092072\n",
            "i 311\n",
            "epoch 33\n",
            " batch Loss train: 0.07104480266571045\n",
            "i 312\n",
            "epoch 33\n",
            " batch Loss train: 0.07577315717935562\n",
            "i 313\n",
            "epoch 33\n",
            " batch Loss train: 0.08861707150936127\n",
            "i 314\n",
            "epoch 33\n",
            " batch Loss train: 0.0969964936375618\n",
            "i 315\n",
            "epoch 33\n",
            " batch Loss train: 0.07452808320522308\n",
            "i 316\n",
            "epoch 33\n",
            " batch Loss train: 0.06988514959812164\n",
            "i 317\n",
            "epoch 33\n",
            " batch Loss train: 0.0767626091837883\n",
            "i 318\n",
            "epoch 33\n",
            " batch Loss train: 0.0847611278295517\n",
            "i 319\n",
            "epoch 33\n",
            " batch Loss train: 0.07996279001235962\n",
            "i 320\n",
            "epoch 33\n",
            " batch Loss train: 0.08759115636348724\n",
            "i 321\n",
            "epoch 33\n",
            " batch Loss train: 0.11901295930147171\n",
            "i 322\n",
            "epoch 33\n",
            " batch Loss train: 0.09435564279556274\n",
            "i 323\n",
            "epoch 33\n",
            " batch Loss train: 0.07485481351613998\n",
            "i 324\n",
            "epoch 33\n",
            " batch Loss train: 0.0785331055521965\n",
            "i 325\n",
            "epoch 33\n",
            " batch Loss train: 0.08307477086782455\n",
            "i 326\n",
            "epoch 33\n",
            " batch Loss train: 0.0914936512708664\n",
            "i 327\n",
            "epoch 33\n",
            " batch Loss train: 0.09315332025289536\n",
            "i 328\n",
            "epoch 33\n",
            " batch Loss train: 0.06271052360534668\n",
            "i 329\n",
            "epoch 33\n",
            " batch Loss train: 0.064223513007164\n",
            "i 330\n",
            "epoch 33\n",
            " batch Loss train: 0.09621772915124893\n",
            "i 331\n",
            "epoch 33\n",
            " batch Loss train: 0.08509369939565659\n",
            "i 332\n",
            "epoch 33\n",
            " batch Loss train: 0.09246265143156052\n",
            "i 333\n",
            "epoch 33\n",
            " batch Loss train: 0.08402810245752335\n",
            "i 334\n",
            "epoch 33\n",
            " batch Loss train: 0.08297723531723022\n",
            "i 335\n",
            "epoch 33\n",
            " batch Loss train: 0.06859041005373001\n",
            "i 336\n",
            "epoch 33\n",
            " batch Loss train: 0.07973720878362656\n",
            "i 337\n",
            "epoch 33\n",
            " batch Loss train: 0.06932540982961655\n",
            "i 338\n",
            "epoch 33\n",
            " batch Loss train: 0.08656685799360275\n",
            "i 339\n",
            "epoch 33\n",
            " batch Loss train: 0.06370292603969574\n",
            "i 340\n",
            "epoch 33\n",
            " batch Loss train: 0.11757848411798477\n",
            "i 341\n",
            "epoch 33\n",
            " batch Loss train: 0.09108755737543106\n",
            "i 342\n",
            "epoch 33\n",
            " batch Loss train: 0.07723166793584824\n",
            "i 343\n",
            "epoch 33\n",
            " batch Loss train: 0.10025879740715027\n",
            "i 344\n",
            "epoch 33\n",
            " batch Loss train: 0.09469374269247055\n",
            "i 345\n",
            "epoch 33\n",
            " batch Loss train: 0.07397084683179855\n",
            "i 346\n",
            "epoch 33\n",
            " batch Loss train: 0.08129499107599258\n",
            "i 347\n",
            "epoch 33\n",
            " batch Loss train: 0.09745307266712189\n",
            "i 348\n",
            "epoch 33\n",
            " batch Loss train: 0.07220250368118286\n",
            "i 349\n",
            "epoch 33\n",
            " batch Loss train: 0.06733536720275879\n",
            "i 350\n",
            "epoch 33\n",
            " batch Loss train: 0.08188784122467041\n",
            "i 351\n",
            "epoch 33\n",
            " batch Loss train: 0.1003907322883606\n",
            "i 352\n",
            "epoch 33\n",
            " batch Loss train: 0.10312454402446747\n",
            "i 353\n",
            "epoch 33\n",
            " batch Loss train: 0.09254612773656845\n",
            "i 354\n",
            "epoch 33\n",
            " batch Loss train: 0.0889897346496582\n",
            "i 355\n",
            "epoch 33\n",
            " batch Loss train: 0.08001257479190826\n",
            "i 356\n",
            "epoch 33\n",
            " batch Loss train: 0.09340800344944\n",
            "i 357\n",
            "epoch 33\n",
            " batch Loss train: 0.08641767501831055\n",
            "i 358\n",
            "epoch 33\n",
            " batch Loss train: 0.10871299356222153\n",
            "i 359\n",
            "epoch 33\n",
            " batch Loss train: 0.1029583215713501\n",
            "i 360\n",
            "epoch 33\n",
            " batch Loss train: 0.09117116779088974\n",
            "i 361\n",
            "epoch 33\n",
            " batch Loss train: 0.07440222054719925\n",
            "i 362\n",
            "epoch 33\n",
            " batch Loss train: 0.10842625796794891\n",
            "i 363\n",
            "epoch 33\n",
            " batch Loss train: 0.05945509672164917\n",
            "i 364\n",
            "epoch 33\n",
            " batch Loss train: 0.07204996049404144\n",
            "i 365\n",
            "epoch 33\n",
            " batch Loss train: 0.07281091809272766\n",
            "i 366\n",
            "epoch 33\n",
            " batch Loss train: 0.07133640348911285\n",
            "i 367\n",
            "epoch 33\n",
            " batch Loss train: 0.09116693586111069\n",
            "i 368\n",
            "epoch 33\n",
            " batch Loss train: 0.09588123112916946\n",
            "i 369\n",
            "epoch 33\n",
            " batch Loss train: 0.0716191828250885\n",
            "i 370\n",
            "epoch 33\n",
            " batch Loss train: 0.09139122068881989\n",
            "i 371\n",
            "epoch 33\n",
            " batch Loss train: 0.09554263204336166\n",
            "i 372\n",
            "epoch 33\n",
            " batch Loss train: 0.08478309214115143\n",
            "i 373\n",
            "epoch 33\n",
            " batch Loss train: 0.08862113207578659\n",
            "i 374\n",
            "epoch 33\n",
            " batch Loss train: 0.08152564615011215\n",
            "i 375\n",
            "epoch 33\n",
            " batch Loss train: 0.07089041918516159\n",
            "i 376\n",
            "epoch 33\n",
            " batch Loss train: 0.11225012689828873\n",
            "i 377\n",
            "epoch 33\n",
            " batch Loss train: 0.09786678850650787\n",
            "i 378\n",
            "epoch 33\n",
            " batch Loss train: 0.07284678518772125\n",
            "i 379\n",
            "epoch 33\n",
            " batch Loss train: 0.10931931436061859\n",
            "i 380\n",
            "epoch 33\n",
            " batch Loss train: 0.07621721923351288\n",
            "i 381\n",
            "epoch 33\n",
            " batch Loss train: 0.08980942517518997\n",
            "i 382\n",
            "epoch 33\n",
            " batch Loss train: 0.08977070450782776\n",
            "i 383\n",
            "epoch 33\n",
            " batch Loss train: 0.10565224289894104\n",
            "i 384\n",
            "epoch 33\n",
            " batch Loss train: 0.1078042984008789\n",
            "i 385\n",
            "epoch 33\n",
            " batch Loss train: 0.07767383754253387\n",
            "i 386\n",
            "epoch 33\n",
            " batch Loss train: 0.08400031924247742\n",
            "i 387\n",
            "epoch 33\n",
            " batch Loss train: 0.10081519186496735\n",
            "i 388\n",
            "epoch 33\n",
            " batch Loss train: 0.10907520353794098\n",
            "i 389\n",
            "epoch 33\n",
            " batch Loss train: 0.11629489809274673\n",
            "i 390\n",
            "epoch 33\n",
            " batch Loss train: 0.07648306339979172\n",
            "i 391\n",
            "epoch 33\n",
            " batch Loss train: 0.08164303004741669\n",
            "i 392\n",
            "epoch 33\n",
            " batch Loss train: 0.08250530064105988\n",
            "i 393\n",
            "epoch 33\n",
            " batch Loss train: 0.09784025698900223\n",
            "i 394\n",
            "epoch 33\n",
            " batch Loss train: 0.05853646621108055\n",
            "i 395\n",
            "epoch 33\n",
            " batch Loss train: 0.0771879106760025\n",
            "i 396\n",
            "epoch 33\n",
            " batch Loss train: 0.08996694535017014\n",
            "i 397\n",
            "epoch 33\n",
            " batch Loss train: 0.07946737855672836\n",
            "i 398\n",
            "epoch 33\n",
            " batch Loss train: 0.10215543955564499\n",
            "i 399\n",
            "epoch 33\n",
            " batch Loss train: 0.06532604247331619\n",
            "i 400\n",
            "epoch 33\n",
            " batch Loss train: 0.08412948250770569\n",
            "i 401\n",
            "epoch 33\n",
            " batch Loss train: 0.09482528269290924\n",
            "i 402\n",
            "epoch 33\n",
            " batch Loss train: 0.07787402719259262\n",
            "i 403\n",
            "epoch 33\n",
            " batch Loss train: 0.10233570635318756\n",
            "i 404\n",
            "epoch 33\n",
            " batch Loss train: 0.07671108841896057\n",
            "i 405\n",
            "epoch 33\n",
            " batch Loss train: 0.11110437661409378\n",
            "i 406\n",
            "epoch 33\n",
            " batch Loss train: 0.08682376891374588\n",
            "i 407\n",
            "epoch 33\n",
            " batch Loss train: 0.09111666679382324\n",
            "i 408\n",
            "epoch 33\n",
            " batch Loss train: 0.10852117836475372\n",
            "i 409\n",
            "epoch 33\n",
            " batch Loss train: 0.083135224878788\n",
            "i 410\n",
            "epoch 33\n",
            " batch Loss train: 0.08484195172786713\n",
            "i 411\n",
            "epoch 33\n",
            " batch Loss train: 0.08110145479440689\n",
            "i 412\n",
            "epoch 33\n",
            " batch Loss train: 0.09321188181638718\n",
            "i 413\n",
            "epoch 33\n",
            " batch Loss train: 0.0795358195900917\n",
            "i 414\n",
            "epoch 33\n",
            " batch Loss train: 0.09035693854093552\n",
            "i 415\n",
            "epoch 33\n",
            " batch Loss train: 0.06770951300859451\n",
            "i 416\n",
            "epoch 33\n",
            " batch Loss train: 0.09946584701538086\n",
            "i 417\n",
            "epoch 33\n",
            " batch Loss train: 0.06792142242193222\n",
            "i 418\n",
            "epoch 33\n",
            " batch Loss train: 0.10056519508361816\n",
            "i 419\n",
            "epoch 33\n",
            " batch Loss train: 0.08990414440631866\n",
            "i 420\n",
            "epoch 33\n",
            " batch Loss train: 0.07750330865383148\n",
            "i 421\n",
            "epoch 33\n",
            " batch Loss train: 0.08238288760185242\n",
            "i 422\n",
            "epoch 33\n",
            " batch Loss train: 0.0742989256978035\n",
            "i 423\n",
            "epoch 33\n",
            " batch Loss train: 0.08524207025766373\n",
            "i 424\n",
            "epoch 33\n",
            " batch Loss train: 0.09375428408384323\n",
            "i 425\n",
            "epoch 33\n",
            " batch Loss train: 0.07564841955900192\n",
            "i 426\n",
            "epoch 33\n",
            " batch Loss train: 0.0950789824128151\n",
            "i 427\n",
            "epoch 33\n",
            " batch Loss train: 0.10015036910772324\n",
            "i 428\n",
            "epoch 33\n",
            " batch Loss train: 0.08563114702701569\n",
            "i 429\n",
            "epoch 33\n",
            " batch Loss train: 0.09601155668497086\n",
            "i 430\n",
            "epoch 33\n",
            " batch Loss train: 0.08139946311712265\n",
            "i 431\n",
            "epoch 33\n",
            " batch Loss train: 0.10278822481632233\n",
            "i 432\n",
            "epoch 33\n",
            " batch Loss train: 0.08190260082483292\n",
            "i 433\n",
            "epoch 33\n",
            " batch Loss train: 0.08485545217990875\n",
            "i 434\n",
            "epoch 33\n",
            " batch Loss train: 0.07398950308561325\n",
            "i 435\n",
            "epoch 33\n",
            " batch Loss train: 0.07524466514587402\n",
            "i 436\n",
            "epoch 33\n",
            " batch Loss train: 0.08473954349756241\n",
            "i 437\n",
            "epoch 33\n",
            " batch Loss train: 0.09065386652946472\n",
            "i 438\n",
            "epoch 33\n",
            " batch Loss train: 0.08885251730680466\n",
            "i 439\n",
            "epoch 33\n",
            " batch Loss train: 0.071223184466362\n",
            "i 440\n",
            "epoch 33\n",
            " batch Loss train: 0.08546636998653412\n",
            "i 441\n",
            "epoch 33\n",
            " batch Loss train: 0.08328832685947418\n",
            "i 442\n",
            "epoch 33\n",
            " batch Loss train: 0.08367020636796951\n",
            "i 443\n",
            "epoch 33\n",
            " batch Loss train: 0.07931896299123764\n",
            "i 444\n",
            "epoch 33\n",
            " batch Loss train: 0.10269161313772202\n",
            "i 445\n",
            "epoch 33\n",
            " batch Loss train: 0.10169979929924011\n",
            "total epoch Loss train: tensor(0.1017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 34\n",
            " batch Loss train: 0.060751836746931076\n",
            "i 1\n",
            "epoch 34\n",
            " batch Loss train: 0.06784088164567947\n",
            "i 2\n",
            "epoch 34\n",
            " batch Loss train: 0.07790353894233704\n",
            "i 3\n",
            "epoch 34\n",
            " batch Loss train: 0.06887543201446533\n",
            "i 4\n",
            "epoch 34\n",
            " batch Loss train: 0.07623832672834396\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 34\n",
            " batch Loss train: 0.06812302768230438\n",
            "i 6\n",
            "epoch 34\n",
            " batch Loss train: 0.10237010568380356\n",
            "i 7\n",
            "epoch 34\n",
            " batch Loss train: 0.07496677339076996\n",
            "i 8\n",
            "epoch 34\n",
            " batch Loss train: 0.07787665724754333\n",
            "i 9\n",
            "epoch 34\n",
            " batch Loss train: 0.06829310953617096\n",
            "i 10\n",
            "epoch 34\n",
            " batch Loss train: 0.05871625244617462\n",
            "i 11\n",
            "epoch 34\n",
            " batch Loss train: 0.06006461754441261\n",
            "i 12\n",
            "epoch 34\n",
            " batch Loss train: 0.09151437133550644\n",
            "i 13\n",
            "epoch 34\n",
            " batch Loss train: 0.07959397882223129\n",
            "i 14\n",
            "epoch 34\n",
            " batch Loss train: 0.08132600039243698\n",
            "i 15\n",
            "epoch 34\n",
            " batch Loss train: 0.06263109296560287\n",
            "i 16\n",
            "epoch 34\n",
            " batch Loss train: 0.08824190497398376\n",
            "i 17\n",
            "epoch 34\n",
            " batch Loss train: 0.07496140897274017\n",
            "i 18\n",
            "epoch 34\n",
            " batch Loss train: 0.060784414410591125\n",
            "i 19\n",
            "epoch 34\n",
            " batch Loss train: 0.05418098717927933\n",
            "i 20\n",
            "epoch 34\n",
            " batch Loss train: 0.08555647730827332\n",
            "i 21\n",
            "epoch 34\n",
            " batch Loss train: 0.08947954326868057\n",
            "i 22\n",
            "epoch 34\n",
            " batch Loss train: 0.07659327238798141\n",
            "i 23\n",
            "epoch 34\n",
            " batch Loss train: 0.06725216656923294\n",
            "i 24\n",
            "epoch 34\n",
            " batch Loss train: 0.06547750532627106\n",
            "i 25\n",
            "epoch 34\n",
            " batch Loss train: 0.05878044664859772\n",
            "i 26\n",
            "epoch 34\n",
            " batch Loss train: 0.06135711073875427\n",
            "i 27\n",
            "epoch 34\n",
            " batch Loss train: 0.04047803580760956\n",
            "i 28\n",
            "epoch 34\n",
            " batch Loss train: 0.06029762327671051\n",
            "i 29\n",
            "epoch 34\n",
            " batch Loss train: 0.06106020510196686\n",
            "i 30\n",
            "epoch 34\n",
            " batch Loss train: 0.0662546455860138\n",
            "i 31\n",
            "epoch 34\n",
            " batch Loss train: 0.05261420086026192\n",
            "i 32\n",
            "epoch 34\n",
            " batch Loss train: 0.06385616958141327\n",
            "i 33\n",
            "epoch 34\n",
            " batch Loss train: 0.053075578063726425\n",
            "i 34\n",
            "epoch 34\n",
            " batch Loss train: 0.07854943722486496\n",
            "i 35\n",
            "epoch 34\n",
            " batch Loss train: 0.0834973007440567\n",
            "i 36\n",
            "epoch 34\n",
            " batch Loss train: 0.0720144733786583\n",
            "i 37\n",
            "epoch 34\n",
            " batch Loss train: 0.06771266460418701\n",
            "i 38\n",
            "epoch 34\n",
            " batch Loss train: 0.09141503274440765\n",
            "i 39\n",
            "epoch 34\n",
            " batch Loss train: 0.09163780510425568\n",
            "i 40\n",
            "epoch 34\n",
            " batch Loss train: 0.07120315730571747\n",
            "i 41\n",
            "epoch 34\n",
            " batch Loss train: 0.07500315457582474\n",
            "i 42\n",
            "epoch 34\n",
            " batch Loss train: 0.057949066162109375\n",
            "i 43\n",
            "epoch 34\n",
            " batch Loss train: 0.0803828090429306\n",
            "i 44\n",
            "epoch 34\n",
            " batch Loss train: 0.07307589054107666\n",
            "i 45\n",
            "epoch 34\n",
            " batch Loss train: 0.058877475559711456\n",
            "i 46\n",
            "epoch 34\n",
            " batch Loss train: 0.0674019530415535\n",
            "i 47\n",
            "epoch 34\n",
            " batch Loss train: 0.0556320920586586\n",
            "i 48\n",
            "epoch 34\n",
            " batch Loss train: 0.07579411566257477\n",
            "i 49\n",
            "epoch 34\n",
            " batch Loss train: 0.07296700030565262\n",
            "i 50\n",
            "epoch 34\n",
            " batch Loss train: 0.07525777071714401\n",
            "i 51\n",
            "epoch 34\n",
            " batch Loss train: 0.08751499652862549\n",
            "i 52\n",
            "epoch 34\n",
            " batch Loss train: 0.07721857726573944\n",
            "i 53\n",
            "epoch 34\n",
            " batch Loss train: 0.07759646326303482\n",
            "i 54\n",
            "epoch 34\n",
            " batch Loss train: 0.06773693859577179\n",
            "i 55\n",
            "epoch 34\n",
            " batch Loss train: 0.0746801495552063\n",
            "i 56\n",
            "epoch 34\n",
            " batch Loss train: 0.0866827517747879\n",
            "i 57\n",
            "epoch 34\n",
            " batch Loss train: 0.08642058074474335\n",
            "i 58\n",
            "epoch 34\n",
            " batch Loss train: 0.05610441416501999\n",
            "i 59\n",
            "epoch 34\n",
            " batch Loss train: 0.07605613023042679\n",
            "i 60\n",
            "epoch 34\n",
            " batch Loss train: 0.05113272741436958\n",
            "i 61\n",
            "epoch 34\n",
            " batch Loss train: 0.062055762857198715\n",
            "i 62\n",
            "epoch 34\n",
            " batch Loss train: 0.05652270466089249\n",
            "i 63\n",
            "epoch 34\n",
            " batch Loss train: 0.08858627080917358\n",
            "i 64\n",
            "epoch 34\n",
            " batch Loss train: 0.061293378472328186\n",
            "i 65\n",
            "epoch 34\n",
            " batch Loss train: 0.09401606768369675\n",
            "i 66\n",
            "epoch 34\n",
            " batch Loss train: 0.059561677277088165\n",
            "i 67\n",
            "epoch 34\n",
            " batch Loss train: 0.08987497538328171\n",
            "i 68\n",
            "epoch 34\n",
            " batch Loss train: 0.06893204152584076\n",
            "i 69\n",
            "epoch 34\n",
            " batch Loss train: 0.07856201380491257\n",
            "i 70\n",
            "epoch 34\n",
            " batch Loss train: 0.10175340622663498\n",
            "i 71\n",
            "epoch 34\n",
            " batch Loss train: 0.08208934217691422\n",
            "i 72\n",
            "epoch 34\n",
            " batch Loss train: 0.0838693156838417\n",
            "i 73\n",
            "epoch 34\n",
            " batch Loss train: 0.08221427351236343\n",
            "i 74\n",
            "epoch 34\n",
            " batch Loss train: 0.08591043949127197\n",
            "i 75\n",
            "epoch 34\n",
            " batch Loss train: 0.08092918246984482\n",
            "i 76\n",
            "epoch 34\n",
            " batch Loss train: 0.07629553228616714\n",
            "i 77\n",
            "epoch 34\n",
            " batch Loss train: 0.07063932716846466\n",
            "i 78\n",
            "epoch 34\n",
            " batch Loss train: 0.08748175948858261\n",
            "i 79\n",
            "epoch 34\n",
            " batch Loss train: 0.06992889195680618\n",
            "i 80\n",
            "epoch 34\n",
            " batch Loss train: 0.06302333623170853\n",
            "i 81\n",
            "epoch 34\n",
            " batch Loss train: 0.09787341207265854\n",
            "i 82\n",
            "epoch 34\n",
            " batch Loss train: 0.0800388902425766\n",
            "i 83\n",
            "epoch 34\n",
            " batch Loss train: 0.0784025639295578\n",
            "i 84\n",
            "epoch 34\n",
            " batch Loss train: 0.06757692992687225\n",
            "i 85\n",
            "epoch 34\n",
            " batch Loss train: 0.07571075856685638\n",
            "i 86\n",
            "epoch 34\n",
            " batch Loss train: 0.0815371721982956\n",
            "i 87\n",
            "epoch 34\n",
            " batch Loss train: 0.06171920523047447\n",
            "i 88\n",
            "epoch 34\n",
            " batch Loss train: 0.07115276902914047\n",
            "i 89\n",
            "epoch 34\n",
            " batch Loss train: 0.08309882879257202\n",
            "i 90\n",
            "epoch 34\n",
            " batch Loss train: 0.09746793657541275\n",
            "i 91\n",
            "epoch 34\n",
            " batch Loss train: 0.08035164326429367\n",
            "i 92\n",
            "epoch 34\n",
            " batch Loss train: 0.07557333260774612\n",
            "i 93\n",
            "epoch 34\n",
            " batch Loss train: 0.09947934746742249\n",
            "i 94\n",
            "epoch 34\n",
            " batch Loss train: 0.08160816133022308\n",
            "i 95\n",
            "epoch 34\n",
            " batch Loss train: 0.06366586685180664\n",
            "i 96\n",
            "epoch 34\n",
            " batch Loss train: 0.09953168779611588\n",
            "i 97\n",
            "epoch 34\n",
            " batch Loss train: 0.08704964071512222\n",
            "i 98\n",
            "epoch 34\n",
            " batch Loss train: 0.053034599870443344\n",
            "i 99\n",
            "epoch 34\n",
            " batch Loss train: 0.06223522871732712\n",
            "i 100\n",
            "epoch 34\n",
            " batch Loss train: 0.08817692846059799\n",
            "i 101\n",
            "epoch 34\n",
            " batch Loss train: 0.12019602954387665\n",
            "i 102\n",
            "epoch 34\n",
            " batch Loss train: 0.08356570452451706\n",
            "i 103\n",
            "epoch 34\n",
            " batch Loss train: 0.0720941349864006\n",
            "i 104\n",
            "epoch 34\n",
            " batch Loss train: 0.06695448607206345\n",
            "i 105\n",
            "epoch 34\n",
            " batch Loss train: 0.0682615339756012\n",
            "i 106\n",
            "epoch 34\n",
            " batch Loss train: 0.09874541312456131\n",
            "i 107\n",
            "epoch 34\n",
            " batch Loss train: 0.06628705561161041\n",
            "i 108\n",
            "epoch 34\n",
            " batch Loss train: 0.09107062965631485\n",
            "i 109\n",
            "epoch 34\n",
            " batch Loss train: 0.11962759494781494\n",
            "i 110\n",
            "epoch 34\n",
            " batch Loss train: 0.07613587379455566\n",
            "i 111\n",
            "epoch 34\n",
            " batch Loss train: 0.09169641882181168\n",
            "i 112\n",
            "epoch 34\n",
            " batch Loss train: 0.08266592770814896\n",
            "i 113\n",
            "epoch 34\n",
            " batch Loss train: 0.09461627900600433\n",
            "i 114\n",
            "epoch 34\n",
            " batch Loss train: 0.08731847256422043\n",
            "i 115\n",
            "epoch 34\n",
            " batch Loss train: 0.060370296239852905\n",
            "i 116\n",
            "epoch 34\n",
            " batch Loss train: 0.06006771698594093\n",
            "i 117\n",
            "epoch 34\n",
            " batch Loss train: 0.10854052752256393\n",
            "i 118\n",
            "epoch 34\n",
            " batch Loss train: 0.07381661981344223\n",
            "i 119\n",
            "epoch 34\n",
            " batch Loss train: 0.09229661524295807\n",
            "i 120\n",
            "epoch 34\n",
            " batch Loss train: 0.06931429356336594\n",
            "i 121\n",
            "epoch 34\n",
            " batch Loss train: 0.05634875223040581\n",
            "i 122\n",
            "epoch 34\n",
            " batch Loss train: 0.09615091234445572\n",
            "i 123\n",
            "epoch 34\n",
            " batch Loss train: 0.07322381436824799\n",
            "i 124\n",
            "epoch 34\n",
            " batch Loss train: 0.10281765460968018\n",
            "i 125\n",
            "epoch 34\n",
            " batch Loss train: 0.11289826780557632\n",
            "i 126\n",
            "epoch 34\n",
            " batch Loss train: 0.08152694255113602\n",
            "i 127\n",
            "epoch 34\n",
            " batch Loss train: 0.07257509231567383\n",
            "i 128\n",
            "epoch 34\n",
            " batch Loss train: 0.08600160479545593\n",
            "i 129\n",
            "epoch 34\n",
            " batch Loss train: 0.08086167275905609\n",
            "i 130\n",
            "epoch 34\n",
            " batch Loss train: 0.07693973183631897\n",
            "i 131\n",
            "epoch 34\n",
            " batch Loss train: 0.09519646316766739\n",
            "i 132\n",
            "epoch 34\n",
            " batch Loss train: 0.07953144609928131\n",
            "i 133\n",
            "epoch 34\n",
            " batch Loss train: 0.09187385439872742\n",
            "i 134\n",
            "epoch 34\n",
            " batch Loss train: 0.08258400112390518\n",
            "i 135\n",
            "epoch 34\n",
            " batch Loss train: 0.0971585139632225\n",
            "i 136\n",
            "epoch 34\n",
            " batch Loss train: 0.0762377604842186\n",
            "i 137\n",
            "epoch 34\n",
            " batch Loss train: 0.08380944281816483\n",
            "i 138\n",
            "epoch 34\n",
            " batch Loss train: 0.08860259503126144\n",
            "i 139\n",
            "epoch 34\n",
            " batch Loss train: 0.08327054977416992\n",
            "i 140\n",
            "epoch 34\n",
            " batch Loss train: 0.08927743881940842\n",
            "i 141\n",
            "epoch 34\n",
            " batch Loss train: 0.09932256489992142\n",
            "i 142\n",
            "epoch 34\n",
            " batch Loss train: 0.10978596657514572\n",
            "i 143\n",
            "epoch 34\n",
            " batch Loss train: 0.07918942719697952\n",
            "i 144\n",
            "epoch 34\n",
            " batch Loss train: 0.07936578243970871\n",
            "i 145\n",
            "epoch 34\n",
            " batch Loss train: 0.06431080400943756\n",
            "i 146\n",
            "epoch 34\n",
            " batch Loss train: 0.07963337004184723\n",
            "i 147\n",
            "epoch 34\n",
            " batch Loss train: 0.0770893320441246\n",
            "i 148\n",
            "epoch 34\n",
            " batch Loss train: 0.1182895079255104\n",
            "i 149\n",
            "epoch 34\n",
            " batch Loss train: 0.08686859905719757\n",
            "i 150\n",
            "epoch 34\n",
            " batch Loss train: 0.08989517390727997\n",
            "i 151\n",
            "epoch 34\n",
            " batch Loss train: 0.09635893255472183\n",
            "i 152\n",
            "epoch 34\n",
            " batch Loss train: 0.07024510949850082\n",
            "i 153\n",
            "epoch 34\n",
            " batch Loss train: 0.11316577345132828\n",
            "i 154\n",
            "epoch 34\n",
            " batch Loss train: 0.0871310755610466\n",
            "i 155\n",
            "epoch 34\n",
            " batch Loss train: 0.08562563359737396\n",
            "i 156\n",
            "epoch 34\n",
            " batch Loss train: 0.07954991608858109\n",
            "i 157\n",
            "epoch 34\n",
            " batch Loss train: 0.07592203468084335\n",
            "i 158\n",
            "epoch 34\n",
            " batch Loss train: 0.08949710428714752\n",
            "i 159\n",
            "epoch 34\n",
            " batch Loss train: 0.07138608396053314\n",
            "i 160\n",
            "epoch 34\n",
            " batch Loss train: 0.08046696335077286\n",
            "i 161\n",
            "epoch 34\n",
            " batch Loss train: 0.06934887170791626\n",
            "i 162\n",
            "epoch 34\n",
            " batch Loss train: 0.08582381159067154\n",
            "i 163\n",
            "epoch 34\n",
            " batch Loss train: 0.1190781518816948\n",
            "i 164\n",
            "epoch 34\n",
            " batch Loss train: 0.10567943006753922\n",
            "i 165\n",
            "epoch 34\n",
            " batch Loss train: 0.08537198603153229\n",
            "i 166\n",
            "epoch 34\n",
            " batch Loss train: 0.10017329454421997\n",
            "i 167\n",
            "epoch 34\n",
            " batch Loss train: 0.09446164220571518\n",
            "i 168\n",
            "epoch 34\n",
            " batch Loss train: 0.07375884056091309\n",
            "i 169\n",
            "epoch 34\n",
            " batch Loss train: 0.08217576146125793\n",
            "i 170\n",
            "epoch 34\n",
            " batch Loss train: 0.07662807404994965\n",
            "i 171\n",
            "epoch 34\n",
            " batch Loss train: 0.1034252941608429\n",
            "i 172\n",
            "epoch 34\n",
            " batch Loss train: 0.07101544737815857\n",
            "i 173\n",
            "epoch 34\n",
            " batch Loss train: 0.07079020142555237\n",
            "i 174\n",
            "epoch 34\n",
            " batch Loss train: 0.09830448031425476\n",
            "i 175\n",
            "epoch 34\n",
            " batch Loss train: 0.09912841022014618\n",
            "i 176\n",
            "epoch 34\n",
            " batch Loss train: 0.0898842066526413\n",
            "i 177\n",
            "epoch 34\n",
            " batch Loss train: 0.09790705144405365\n",
            "i 178\n",
            "epoch 34\n",
            " batch Loss train: 0.08290353417396545\n",
            "i 179\n",
            "epoch 34\n",
            " batch Loss train: 0.12762483954429626\n",
            "i 180\n",
            "epoch 34\n",
            " batch Loss train: 0.06755627691745758\n",
            "i 181\n",
            "epoch 34\n",
            " batch Loss train: 0.055524639785289764\n",
            "i 182\n",
            "epoch 34\n",
            " batch Loss train: 0.08155649155378342\n",
            "i 183\n",
            "epoch 34\n",
            " batch Loss train: 0.06125932186841965\n",
            "i 184\n",
            "epoch 34\n",
            " batch Loss train: 0.07807779312133789\n",
            "i 185\n",
            "epoch 34\n",
            " batch Loss train: 0.0969657152891159\n",
            "i 186\n",
            "epoch 34\n",
            " batch Loss train: 0.06309253722429276\n",
            "i 187\n",
            "epoch 34\n",
            " batch Loss train: 0.08431815356016159\n",
            "i 188\n",
            "epoch 34\n",
            " batch Loss train: 0.08258821815252304\n",
            "i 189\n",
            "epoch 34\n",
            " batch Loss train: 0.05592268705368042\n",
            "i 190\n",
            "epoch 34\n",
            " batch Loss train: 0.08878900855779648\n",
            "i 191\n",
            "epoch 34\n",
            " batch Loss train: 0.10191531479358673\n",
            "i 192\n",
            "epoch 34\n",
            " batch Loss train: 0.09127797931432724\n",
            "i 193\n",
            "epoch 34\n",
            " batch Loss train: 0.07132120430469513\n",
            "i 194\n",
            "epoch 34\n",
            " batch Loss train: 0.061206504702568054\n",
            "i 195\n",
            "epoch 34\n",
            " batch Loss train: 0.09326882660388947\n",
            "i 196\n",
            "epoch 34\n",
            " batch Loss train: 0.10733870416879654\n",
            "i 197\n",
            "epoch 34\n",
            " batch Loss train: 0.07451712340116501\n",
            "i 198\n",
            "epoch 34\n",
            " batch Loss train: 0.06936994940042496\n",
            "i 199\n",
            "epoch 34\n",
            " batch Loss train: 0.0875607579946518\n",
            "i 200\n",
            "epoch 34\n",
            " batch Loss train: 0.08221352100372314\n",
            "i 201\n",
            "epoch 34\n",
            " batch Loss train: 0.08234088122844696\n",
            "i 202\n",
            "epoch 34\n",
            " batch Loss train: 0.07041393220424652\n",
            "i 203\n",
            "epoch 34\n",
            " batch Loss train: 0.076256662607193\n",
            "i 204\n",
            "epoch 34\n",
            " batch Loss train: 0.06243276223540306\n",
            "i 205\n",
            "epoch 34\n",
            " batch Loss train: 0.08364693820476532\n",
            "i 206\n",
            "epoch 34\n",
            " batch Loss train: 0.07686559110879898\n",
            "i 207\n",
            "epoch 34\n",
            " batch Loss train: 0.08399086445569992\n",
            "i 208\n",
            "epoch 34\n",
            " batch Loss train: 0.0862915962934494\n",
            "i 209\n",
            "epoch 34\n",
            " batch Loss train: 0.07570119202136993\n",
            "i 210\n",
            "epoch 34\n",
            " batch Loss train: 0.09662465751171112\n",
            "i 211\n",
            "epoch 34\n",
            " batch Loss train: 0.06166589632630348\n",
            "i 212\n",
            "epoch 34\n",
            " batch Loss train: 0.08925008028745651\n",
            "i 213\n",
            "epoch 34\n",
            " batch Loss train: 0.07820867747068405\n",
            "i 214\n",
            "epoch 34\n",
            " batch Loss train: 0.10818841308355331\n",
            "i 215\n",
            "epoch 34\n",
            " batch Loss train: 0.07324028015136719\n",
            "i 216\n",
            "epoch 34\n",
            " batch Loss train: 0.07805413752794266\n",
            "i 217\n",
            "epoch 34\n",
            " batch Loss train: 0.06971476227045059\n",
            "i 218\n",
            "epoch 34\n",
            " batch Loss train: 0.09818378835916519\n",
            "i 219\n",
            "epoch 34\n",
            " batch Loss train: 0.09392556548118591\n",
            "i 220\n",
            "epoch 34\n",
            " batch Loss train: 0.11482300609350204\n",
            "i 221\n",
            "epoch 34\n",
            " batch Loss train: 0.08104309439659119\n",
            "i 222\n",
            "epoch 34\n",
            " batch Loss train: 0.09038987010717392\n",
            "i 223\n",
            "epoch 34\n",
            " batch Loss train: 0.06772957742214203\n",
            "i 224\n",
            "epoch 34\n",
            " batch Loss train: 0.0725092664361\n",
            "i 225\n",
            "epoch 34\n",
            " batch Loss train: 0.10371671617031097\n",
            "i 226\n",
            "epoch 34\n",
            " batch Loss train: 0.08239425718784332\n",
            "i 227\n",
            "epoch 34\n",
            " batch Loss train: 0.05562851205468178\n",
            "i 228\n",
            "epoch 34\n",
            " batch Loss train: 0.09146056324243546\n",
            "i 229\n",
            "epoch 34\n",
            " batch Loss train: 0.08235857635736465\n",
            "i 230\n",
            "epoch 34\n",
            " batch Loss train: 0.0882045328617096\n",
            "i 231\n",
            "epoch 34\n",
            " batch Loss train: 0.07727537304162979\n",
            "i 232\n",
            "epoch 34\n",
            " batch Loss train: 0.0699932649731636\n",
            "i 233\n",
            "epoch 34\n",
            " batch Loss train: 0.08412081003189087\n",
            "i 234\n",
            "epoch 34\n",
            " batch Loss train: 0.08748709410429001\n",
            "i 235\n",
            "epoch 34\n",
            " batch Loss train: 0.07381683588027954\n",
            "i 236\n",
            "epoch 34\n",
            " batch Loss train: 0.08911836892366409\n",
            "i 237\n",
            "epoch 34\n",
            " batch Loss train: 0.07402251660823822\n",
            "i 238\n",
            "epoch 34\n",
            " batch Loss train: 0.09286975115537643\n",
            "i 239\n",
            "epoch 34\n",
            " batch Loss train: 0.08240117132663727\n",
            "i 240\n",
            "epoch 34\n",
            " batch Loss train: 0.12254052609205246\n",
            "i 241\n",
            "epoch 34\n",
            " batch Loss train: 0.08201253414154053\n",
            "i 242\n",
            "epoch 34\n",
            " batch Loss train: 0.07945974916219711\n",
            "i 243\n",
            "epoch 34\n",
            " batch Loss train: 0.07470737397670746\n",
            "i 244\n",
            "epoch 34\n",
            " batch Loss train: 0.0752653256058693\n",
            "i 245\n",
            "epoch 34\n",
            " batch Loss train: 0.09961329400539398\n",
            "i 246\n",
            "epoch 34\n",
            " batch Loss train: 0.08054213970899582\n",
            "i 247\n",
            "epoch 34\n",
            " batch Loss train: 0.08293630927801132\n",
            "i 248\n",
            "epoch 34\n",
            " batch Loss train: 0.06391848623752594\n",
            "i 249\n",
            "epoch 34\n",
            " batch Loss train: 0.07416011393070221\n",
            "i 250\n",
            "epoch 34\n",
            " batch Loss train: 0.09558229148387909\n",
            "i 251\n",
            "epoch 34\n",
            " batch Loss train: 0.11173369735479355\n",
            "i 252\n",
            "epoch 34\n",
            " batch Loss train: 0.06801597774028778\n",
            "i 253\n",
            "epoch 34\n",
            " batch Loss train: 0.07517921924591064\n",
            "i 254\n",
            "epoch 34\n",
            " batch Loss train: 0.0823429673910141\n",
            "i 255\n",
            "epoch 34\n",
            " batch Loss train: 0.0902804583311081\n",
            "i 256\n",
            "epoch 34\n",
            " batch Loss train: 0.08438412845134735\n",
            "i 257\n",
            "epoch 34\n",
            " batch Loss train: 0.062057752162218094\n",
            "i 258\n",
            "epoch 34\n",
            " batch Loss train: 0.08852678537368774\n",
            "i 259\n",
            "epoch 34\n",
            " batch Loss train: 0.07461825758218765\n",
            "i 260\n",
            "epoch 34\n",
            " batch Loss train: 0.08211703598499298\n",
            "i 261\n",
            "epoch 34\n",
            " batch Loss train: 0.0681336298584938\n",
            "i 262\n",
            "epoch 34\n",
            " batch Loss train: 0.09247375279664993\n",
            "i 263\n",
            "epoch 34\n",
            " batch Loss train: 0.07222790271043777\n",
            "i 264\n",
            "epoch 34\n",
            " batch Loss train: 0.07377588003873825\n",
            "i 265\n",
            "epoch 34\n",
            " batch Loss train: 0.09880361706018448\n",
            "i 266\n",
            "epoch 34\n",
            " batch Loss train: 0.07591081410646439\n",
            "i 267\n",
            "epoch 34\n",
            " batch Loss train: 0.06914332509040833\n",
            "i 268\n",
            "epoch 34\n",
            " batch Loss train: 0.07634832710027695\n",
            "i 269\n",
            "epoch 34\n",
            " batch Loss train: 0.06347823888063431\n",
            "i 270\n",
            "epoch 34\n",
            " batch Loss train: 0.07096295803785324\n",
            "i 271\n",
            "epoch 34\n",
            " batch Loss train: 0.07085399329662323\n",
            "i 272\n",
            "epoch 34\n",
            " batch Loss train: 0.07512874156236649\n",
            "i 273\n",
            "epoch 34\n",
            " batch Loss train: 0.09646113216876984\n",
            "i 274\n",
            "epoch 34\n",
            " batch Loss train: 0.08494795858860016\n",
            "i 275\n",
            "epoch 34\n",
            " batch Loss train: 0.08801434934139252\n",
            "i 276\n",
            "epoch 34\n",
            " batch Loss train: 0.08602224290370941\n",
            "i 277\n",
            "epoch 34\n",
            " batch Loss train: 0.08462627232074738\n",
            "i 278\n",
            "epoch 34\n",
            " batch Loss train: 0.07310495525598526\n",
            "i 279\n",
            "epoch 34\n",
            " batch Loss train: 0.07998842746019363\n",
            "i 280\n",
            "epoch 34\n",
            " batch Loss train: 0.0752519741654396\n",
            "i 281\n",
            "epoch 34\n",
            " batch Loss train: 0.09238988161087036\n",
            "i 282\n",
            "epoch 34\n",
            " batch Loss train: 0.07516222447156906\n",
            "i 283\n",
            "epoch 34\n",
            " batch Loss train: 0.06531183421611786\n",
            "i 284\n",
            "epoch 34\n",
            " batch Loss train: 0.08072435855865479\n",
            "i 285\n",
            "epoch 34\n",
            " batch Loss train: 0.09106909483671188\n",
            "i 286\n",
            "epoch 34\n",
            " batch Loss train: 0.07995827496051788\n",
            "i 287\n",
            "epoch 34\n",
            " batch Loss train: 0.08672074228525162\n",
            "i 288\n",
            "epoch 34\n",
            " batch Loss train: 0.09377719461917877\n",
            "i 289\n",
            "epoch 34\n",
            " batch Loss train: 0.07417255640029907\n",
            "i 290\n",
            "epoch 34\n",
            " batch Loss train: 0.07397165149450302\n",
            "i 291\n",
            "epoch 34\n",
            " batch Loss train: 0.09914170950651169\n",
            "i 292\n",
            "epoch 34\n",
            " batch Loss train: 0.07447220385074615\n",
            "i 293\n",
            "epoch 34\n",
            " batch Loss train: 0.07918515056371689\n",
            "i 294\n",
            "epoch 34\n",
            " batch Loss train: 0.07038834691047668\n",
            "i 295\n",
            "epoch 34\n",
            " batch Loss train: 0.06915652751922607\n",
            "i 296\n",
            "epoch 34\n",
            " batch Loss train: 0.11357806622982025\n",
            "i 297\n",
            "epoch 34\n",
            " batch Loss train: 0.055584587156772614\n",
            "i 298\n",
            "epoch 34\n",
            " batch Loss train: 0.08732390403747559\n",
            "i 299\n",
            "epoch 34\n",
            " batch Loss train: 0.06387471407651901\n",
            "i 300\n",
            "epoch 34\n",
            " batch Loss train: 0.08181251585483551\n",
            "i 301\n",
            "epoch 34\n",
            " batch Loss train: 0.09591756016016006\n",
            "i 302\n",
            "epoch 34\n",
            " batch Loss train: 0.08033736795186996\n",
            "i 303\n",
            "epoch 34\n",
            " batch Loss train: 0.05492312088608742\n",
            "i 304\n",
            "epoch 34\n",
            " batch Loss train: 0.08427038788795471\n",
            "i 305\n",
            "epoch 34\n",
            " batch Loss train: 0.08566881716251373\n",
            "i 306\n",
            "epoch 34\n",
            " batch Loss train: 0.07045916467905045\n",
            "i 307\n",
            "epoch 34\n",
            " batch Loss train: 0.07740429043769836\n",
            "i 308\n",
            "epoch 34\n",
            " batch Loss train: 0.07717888802289963\n",
            "i 309\n",
            "epoch 34\n",
            " batch Loss train: 0.08205622434616089\n",
            "i 310\n",
            "epoch 34\n",
            " batch Loss train: 0.056526489555835724\n",
            "i 311\n",
            "epoch 34\n",
            " batch Loss train: 0.09191867709159851\n",
            "i 312\n",
            "epoch 34\n",
            " batch Loss train: 0.09851793199777603\n",
            "i 313\n",
            "epoch 34\n",
            " batch Loss train: 0.08687059581279755\n",
            "i 314\n",
            "epoch 34\n",
            " batch Loss train: 0.08591535687446594\n",
            "i 315\n",
            "epoch 34\n",
            " batch Loss train: 0.0797845795750618\n",
            "i 316\n",
            "epoch 34\n",
            " batch Loss train: 0.08294763416051865\n",
            "i 317\n",
            "epoch 34\n",
            " batch Loss train: 0.06479942053556442\n",
            "i 318\n",
            "epoch 34\n",
            " batch Loss train: 0.08965615928173065\n",
            "i 319\n",
            "epoch 34\n",
            " batch Loss train: 0.11354774981737137\n",
            "i 320\n",
            "epoch 34\n",
            " batch Loss train: 0.09907077252864838\n",
            "i 321\n",
            "epoch 34\n",
            " batch Loss train: 0.06598947197198868\n",
            "i 322\n",
            "epoch 34\n",
            " batch Loss train: 0.11538124084472656\n",
            "i 323\n",
            "epoch 34\n",
            " batch Loss train: 0.10348527133464813\n",
            "i 324\n",
            "epoch 34\n",
            " batch Loss train: 0.08286015689373016\n",
            "i 325\n",
            "epoch 34\n",
            " batch Loss train: 0.08085037022829056\n",
            "i 326\n",
            "epoch 34\n",
            " batch Loss train: 0.095330610871315\n",
            "i 327\n",
            "epoch 34\n",
            " batch Loss train: 0.10544434189796448\n",
            "i 328\n",
            "epoch 34\n",
            " batch Loss train: 0.09176020324230194\n",
            "i 329\n",
            "epoch 34\n",
            " batch Loss train: 0.0907546803355217\n",
            "i 330\n",
            "epoch 34\n",
            " batch Loss train: 0.11885431408882141\n",
            "i 331\n",
            "epoch 34\n",
            " batch Loss train: 0.09863347560167313\n",
            "i 332\n",
            "epoch 34\n",
            " batch Loss train: 0.0804852619767189\n",
            "i 333\n",
            "epoch 34\n",
            " batch Loss train: 0.09275352209806442\n",
            "i 334\n",
            "epoch 34\n",
            " batch Loss train: 0.0813465490937233\n",
            "i 335\n",
            "epoch 34\n",
            " batch Loss train: 0.08838918805122375\n",
            "i 336\n",
            "epoch 34\n",
            " batch Loss train: 0.10365092754364014\n",
            "i 337\n",
            "epoch 34\n",
            " batch Loss train: 0.07508038729429245\n",
            "i 338\n",
            "epoch 34\n",
            " batch Loss train: 0.07808465510606766\n",
            "i 339\n",
            "epoch 34\n",
            " batch Loss train: 0.055930301547050476\n",
            "i 340\n",
            "epoch 34\n",
            " batch Loss train: 0.08440890163183212\n",
            "i 341\n",
            "epoch 34\n",
            " batch Loss train: 0.07953553646802902\n",
            "i 342\n",
            "epoch 34\n",
            " batch Loss train: 0.05848151445388794\n",
            "i 343\n",
            "epoch 34\n",
            " batch Loss train: 0.08664703369140625\n",
            "i 344\n",
            "epoch 34\n",
            " batch Loss train: 0.07934898883104324\n",
            "i 345\n",
            "epoch 34\n",
            " batch Loss train: 0.0843554362654686\n",
            "i 346\n",
            "epoch 34\n",
            " batch Loss train: 0.08573639392852783\n",
            "i 347\n",
            "epoch 34\n",
            " batch Loss train: 0.07208661735057831\n",
            "i 348\n",
            "epoch 34\n",
            " batch Loss train: 0.06770997494459152\n",
            "i 349\n",
            "epoch 34\n",
            " batch Loss train: 0.09525196999311447\n",
            "i 350\n",
            "epoch 34\n",
            " batch Loss train: 0.07355469465255737\n",
            "i 351\n",
            "epoch 34\n",
            " batch Loss train: 0.10270236432552338\n",
            "i 352\n",
            "epoch 34\n",
            " batch Loss train: 0.068990059196949\n",
            "i 353\n",
            "epoch 34\n",
            " batch Loss train: 0.08813182264566422\n",
            "i 354\n",
            "epoch 34\n",
            " batch Loss train: 0.08672726899385452\n",
            "i 355\n",
            "epoch 34\n",
            " batch Loss train: 0.07236725836992264\n",
            "i 356\n",
            "epoch 34\n",
            " batch Loss train: 0.0924735739827156\n",
            "i 357\n",
            "epoch 34\n",
            " batch Loss train: 0.0993824228644371\n",
            "i 358\n",
            "epoch 34\n",
            " batch Loss train: 0.08565394580364227\n",
            "i 359\n",
            "epoch 34\n",
            " batch Loss train: 0.11199735105037689\n",
            "i 360\n",
            "epoch 34\n",
            " batch Loss train: 0.10621921718120575\n",
            "i 361\n",
            "epoch 34\n",
            " batch Loss train: 0.08533712476491928\n",
            "i 362\n",
            "epoch 34\n",
            " batch Loss train: 0.12262823432683945\n",
            "i 363\n",
            "epoch 34\n",
            " batch Loss train: 0.08887400478124619\n",
            "i 364\n",
            "epoch 34\n",
            " batch Loss train: 0.09921973198652267\n",
            "i 365\n",
            "epoch 34\n",
            " batch Loss train: 0.06671029329299927\n",
            "i 366\n",
            "epoch 34\n",
            " batch Loss train: 0.07389070093631744\n",
            "i 367\n",
            "epoch 34\n",
            " batch Loss train: 0.07357713580131531\n",
            "i 368\n",
            "epoch 34\n",
            " batch Loss train: 0.08222158253192902\n",
            "i 369\n",
            "epoch 34\n",
            " batch Loss train: 0.09114374965429306\n",
            "i 370\n",
            "epoch 34\n",
            " batch Loss train: 0.06387064605951309\n",
            "i 371\n",
            "epoch 34\n",
            " batch Loss train: 0.07411698251962662\n",
            "i 372\n",
            "epoch 34\n",
            " batch Loss train: 0.08743084222078323\n",
            "i 373\n",
            "epoch 34\n",
            " batch Loss train: 0.07039656490087509\n",
            "i 374\n",
            "epoch 34\n",
            " batch Loss train: 0.07335294038057327\n",
            "i 375\n",
            "epoch 34\n",
            " batch Loss train: 0.07766997069120407\n",
            "i 376\n",
            "epoch 34\n",
            " batch Loss train: 0.07280480861663818\n",
            "i 377\n",
            "epoch 34\n",
            " batch Loss train: 0.09875607490539551\n",
            "i 378\n",
            "epoch 34\n",
            " batch Loss train: 0.0669381394982338\n",
            "i 379\n",
            "epoch 34\n",
            " batch Loss train: 0.07048632949590683\n",
            "i 380\n",
            "epoch 34\n",
            " batch Loss train: 0.09048470109701157\n",
            "i 381\n",
            "epoch 34\n",
            " batch Loss train: 0.08903557807207108\n",
            "i 382\n",
            "epoch 34\n",
            " batch Loss train: 0.08386307954788208\n",
            "i 383\n",
            "epoch 34\n",
            " batch Loss train: 0.09220590442419052\n",
            "i 384\n",
            "epoch 34\n",
            " batch Loss train: 0.0706561952829361\n",
            "i 385\n",
            "epoch 34\n",
            " batch Loss train: 0.06149812042713165\n",
            "i 386\n",
            "epoch 34\n",
            " batch Loss train: 0.10248582065105438\n",
            "i 387\n",
            "epoch 34\n",
            " batch Loss train: 0.09322837740182877\n",
            "i 388\n",
            "epoch 34\n",
            " batch Loss train: 0.0830187126994133\n",
            "i 389\n",
            "epoch 34\n",
            " batch Loss train: 0.07032889872789383\n",
            "i 390\n",
            "epoch 34\n",
            " batch Loss train: 0.0792679712176323\n",
            "i 391\n",
            "epoch 34\n",
            " batch Loss train: 0.10642659664154053\n",
            "i 392\n",
            "epoch 34\n",
            " batch Loss train: 0.08500758558511734\n",
            "i 393\n",
            "epoch 34\n",
            " batch Loss train: 0.06896176934242249\n",
            "i 394\n",
            "epoch 34\n",
            " batch Loss train: 0.07175587862730026\n",
            "i 395\n",
            "epoch 34\n",
            " batch Loss train: 0.07256443053483963\n",
            "i 396\n",
            "epoch 34\n",
            " batch Loss train: 0.0998377650976181\n",
            "i 397\n",
            "epoch 34\n",
            " batch Loss train: 0.0755985677242279\n",
            "i 398\n",
            "epoch 34\n",
            " batch Loss train: 0.07856789976358414\n",
            "i 399\n",
            "epoch 34\n",
            " batch Loss train: 0.09438029676675797\n",
            "i 400\n",
            "epoch 34\n",
            " batch Loss train: 0.09638068079948425\n",
            "i 401\n",
            "epoch 34\n",
            " batch Loss train: 0.08299019187688828\n",
            "i 402\n",
            "epoch 34\n",
            " batch Loss train: 0.12422642111778259\n",
            "i 403\n",
            "epoch 34\n",
            " batch Loss train: 0.07860422879457474\n",
            "i 404\n",
            "epoch 34\n",
            " batch Loss train: 0.09102483093738556\n",
            "i 405\n",
            "epoch 34\n",
            " batch Loss train: 0.09935261309146881\n",
            "i 406\n",
            "epoch 34\n",
            " batch Loss train: 0.12868238985538483\n",
            "i 407\n",
            "epoch 34\n",
            " batch Loss train: 0.08411605656147003\n",
            "i 408\n",
            "epoch 34\n",
            " batch Loss train: 0.08491992950439453\n",
            "i 409\n",
            "epoch 34\n",
            " batch Loss train: 0.05290425196290016\n",
            "i 410\n",
            "epoch 34\n",
            " batch Loss train: 0.08530879765748978\n",
            "i 411\n",
            "epoch 34\n",
            " batch Loss train: 0.09489604085683823\n",
            "i 412\n",
            "epoch 34\n",
            " batch Loss train: 0.11962059885263443\n",
            "i 413\n",
            "epoch 34\n",
            " batch Loss train: 0.0746002048254013\n",
            "i 414\n",
            "epoch 34\n",
            " batch Loss train: 0.08343873172998428\n",
            "i 415\n",
            "epoch 34\n",
            " batch Loss train: 0.0693112388253212\n",
            "i 416\n",
            "epoch 34\n",
            " batch Loss train: 0.0739324614405632\n",
            "i 417\n",
            "epoch 34\n",
            " batch Loss train: 0.060907091945409775\n",
            "i 418\n",
            "epoch 34\n",
            " batch Loss train: 0.0810755118727684\n",
            "i 419\n",
            "epoch 34\n",
            " batch Loss train: 0.06684171408414841\n",
            "i 420\n",
            "epoch 34\n",
            " batch Loss train: 0.05951503664255142\n",
            "i 421\n",
            "epoch 34\n",
            " batch Loss train: 0.07661992311477661\n",
            "i 422\n",
            "epoch 34\n",
            " batch Loss train: 0.07185234129428864\n",
            "i 423\n",
            "epoch 34\n",
            " batch Loss train: 0.08567093312740326\n",
            "i 424\n",
            "epoch 34\n",
            " batch Loss train: 0.06296056509017944\n",
            "i 425\n",
            "epoch 34\n",
            " batch Loss train: 0.06943102180957794\n",
            "i 426\n",
            "epoch 34\n",
            " batch Loss train: 0.10663775354623795\n",
            "i 427\n",
            "epoch 34\n",
            " batch Loss train: 0.0917903333902359\n",
            "i 428\n",
            "epoch 34\n",
            " batch Loss train: 0.051667120307683945\n",
            "i 429\n",
            "epoch 34\n",
            " batch Loss train: 0.07158486545085907\n",
            "i 430\n",
            "epoch 34\n",
            " batch Loss train: 0.07939445227384567\n",
            "i 431\n",
            "epoch 34\n",
            " batch Loss train: 0.08313736319541931\n",
            "i 432\n",
            "epoch 34\n",
            " batch Loss train: 0.07408438622951508\n",
            "i 433\n",
            "epoch 34\n",
            " batch Loss train: 0.08246012777090073\n",
            "i 434\n",
            "epoch 34\n",
            " batch Loss train: 0.07592935115098953\n",
            "i 435\n",
            "epoch 34\n",
            " batch Loss train: 0.10529948770999908\n",
            "i 436\n",
            "epoch 34\n",
            " batch Loss train: 0.08078140020370483\n",
            "i 437\n",
            "epoch 34\n",
            " batch Loss train: 0.10616147518157959\n",
            "i 438\n",
            "epoch 34\n",
            " batch Loss train: 0.09156956523656845\n",
            "i 439\n",
            "epoch 34\n",
            " batch Loss train: 0.0740586593747139\n",
            "i 440\n",
            "epoch 34\n",
            " batch Loss train: 0.0984189435839653\n",
            "i 441\n",
            "epoch 34\n",
            " batch Loss train: 0.0748346596956253\n",
            "i 442\n",
            "epoch 34\n",
            " batch Loss train: 0.12243712693452835\n",
            "i 443\n",
            "epoch 34\n",
            " batch Loss train: 0.10911758244037628\n",
            "i 444\n",
            "epoch 34\n",
            " batch Loss train: 0.09009527415037155\n",
            "i 445\n",
            "epoch 34\n",
            " batch Loss train: 0.11614735424518585\n",
            "total epoch Loss train: tensor(0.1161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 35\n",
            " batch Loss train: 0.0801074206829071\n",
            "i 1\n",
            "epoch 35\n",
            " batch Loss train: 0.07447151094675064\n",
            "i 2\n",
            "epoch 35\n",
            " batch Loss train: 0.07748739421367645\n",
            "i 3\n",
            "epoch 35\n",
            " batch Loss train: 0.06442981958389282\n",
            "i 4\n",
            "epoch 35\n",
            " batch Loss train: 0.0860491544008255\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 35\n",
            " batch Loss train: 0.07586728036403656\n",
            "i 6\n",
            "epoch 35\n",
            " batch Loss train: 0.0554778054356575\n",
            "i 7\n",
            "epoch 35\n",
            " batch Loss train: 0.06644833832979202\n",
            "i 8\n",
            "epoch 35\n",
            " batch Loss train: 0.07228713482618332\n",
            "i 9\n",
            "epoch 35\n",
            " batch Loss train: 0.06790081411600113\n",
            "i 10\n",
            "epoch 35\n",
            " batch Loss train: 0.07366997748613358\n",
            "i 11\n",
            "epoch 35\n",
            " batch Loss train: 0.08169601857662201\n",
            "i 12\n",
            "epoch 35\n",
            " batch Loss train: 0.07734636962413788\n",
            "i 13\n",
            "epoch 35\n",
            " batch Loss train: 0.05731385201215744\n",
            "i 14\n",
            "epoch 35\n",
            " batch Loss train: 0.07144796848297119\n",
            "i 15\n",
            "epoch 35\n",
            " batch Loss train: 0.08890434354543686\n",
            "i 16\n",
            "epoch 35\n",
            " batch Loss train: 0.08741538226604462\n",
            "i 17\n",
            "epoch 35\n",
            " batch Loss train: 0.05639104172587395\n",
            "i 18\n",
            "epoch 35\n",
            " batch Loss train: 0.06829386949539185\n",
            "i 19\n",
            "epoch 35\n",
            " batch Loss train: 0.08190000057220459\n",
            "i 20\n",
            "epoch 35\n",
            " batch Loss train: 0.07377250492572784\n",
            "i 21\n",
            "epoch 35\n",
            " batch Loss train: 0.061533283442258835\n",
            "i 22\n",
            "epoch 35\n",
            " batch Loss train: 0.08451047539710999\n",
            "i 23\n",
            "epoch 35\n",
            " batch Loss train: 0.08124462515115738\n",
            "i 24\n",
            "epoch 35\n",
            " batch Loss train: 0.07688334584236145\n",
            "i 25\n",
            "epoch 35\n",
            " batch Loss train: 0.05888236686587334\n",
            "i 26\n",
            "epoch 35\n",
            " batch Loss train: 0.06353193521499634\n",
            "i 27\n",
            "epoch 35\n",
            " batch Loss train: 0.0879494771361351\n",
            "i 28\n",
            "epoch 35\n",
            " batch Loss train: 0.07105027139186859\n",
            "i 29\n",
            "epoch 35\n",
            " batch Loss train: 0.08789330720901489\n",
            "i 30\n",
            "epoch 35\n",
            " batch Loss train: 0.06033627316355705\n",
            "i 31\n",
            "epoch 35\n",
            " batch Loss train: 0.04889901354908943\n",
            "i 32\n",
            "epoch 35\n",
            " batch Loss train: 0.07181823998689651\n",
            "i 33\n",
            "epoch 35\n",
            " batch Loss train: 0.062464263290166855\n",
            "i 34\n",
            "epoch 35\n",
            " batch Loss train: 0.056178148835897446\n",
            "i 35\n",
            "epoch 35\n",
            " batch Loss train: 0.06835173815488815\n",
            "i 36\n",
            "epoch 35\n",
            " batch Loss train: 0.0559893473982811\n",
            "i 37\n",
            "epoch 35\n",
            " batch Loss train: 0.05462617427110672\n",
            "i 38\n",
            "epoch 35\n",
            " batch Loss train: 0.05939960107207298\n",
            "i 39\n",
            "epoch 35\n",
            " batch Loss train: 0.05456257238984108\n",
            "i 40\n",
            "epoch 35\n",
            " batch Loss train: 0.07296103239059448\n",
            "i 41\n",
            "epoch 35\n",
            " batch Loss train: 0.07212548702955246\n",
            "i 42\n",
            "epoch 35\n",
            " batch Loss train: 0.06315559148788452\n",
            "i 43\n",
            "epoch 35\n",
            " batch Loss train: 0.06856156140565872\n",
            "i 44\n",
            "epoch 35\n",
            " batch Loss train: 0.059521790593862534\n",
            "i 45\n",
            "epoch 35\n",
            " batch Loss train: 0.07639497518539429\n",
            "i 46\n",
            "epoch 35\n",
            " batch Loss train: 0.08349331468343735\n",
            "i 47\n",
            "epoch 35\n",
            " batch Loss train: 0.05919268727302551\n",
            "i 48\n",
            "epoch 35\n",
            " batch Loss train: 0.08668582141399384\n",
            "i 49\n",
            "epoch 35\n",
            " batch Loss train: 0.07216577976942062\n",
            "i 50\n",
            "epoch 35\n",
            " batch Loss train: 0.07127740979194641\n",
            "i 51\n",
            "epoch 35\n",
            " batch Loss train: 0.07164612412452698\n",
            "i 52\n",
            "epoch 35\n",
            " batch Loss train: 0.04997658357024193\n",
            "i 53\n",
            "epoch 35\n",
            " batch Loss train: 0.08111037313938141\n",
            "i 54\n",
            "epoch 35\n",
            " batch Loss train: 0.07962783426046371\n",
            "i 55\n",
            "epoch 35\n",
            " batch Loss train: 0.08132604509592056\n",
            "i 56\n",
            "epoch 35\n",
            " batch Loss train: 0.06447065621614456\n",
            "i 57\n",
            "epoch 35\n",
            " batch Loss train: 0.06582050025463104\n",
            "i 58\n",
            "epoch 35\n",
            " batch Loss train: 0.07646209746599197\n",
            "i 59\n",
            "epoch 35\n",
            " batch Loss train: 0.06756798177957535\n",
            "i 60\n",
            "epoch 35\n",
            " batch Loss train: 0.07386770099401474\n",
            "i 61\n",
            "epoch 35\n",
            " batch Loss train: 0.10257381200790405\n",
            "i 62\n",
            "epoch 35\n",
            " batch Loss train: 0.056628089398145676\n",
            "i 63\n",
            "epoch 35\n",
            " batch Loss train: 0.0680212527513504\n",
            "i 64\n",
            "epoch 35\n",
            " batch Loss train: 0.0759313702583313\n",
            "i 65\n",
            "epoch 35\n",
            " batch Loss train: 0.06748120486736298\n",
            "i 66\n",
            "epoch 35\n",
            " batch Loss train: 0.06911633908748627\n",
            "i 67\n",
            "epoch 35\n",
            " batch Loss train: 0.05017010495066643\n",
            "i 68\n",
            "epoch 35\n",
            " batch Loss train: 0.07384005188941956\n",
            "i 69\n",
            "epoch 35\n",
            " batch Loss train: 0.0668327659368515\n",
            "i 70\n",
            "epoch 35\n",
            " batch Loss train: 0.05696004629135132\n",
            "i 71\n",
            "epoch 35\n",
            " batch Loss train: 0.09874972701072693\n",
            "i 72\n",
            "epoch 35\n",
            " batch Loss train: 0.06476695835590363\n",
            "i 73\n",
            "epoch 35\n",
            " batch Loss train: 0.08648653328418732\n",
            "i 74\n",
            "epoch 35\n",
            " batch Loss train: 0.07849329710006714\n",
            "i 75\n",
            "epoch 35\n",
            " batch Loss train: 0.061610471457242966\n",
            "i 76\n",
            "epoch 35\n",
            " batch Loss train: 0.0758143737912178\n",
            "i 77\n",
            "epoch 35\n",
            " batch Loss train: 0.07836496084928513\n",
            "i 78\n",
            "epoch 35\n",
            " batch Loss train: 0.07180050015449524\n",
            "i 79\n",
            "epoch 35\n",
            " batch Loss train: 0.07158063352108002\n",
            "i 80\n",
            "epoch 35\n",
            " batch Loss train: 0.06317167729139328\n",
            "i 81\n",
            "epoch 35\n",
            " batch Loss train: 0.07025256007909775\n",
            "i 82\n",
            "epoch 35\n",
            " batch Loss train: 0.08665291965007782\n",
            "i 83\n",
            "epoch 35\n",
            " batch Loss train: 0.07816391438245773\n",
            "i 84\n",
            "epoch 35\n",
            " batch Loss train: 0.07963111996650696\n",
            "i 85\n",
            "epoch 35\n",
            " batch Loss train: 0.07433351874351501\n",
            "i 86\n",
            "epoch 35\n",
            " batch Loss train: 0.08110733330249786\n",
            "i 87\n",
            "epoch 35\n",
            " batch Loss train: 0.07239465415477753\n",
            "i 88\n",
            "epoch 35\n",
            " batch Loss train: 0.061259154230356216\n",
            "i 89\n",
            "epoch 35\n",
            " batch Loss train: 0.0670350193977356\n",
            "i 90\n",
            "epoch 35\n",
            " batch Loss train: 0.0637388676404953\n",
            "i 91\n",
            "epoch 35\n",
            " batch Loss train: 0.05998992919921875\n",
            "i 92\n",
            "epoch 35\n",
            " batch Loss train: 0.07185616344213486\n",
            "i 93\n",
            "epoch 35\n",
            " batch Loss train: 0.06892484426498413\n",
            "i 94\n",
            "epoch 35\n",
            " batch Loss train: 0.07461816817522049\n",
            "i 95\n",
            "epoch 35\n",
            " batch Loss train: 0.09322316199541092\n",
            "i 96\n",
            "epoch 35\n",
            " batch Loss train: 0.06351516395807266\n",
            "i 97\n",
            "epoch 35\n",
            " batch Loss train: 0.06314050406217575\n",
            "i 98\n",
            "epoch 35\n",
            " batch Loss train: 0.0696479082107544\n",
            "i 99\n",
            "epoch 35\n",
            " batch Loss train: 0.07440516352653503\n",
            "i 100\n",
            "epoch 35\n",
            " batch Loss train: 0.0887909084558487\n",
            "i 101\n",
            "epoch 35\n",
            " batch Loss train: 0.07858682423830032\n",
            "i 102\n",
            "epoch 35\n",
            " batch Loss train: 0.054020266979932785\n",
            "i 103\n",
            "epoch 35\n",
            " batch Loss train: 0.07598886638879776\n",
            "i 104\n",
            "epoch 35\n",
            " batch Loss train: 0.06828638166189194\n",
            "i 105\n",
            "epoch 35\n",
            " batch Loss train: 0.07597052305936813\n",
            "i 106\n",
            "epoch 35\n",
            " batch Loss train: 0.0805288702249527\n",
            "i 107\n",
            "epoch 35\n",
            " batch Loss train: 0.059184037148952484\n",
            "i 108\n",
            "epoch 35\n",
            " batch Loss train: 0.08419379591941833\n",
            "i 109\n",
            "epoch 35\n",
            " batch Loss train: 0.07211871445178986\n",
            "i 110\n",
            "epoch 35\n",
            " batch Loss train: 0.07199326157569885\n",
            "i 111\n",
            "epoch 35\n",
            " batch Loss train: 0.08407483994960785\n",
            "i 112\n",
            "epoch 35\n",
            " batch Loss train: 0.07818956673145294\n",
            "i 113\n",
            "epoch 35\n",
            " batch Loss train: 0.07425294071435928\n",
            "i 114\n",
            "epoch 35\n",
            " batch Loss train: 0.0774829313158989\n",
            "i 115\n",
            "epoch 35\n",
            " batch Loss train: 0.05217334255576134\n",
            "i 116\n",
            "epoch 35\n",
            " batch Loss train: 0.07624982297420502\n",
            "i 117\n",
            "epoch 35\n",
            " batch Loss train: 0.07464471459388733\n",
            "i 118\n",
            "epoch 35\n",
            " batch Loss train: 0.08896792680025101\n",
            "i 119\n",
            "epoch 35\n",
            " batch Loss train: 0.09473343193531036\n",
            "i 120\n",
            "epoch 35\n",
            " batch Loss train: 0.07268337160348892\n",
            "i 121\n",
            "epoch 35\n",
            " batch Loss train: 0.08142805099487305\n",
            "i 122\n",
            "epoch 35\n",
            " batch Loss train: 0.06623591482639313\n",
            "i 123\n",
            "epoch 35\n",
            " batch Loss train: 0.06536226719617844\n",
            "i 124\n",
            "epoch 35\n",
            " batch Loss train: 0.06595493853092194\n",
            "i 125\n",
            "epoch 35\n",
            " batch Loss train: 0.08338882774114609\n",
            "i 126\n",
            "epoch 35\n",
            " batch Loss train: 0.07938536256551743\n",
            "i 127\n",
            "epoch 35\n",
            " batch Loss train: 0.06259516626596451\n",
            "i 128\n",
            "epoch 35\n",
            " batch Loss train: 0.07459460943937302\n",
            "i 129\n",
            "epoch 35\n",
            " batch Loss train: 0.11249177157878876\n",
            "i 130\n",
            "epoch 35\n",
            " batch Loss train: 0.07546762377023697\n",
            "i 131\n",
            "epoch 35\n",
            " batch Loss train: 0.08001618832349777\n",
            "i 132\n",
            "epoch 35\n",
            " batch Loss train: 0.0847286581993103\n",
            "i 133\n",
            "epoch 35\n",
            " batch Loss train: 0.09375772625207901\n",
            "i 134\n",
            "epoch 35\n",
            " batch Loss train: 0.07290486246347427\n",
            "i 135\n",
            "epoch 35\n",
            " batch Loss train: 0.09037092328071594\n",
            "i 136\n",
            "epoch 35\n",
            " batch Loss train: 0.06838205456733704\n",
            "i 137\n",
            "epoch 35\n",
            " batch Loss train: 0.07287269085645676\n",
            "i 138\n",
            "epoch 35\n",
            " batch Loss train: 0.08732621371746063\n",
            "i 139\n",
            "epoch 35\n",
            " batch Loss train: 0.061067696660757065\n",
            "i 140\n",
            "epoch 35\n",
            " batch Loss train: 0.12476110458374023\n",
            "i 141\n",
            "epoch 35\n",
            " batch Loss train: 0.07588216662406921\n",
            "i 142\n",
            "epoch 35\n",
            " batch Loss train: 0.07696373760700226\n",
            "i 143\n",
            "epoch 35\n",
            " batch Loss train: 0.07068291306495667\n",
            "i 144\n",
            "epoch 35\n",
            " batch Loss train: 0.07195756584405899\n",
            "i 145\n",
            "epoch 35\n",
            " batch Loss train: 0.10031984746456146\n",
            "i 146\n",
            "epoch 35\n",
            " batch Loss train: 0.06155824288725853\n",
            "i 147\n",
            "epoch 35\n",
            " batch Loss train: 0.06258564442396164\n",
            "i 148\n",
            "epoch 35\n",
            " batch Loss train: 0.06747075170278549\n",
            "i 149\n",
            "epoch 35\n",
            " batch Loss train: 0.09303923696279526\n",
            "i 150\n",
            "epoch 35\n",
            " batch Loss train: 0.06564155220985413\n",
            "i 151\n",
            "epoch 35\n",
            " batch Loss train: 0.0700291097164154\n",
            "i 152\n",
            "epoch 35\n",
            " batch Loss train: 0.08961528539657593\n",
            "i 153\n",
            "epoch 35\n",
            " batch Loss train: 0.06435905396938324\n",
            "i 154\n",
            "epoch 35\n",
            " batch Loss train: 0.07350203394889832\n",
            "i 155\n",
            "epoch 35\n",
            " batch Loss train: 0.06729407608509064\n",
            "i 156\n",
            "epoch 35\n",
            " batch Loss train: 0.0655384212732315\n",
            "i 157\n",
            "epoch 35\n",
            " batch Loss train: 0.06740597635507584\n",
            "i 158\n",
            "epoch 35\n",
            " batch Loss train: 0.0593860037624836\n",
            "i 159\n",
            "epoch 35\n",
            " batch Loss train: 0.09183676540851593\n",
            "i 160\n",
            "epoch 35\n",
            " batch Loss train: 0.06737442314624786\n",
            "i 161\n",
            "epoch 35\n",
            " batch Loss train: 0.07162507623434067\n",
            "i 162\n",
            "epoch 35\n",
            " batch Loss train: 0.06478938460350037\n",
            "i 163\n",
            "epoch 35\n",
            " batch Loss train: 0.060043685138225555\n",
            "i 164\n",
            "epoch 35\n",
            " batch Loss train: 0.06055878847837448\n",
            "i 165\n",
            "epoch 35\n",
            " batch Loss train: 0.08731548488140106\n",
            "i 166\n",
            "epoch 35\n",
            " batch Loss train: 0.07868743687868118\n",
            "i 167\n",
            "epoch 35\n",
            " batch Loss train: 0.057567186653614044\n",
            "i 168\n",
            "epoch 35\n",
            " batch Loss train: 0.08416987955570221\n",
            "i 169\n",
            "epoch 35\n",
            " batch Loss train: 0.08576314896345139\n",
            "i 170\n",
            "epoch 35\n",
            " batch Loss train: 0.10447053611278534\n",
            "i 171\n",
            "epoch 35\n",
            " batch Loss train: 0.06019343063235283\n",
            "i 172\n",
            "epoch 35\n",
            " batch Loss train: 0.05895662307739258\n",
            "i 173\n",
            "epoch 35\n",
            " batch Loss train: 0.06865154206752777\n",
            "i 174\n",
            "epoch 35\n",
            " batch Loss train: 0.10047478973865509\n",
            "i 175\n",
            "epoch 35\n",
            " batch Loss train: 0.09057717025279999\n",
            "i 176\n",
            "epoch 35\n",
            " batch Loss train: 0.09332919120788574\n",
            "i 177\n",
            "epoch 35\n",
            " batch Loss train: 0.08458549529314041\n",
            "i 178\n",
            "epoch 35\n",
            " batch Loss train: 0.07231853157281876\n",
            "i 179\n",
            "epoch 35\n",
            " batch Loss train: 0.06562428921461105\n",
            "i 180\n",
            "epoch 35\n",
            " batch Loss train: 0.0701751634478569\n",
            "i 181\n",
            "epoch 35\n",
            " batch Loss train: 0.06329011917114258\n",
            "i 182\n",
            "epoch 35\n",
            " batch Loss train: 0.07113920152187347\n",
            "i 183\n",
            "epoch 35\n",
            " batch Loss train: 0.08492499589920044\n",
            "i 184\n",
            "epoch 35\n",
            " batch Loss train: 0.06843752413988113\n",
            "i 185\n",
            "epoch 35\n",
            " batch Loss train: 0.0668201595544815\n",
            "i 186\n",
            "epoch 35\n",
            " batch Loss train: 0.07649189978837967\n",
            "i 187\n",
            "epoch 35\n",
            " batch Loss train: 0.0848890021443367\n",
            "i 188\n",
            "epoch 35\n",
            " batch Loss train: 0.07114546746015549\n",
            "i 189\n",
            "epoch 35\n",
            " batch Loss train: 0.0735514909029007\n",
            "i 190\n",
            "epoch 35\n",
            " batch Loss train: 0.07694525271654129\n",
            "i 191\n",
            "epoch 35\n",
            " batch Loss train: 0.06507576256990433\n",
            "i 192\n",
            "epoch 35\n",
            " batch Loss train: 0.08329617977142334\n",
            "i 193\n",
            "epoch 35\n",
            " batch Loss train: 0.07551325857639313\n",
            "i 194\n",
            "epoch 35\n",
            " batch Loss train: 0.08294392377138138\n",
            "i 195\n",
            "epoch 35\n",
            " batch Loss train: 0.07947630435228348\n",
            "i 196\n",
            "epoch 35\n",
            " batch Loss train: 0.05938753858208656\n",
            "i 197\n",
            "epoch 35\n",
            " batch Loss train: 0.0675770565867424\n",
            "i 198\n",
            "epoch 35\n",
            " batch Loss train: 0.11175300925970078\n",
            "i 199\n",
            "epoch 35\n",
            " batch Loss train: 0.07909546047449112\n",
            "i 200\n",
            "epoch 35\n",
            " batch Loss train: 0.09309467673301697\n",
            "i 201\n",
            "epoch 35\n",
            " batch Loss train: 0.07387454062700272\n",
            "i 202\n",
            "epoch 35\n",
            " batch Loss train: 0.0810542032122612\n",
            "i 203\n",
            "epoch 35\n",
            " batch Loss train: 0.08203703910112381\n",
            "i 204\n",
            "epoch 35\n",
            " batch Loss train: 0.07075361907482147\n",
            "i 205\n",
            "epoch 35\n",
            " batch Loss train: 0.07650791853666306\n",
            "i 206\n",
            "epoch 35\n",
            " batch Loss train: 0.08201877772808075\n",
            "i 207\n",
            "epoch 35\n",
            " batch Loss train: 0.0767069086432457\n",
            "i 208\n",
            "epoch 35\n",
            " batch Loss train: 0.08941201120615005\n",
            "i 209\n",
            "epoch 35\n",
            " batch Loss train: 0.08701983094215393\n",
            "i 210\n",
            "epoch 35\n",
            " batch Loss train: 0.0842275395989418\n",
            "i 211\n",
            "epoch 35\n",
            " batch Loss train: 0.11448288708925247\n",
            "i 212\n",
            "epoch 35\n",
            " batch Loss train: 0.08415596932172775\n",
            "i 213\n",
            "epoch 35\n",
            " batch Loss train: 0.06845969706773758\n",
            "i 214\n",
            "epoch 35\n",
            " batch Loss train: 0.1090330183506012\n",
            "i 215\n",
            "epoch 35\n",
            " batch Loss train: 0.07849278301000595\n",
            "i 216\n",
            "epoch 35\n",
            " batch Loss train: 0.08303576707839966\n",
            "i 217\n",
            "epoch 35\n",
            " batch Loss train: 0.06566166132688522\n",
            "i 218\n",
            "epoch 35\n",
            " batch Loss train: 0.06100229173898697\n",
            "i 219\n",
            "epoch 35\n",
            " batch Loss train: 0.08297865092754364\n",
            "i 220\n",
            "epoch 35\n",
            " batch Loss train: 0.0647507756948471\n",
            "i 221\n",
            "epoch 35\n",
            " batch Loss train: 0.07071129232645035\n",
            "i 222\n",
            "epoch 35\n",
            " batch Loss train: 0.06786629557609558\n",
            "i 223\n",
            "epoch 35\n",
            " batch Loss train: 0.0760866105556488\n",
            "i 224\n",
            "epoch 35\n",
            " batch Loss train: 0.07749009877443314\n",
            "i 225\n",
            "epoch 35\n",
            " batch Loss train: 0.08776021003723145\n",
            "i 226\n",
            "epoch 35\n",
            " batch Loss train: 0.07968447357416153\n",
            "i 227\n",
            "epoch 35\n",
            " batch Loss train: 0.0792020782828331\n",
            "i 228\n",
            "epoch 35\n",
            " batch Loss train: 0.0907033309340477\n",
            "i 229\n",
            "epoch 35\n",
            " batch Loss train: 0.06781946122646332\n",
            "i 230\n",
            "epoch 35\n",
            " batch Loss train: 0.07454659789800644\n",
            "i 231\n",
            "epoch 35\n",
            " batch Loss train: 0.06432259827852249\n",
            "i 232\n",
            "epoch 35\n",
            " batch Loss train: 0.07160598784685135\n",
            "i 233\n",
            "epoch 35\n",
            " batch Loss train: 0.09363984316587448\n",
            "i 234\n",
            "epoch 35\n",
            " batch Loss train: 0.06594017148017883\n",
            "i 235\n",
            "epoch 35\n",
            " batch Loss train: 0.09736551344394684\n",
            "i 236\n",
            "epoch 35\n",
            " batch Loss train: 0.08837064355611801\n",
            "i 237\n",
            "epoch 35\n",
            " batch Loss train: 0.08945891261100769\n",
            "i 238\n",
            "epoch 35\n",
            " batch Loss train: 0.1302626132965088\n",
            "i 239\n",
            "epoch 35\n",
            " batch Loss train: 0.08530516922473907\n",
            "i 240\n",
            "epoch 35\n",
            " batch Loss train: 0.08712303638458252\n",
            "i 241\n",
            "epoch 35\n",
            " batch Loss train: 0.08274811506271362\n",
            "i 242\n",
            "epoch 35\n",
            " batch Loss train: 0.10076772421598434\n",
            "i 243\n",
            "epoch 35\n",
            " batch Loss train: 0.13041825592517853\n",
            "i 244\n",
            "epoch 35\n",
            " batch Loss train: 0.11503396928310394\n",
            "i 245\n",
            "epoch 35\n",
            " batch Loss train: 0.06291797012090683\n",
            "i 246\n",
            "epoch 35\n",
            " batch Loss train: 0.0881979689002037\n",
            "i 247\n",
            "epoch 35\n",
            " batch Loss train: 0.07922601699829102\n",
            "i 248\n",
            "epoch 35\n",
            " batch Loss train: 0.08418066799640656\n",
            "i 249\n",
            "epoch 35\n",
            " batch Loss train: 0.060988783836364746\n",
            "i 250\n",
            "epoch 35\n",
            " batch Loss train: 0.05976337939500809\n",
            "i 251\n",
            "epoch 35\n",
            " batch Loss train: 0.08955865353345871\n",
            "i 252\n",
            "epoch 35\n",
            " batch Loss train: 0.08176637440919876\n",
            "i 253\n",
            "epoch 35\n",
            " batch Loss train: 0.08335455507040024\n",
            "i 254\n",
            "epoch 35\n",
            " batch Loss train: 0.07501846551895142\n",
            "i 255\n",
            "epoch 35\n",
            " batch Loss train: 0.07483480870723724\n",
            "i 256\n",
            "epoch 35\n",
            " batch Loss train: 0.09695541858673096\n",
            "i 257\n",
            "epoch 35\n",
            " batch Loss train: 0.07672708481550217\n",
            "i 258\n",
            "epoch 35\n",
            " batch Loss train: 0.07767754048109055\n",
            "i 259\n",
            "epoch 35\n",
            " batch Loss train: 0.07693499326705933\n",
            "i 260\n",
            "epoch 35\n",
            " batch Loss train: 0.08186030387878418\n",
            "i 261\n",
            "epoch 35\n",
            " batch Loss train: 0.08503761887550354\n",
            "i 262\n",
            "epoch 35\n",
            " batch Loss train: 0.09260932356119156\n",
            "i 263\n",
            "epoch 35\n",
            " batch Loss train: 0.06166782230138779\n",
            "i 264\n",
            "epoch 35\n",
            " batch Loss train: 0.09447415918111801\n",
            "i 265\n",
            "epoch 35\n",
            " batch Loss train: 0.0774720162153244\n",
            "i 266\n",
            "epoch 35\n",
            " batch Loss train: 0.11416073888540268\n",
            "i 267\n",
            "epoch 35\n",
            " batch Loss train: 0.0630602091550827\n",
            "i 268\n",
            "epoch 35\n",
            " batch Loss train: 0.08280640095472336\n",
            "i 269\n",
            "epoch 35\n",
            " batch Loss train: 0.10453975945711136\n",
            "i 270\n",
            "epoch 35\n",
            " batch Loss train: 0.07543978095054626\n",
            "i 271\n",
            "epoch 35\n",
            " batch Loss train: 0.056284770369529724\n",
            "i 272\n",
            "epoch 35\n",
            " batch Loss train: 0.07025163620710373\n",
            "i 273\n",
            "epoch 35\n",
            " batch Loss train: 0.08668246120214462\n",
            "i 274\n",
            "epoch 35\n",
            " batch Loss train: 0.09526225179433823\n",
            "i 275\n",
            "epoch 35\n",
            " batch Loss train: 0.07315131276845932\n",
            "i 276\n",
            "epoch 35\n",
            " batch Loss train: 0.08871541172266006\n",
            "i 277\n",
            "epoch 35\n",
            " batch Loss train: 0.08297520130872726\n",
            "i 278\n",
            "epoch 35\n",
            " batch Loss train: 0.08987542241811752\n",
            "i 279\n",
            "epoch 35\n",
            " batch Loss train: 0.07787866145372391\n",
            "i 280\n",
            "epoch 35\n",
            " batch Loss train: 0.07193183898925781\n",
            "i 281\n",
            "epoch 35\n",
            " batch Loss train: 0.08284255862236023\n",
            "i 282\n",
            "epoch 35\n",
            " batch Loss train: 0.060499921441078186\n",
            "i 283\n",
            "epoch 35\n",
            " batch Loss train: 0.059527359902858734\n",
            "i 284\n",
            "epoch 35\n",
            " batch Loss train: 0.060404933989048004\n",
            "i 285\n",
            "epoch 35\n",
            " batch Loss train: 0.07389166951179504\n",
            "i 286\n",
            "epoch 35\n",
            " batch Loss train: 0.0761505737900734\n",
            "i 287\n",
            "epoch 35\n",
            " batch Loss train: 0.06891487538814545\n",
            "i 288\n",
            "epoch 35\n",
            " batch Loss train: 0.06204494833946228\n",
            "i 289\n",
            "epoch 35\n",
            " batch Loss train: 0.06219865754246712\n",
            "i 290\n",
            "epoch 35\n",
            " batch Loss train: 0.07196149230003357\n",
            "i 291\n",
            "epoch 35\n",
            " batch Loss train: 0.07757629454135895\n",
            "i 292\n",
            "epoch 35\n",
            " batch Loss train: 0.06676428765058517\n",
            "i 293\n",
            "epoch 35\n",
            " batch Loss train: 0.07540526986122131\n",
            "i 294\n",
            "epoch 35\n",
            " batch Loss train: 0.05981624126434326\n",
            "i 295\n",
            "epoch 35\n",
            " batch Loss train: 0.07265215367078781\n",
            "i 296\n",
            "epoch 35\n",
            " batch Loss train: 0.07226196676492691\n",
            "i 297\n",
            "epoch 35\n",
            " batch Loss train: 0.06997007131576538\n",
            "i 298\n",
            "epoch 35\n",
            " batch Loss train: 0.066416434943676\n",
            "i 299\n",
            "epoch 35\n",
            " batch Loss train: 0.06999492645263672\n",
            "i 300\n",
            "epoch 35\n",
            " batch Loss train: 0.08567142486572266\n",
            "i 301\n",
            "epoch 35\n",
            " batch Loss train: 0.09891685098409653\n",
            "i 302\n",
            "epoch 35\n",
            " batch Loss train: 0.077741339802742\n",
            "i 303\n",
            "epoch 35\n",
            " batch Loss train: 0.10654088109731674\n",
            "i 304\n",
            "epoch 35\n",
            " batch Loss train: 0.071633480489254\n",
            "i 305\n",
            "epoch 35\n",
            " batch Loss train: 0.07663778960704803\n",
            "i 306\n",
            "epoch 35\n",
            " batch Loss train: 0.0943688228726387\n",
            "i 307\n",
            "epoch 35\n",
            " batch Loss train: 0.0987580344080925\n",
            "i 308\n",
            "epoch 35\n",
            " batch Loss train: 0.06656414270401001\n",
            "i 309\n",
            "epoch 35\n",
            " batch Loss train: 0.07138992100954056\n",
            "i 310\n",
            "epoch 35\n",
            " batch Loss train: 0.09559422731399536\n",
            "i 311\n",
            "epoch 35\n",
            " batch Loss train: 0.07409577816724777\n",
            "i 312\n",
            "epoch 35\n",
            " batch Loss train: 0.10246054083108902\n",
            "i 313\n",
            "epoch 35\n",
            " batch Loss train: 0.0717553123831749\n",
            "i 314\n",
            "epoch 35\n",
            " batch Loss train: 0.07129071652889252\n",
            "i 315\n",
            "epoch 35\n",
            " batch Loss train: 0.12480685859918594\n",
            "i 316\n",
            "epoch 35\n",
            " batch Loss train: 0.06594312936067581\n",
            "i 317\n",
            "epoch 35\n",
            " batch Loss train: 0.08406633883714676\n",
            "i 318\n",
            "epoch 35\n",
            " batch Loss train: 0.0692623108625412\n",
            "i 319\n",
            "epoch 35\n",
            " batch Loss train: 0.09466203302145004\n",
            "i 320\n",
            "epoch 35\n",
            " batch Loss train: 0.1013120785355568\n",
            "i 321\n",
            "epoch 35\n",
            " batch Loss train: 0.07869232445955276\n",
            "i 322\n",
            "epoch 35\n",
            " batch Loss train: 0.08597705513238907\n",
            "i 323\n",
            "epoch 35\n",
            " batch Loss train: 0.09106876701116562\n",
            "i 324\n",
            "epoch 35\n",
            " batch Loss train: 0.09142066538333893\n",
            "i 325\n",
            "epoch 35\n",
            " batch Loss train: 0.06894243508577347\n",
            "i 326\n",
            "epoch 35\n",
            " batch Loss train: 0.08312493562698364\n",
            "i 327\n",
            "epoch 35\n",
            " batch Loss train: 0.07612910866737366\n",
            "i 328\n",
            "epoch 35\n",
            " batch Loss train: 0.0644131600856781\n",
            "i 329\n",
            "epoch 35\n",
            " batch Loss train: 0.08495648205280304\n",
            "i 330\n",
            "epoch 35\n",
            " batch Loss train: 0.07353443652391434\n",
            "i 331\n",
            "epoch 35\n",
            " batch Loss train: 0.07194481045007706\n",
            "i 332\n",
            "epoch 35\n",
            " batch Loss train: 0.08079027384519577\n",
            "i 333\n",
            "epoch 35\n",
            " batch Loss train: 0.09343927353620529\n",
            "i 334\n",
            "epoch 35\n",
            " batch Loss train: 0.0850076973438263\n",
            "i 335\n",
            "epoch 35\n",
            " batch Loss train: 0.08265942335128784\n",
            "i 336\n",
            "epoch 35\n",
            " batch Loss train: 0.07434965670108795\n",
            "i 337\n",
            "epoch 35\n",
            " batch Loss train: 0.08981014788150787\n",
            "i 338\n",
            "epoch 35\n",
            " batch Loss train: 0.10460365563631058\n",
            "i 339\n",
            "epoch 35\n",
            " batch Loss train: 0.08272551000118256\n",
            "i 340\n",
            "epoch 35\n",
            " batch Loss train: 0.07310818880796432\n",
            "i 341\n",
            "epoch 35\n",
            " batch Loss train: 0.07914596796035767\n",
            "i 342\n",
            "epoch 35\n",
            " batch Loss train: 0.08673638105392456\n",
            "i 343\n",
            "epoch 35\n",
            " batch Loss train: 0.08579253405332565\n",
            "i 344\n",
            "epoch 35\n",
            " batch Loss train: 0.08435425162315369\n",
            "i 345\n",
            "epoch 35\n",
            " batch Loss train: 0.0695791244506836\n",
            "i 346\n",
            "epoch 35\n",
            " batch Loss train: 0.07279593497514725\n",
            "i 347\n",
            "epoch 35\n",
            " batch Loss train: 0.07508915662765503\n",
            "i 348\n",
            "epoch 35\n",
            " batch Loss train: 0.08585633337497711\n",
            "i 349\n",
            "epoch 35\n",
            " batch Loss train: 0.09257964044809341\n",
            "i 350\n",
            "epoch 35\n",
            " batch Loss train: 0.09626380354166031\n",
            "i 351\n",
            "epoch 35\n",
            " batch Loss train: 0.0770593211054802\n",
            "i 352\n",
            "epoch 35\n",
            " batch Loss train: 0.052412863820791245\n",
            "i 353\n",
            "epoch 35\n",
            " batch Loss train: 0.08901621401309967\n",
            "i 354\n",
            "epoch 35\n",
            " batch Loss train: 0.07325636595487595\n",
            "i 355\n",
            "epoch 35\n",
            " batch Loss train: 0.07155999541282654\n",
            "i 356\n",
            "epoch 35\n",
            " batch Loss train: 0.09025172144174576\n",
            "i 357\n",
            "epoch 35\n",
            " batch Loss train: 0.0871981680393219\n",
            "i 358\n",
            "epoch 35\n",
            " batch Loss train: 0.0798841044306755\n",
            "i 359\n",
            "epoch 35\n",
            " batch Loss train: 0.0815938338637352\n",
            "i 360\n",
            "epoch 35\n",
            " batch Loss train: 0.0887981429696083\n",
            "i 361\n",
            "epoch 35\n",
            " batch Loss train: 0.07840341329574585\n",
            "i 362\n",
            "epoch 35\n",
            " batch Loss train: 0.06790155917406082\n",
            "i 363\n",
            "epoch 35\n",
            " batch Loss train: 0.07719221711158752\n",
            "i 364\n",
            "epoch 35\n",
            " batch Loss train: 0.05543575435876846\n",
            "i 365\n",
            "epoch 35\n",
            " batch Loss train: 0.09737604856491089\n",
            "i 366\n",
            "epoch 35\n",
            " batch Loss train: 0.0974816381931305\n",
            "i 367\n",
            "epoch 35\n",
            " batch Loss train: 0.07405988872051239\n",
            "i 368\n",
            "epoch 35\n",
            " batch Loss train: 0.07672629505395889\n",
            "i 369\n",
            "epoch 35\n",
            " batch Loss train: 0.09726609289646149\n",
            "i 370\n",
            "epoch 35\n",
            " batch Loss train: 0.070046067237854\n",
            "i 371\n",
            "epoch 35\n",
            " batch Loss train: 0.08107827603816986\n",
            "i 372\n",
            "epoch 35\n",
            " batch Loss train: 0.08835147321224213\n",
            "i 373\n",
            "epoch 35\n",
            " batch Loss train: 0.08094129711389542\n",
            "i 374\n",
            "epoch 35\n",
            " batch Loss train: 0.08321661502122879\n",
            "i 375\n",
            "epoch 35\n",
            " batch Loss train: 0.08565918356180191\n",
            "i 376\n",
            "epoch 35\n",
            " batch Loss train: 0.09138653427362442\n",
            "i 377\n",
            "epoch 35\n",
            " batch Loss train: 0.06693941354751587\n",
            "i 378\n",
            "epoch 35\n",
            " batch Loss train: 0.08741727471351624\n",
            "i 379\n",
            "epoch 35\n",
            " batch Loss train: 0.08279090374708176\n",
            "i 380\n",
            "epoch 35\n",
            " batch Loss train: 0.07732215523719788\n",
            "i 381\n",
            "epoch 35\n",
            " batch Loss train: 0.07727586477994919\n",
            "i 382\n",
            "epoch 35\n",
            " batch Loss train: 0.07005951553583145\n",
            "i 383\n",
            "epoch 35\n",
            " batch Loss train: 0.07807480543851852\n",
            "i 384\n",
            "epoch 35\n",
            " batch Loss train: 0.0793944001197815\n",
            "i 385\n",
            "epoch 35\n",
            " batch Loss train: 0.08742216974496841\n",
            "i 386\n",
            "epoch 35\n",
            " batch Loss train: 0.09344302862882614\n",
            "i 387\n",
            "epoch 35\n",
            " batch Loss train: 0.0896967202425003\n",
            "i 388\n",
            "epoch 35\n",
            " batch Loss train: 0.09200040251016617\n",
            "i 389\n",
            "epoch 35\n",
            " batch Loss train: 0.09275265038013458\n",
            "i 390\n",
            "epoch 35\n",
            " batch Loss train: 0.08029627054929733\n",
            "i 391\n",
            "epoch 35\n",
            " batch Loss train: 0.10098408907651901\n",
            "i 392\n",
            "epoch 35\n",
            " batch Loss train: 0.09123649448156357\n",
            "i 393\n",
            "epoch 35\n",
            " batch Loss train: 0.09107908606529236\n",
            "i 394\n",
            "epoch 35\n",
            " batch Loss train: 0.08361310511827469\n",
            "i 395\n",
            "epoch 35\n",
            " batch Loss train: 0.09007866680622101\n",
            "i 396\n",
            "epoch 35\n",
            " batch Loss train: 0.11262363195419312\n",
            "i 397\n",
            "epoch 35\n",
            " batch Loss train: 0.07996174693107605\n",
            "i 398\n",
            "epoch 35\n",
            " batch Loss train: 0.1279950588941574\n",
            "i 399\n",
            "epoch 35\n",
            " batch Loss train: 0.06965679675340652\n",
            "i 400\n",
            "epoch 35\n",
            " batch Loss train: 0.08318540453910828\n",
            "i 401\n",
            "epoch 35\n",
            " batch Loss train: 0.08211775124073029\n",
            "i 402\n",
            "epoch 35\n",
            " batch Loss train: 0.127168208360672\n",
            "i 403\n",
            "epoch 35\n",
            " batch Loss train: 0.06783174723386765\n",
            "i 404\n",
            "epoch 35\n",
            " batch Loss train: 0.08608580380678177\n",
            "i 405\n",
            "epoch 35\n",
            " batch Loss train: 0.07641026377677917\n",
            "i 406\n",
            "epoch 35\n",
            " batch Loss train: 0.061176661401987076\n",
            "i 407\n",
            "epoch 35\n",
            " batch Loss train: 0.08635684102773666\n",
            "i 408\n",
            "epoch 35\n",
            " batch Loss train: 0.08051376044750214\n",
            "i 409\n",
            "epoch 35\n",
            " batch Loss train: 0.07933337986469269\n",
            "i 410\n",
            "epoch 35\n",
            " batch Loss train: 0.08290494233369827\n",
            "i 411\n",
            "epoch 35\n",
            " batch Loss train: 0.08232133090496063\n",
            "i 412\n",
            "epoch 35\n",
            " batch Loss train: 0.08906052261590958\n",
            "i 413\n",
            "epoch 35\n",
            " batch Loss train: 0.10059583932161331\n",
            "i 414\n",
            "epoch 35\n",
            " batch Loss train: 0.08549212664365768\n",
            "i 415\n",
            "epoch 35\n",
            " batch Loss train: 0.0767015591263771\n",
            "i 416\n",
            "epoch 35\n",
            " batch Loss train: 0.09783011674880981\n",
            "i 417\n",
            "epoch 35\n",
            " batch Loss train: 0.10669135302305222\n",
            "i 418\n",
            "epoch 35\n",
            " batch Loss train: 0.08226615935564041\n",
            "i 419\n",
            "epoch 35\n",
            " batch Loss train: 0.08378636091947556\n",
            "i 420\n",
            "epoch 35\n",
            " batch Loss train: 0.09494722634553909\n",
            "i 421\n",
            "epoch 35\n",
            " batch Loss train: 0.08053077012300491\n",
            "i 422\n",
            "epoch 35\n",
            " batch Loss train: 0.09666261076927185\n",
            "i 423\n",
            "epoch 35\n",
            " batch Loss train: 0.10288434475660324\n",
            "i 424\n",
            "epoch 35\n",
            " batch Loss train: 0.08087915927171707\n",
            "i 425\n",
            "epoch 35\n",
            " batch Loss train: 0.07622537016868591\n",
            "i 426\n",
            "epoch 35\n",
            " batch Loss train: 0.057643432170152664\n",
            "i 427\n",
            "epoch 35\n",
            " batch Loss train: 0.07503759860992432\n",
            "i 428\n",
            "epoch 35\n",
            " batch Loss train: 0.0855131596326828\n",
            "i 429\n",
            "epoch 35\n",
            " batch Loss train: 0.1170668751001358\n",
            "i 430\n",
            "epoch 35\n",
            " batch Loss train: 0.07468072324991226\n",
            "i 431\n",
            "epoch 35\n",
            " batch Loss train: 0.08332574367523193\n",
            "i 432\n",
            "epoch 35\n",
            " batch Loss train: 0.09713747352361679\n",
            "i 433\n",
            "epoch 35\n",
            " batch Loss train: 0.0959153100848198\n",
            "i 434\n",
            "epoch 35\n",
            " batch Loss train: 0.09543921053409576\n",
            "i 435\n",
            "epoch 35\n",
            " batch Loss train: 0.08179134875535965\n",
            "i 436\n",
            "epoch 35\n",
            " batch Loss train: 0.08560964465141296\n",
            "i 437\n",
            "epoch 35\n",
            " batch Loss train: 0.095098115503788\n",
            "i 438\n",
            "epoch 35\n",
            " batch Loss train: 0.07435519993305206\n",
            "i 439\n",
            "epoch 35\n",
            " batch Loss train: 0.08028265088796616\n",
            "i 440\n",
            "epoch 35\n",
            " batch Loss train: 0.06633102893829346\n",
            "i 441\n",
            "epoch 35\n",
            " batch Loss train: 0.08103244006633759\n",
            "i 442\n",
            "epoch 35\n",
            " batch Loss train: 0.0927506610751152\n",
            "i 443\n",
            "epoch 35\n",
            " batch Loss train: 0.09183712303638458\n",
            "i 444\n",
            "epoch 35\n",
            " batch Loss train: 0.09060848504304886\n",
            "i 445\n",
            "epoch 35\n",
            " batch Loss train: 0.109134741127491\n",
            "total epoch Loss train: tensor(0.1091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 36\n",
            " batch Loss train: 0.07305146008729935\n",
            "i 1\n",
            "epoch 36\n",
            " batch Loss train: 0.07424094527959824\n",
            "i 2\n",
            "epoch 36\n",
            " batch Loss train: 0.0649123415350914\n",
            "i 3\n",
            "epoch 36\n",
            " batch Loss train: 0.06494761258363724\n",
            "i 4\n",
            "epoch 36\n",
            " batch Loss train: 0.07172641158103943\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 36\n",
            " batch Loss train: 0.08053100109100342\n",
            "i 6\n",
            "epoch 36\n",
            " batch Loss train: 0.06245715171098709\n",
            "i 7\n",
            "epoch 36\n",
            " batch Loss train: 0.06557504832744598\n",
            "i 8\n",
            "epoch 36\n",
            " batch Loss train: 0.05842282995581627\n",
            "i 9\n",
            "epoch 36\n",
            " batch Loss train: 0.05908772721886635\n",
            "i 10\n",
            "epoch 36\n",
            " batch Loss train: 0.08475249260663986\n",
            "i 11\n",
            "epoch 36\n",
            " batch Loss train: 0.07332292199134827\n",
            "i 12\n",
            "epoch 36\n",
            " batch Loss train: 0.06878187507390976\n",
            "i 13\n",
            "epoch 36\n",
            " batch Loss train: 0.07952719181776047\n",
            "i 14\n",
            "epoch 36\n",
            " batch Loss train: 0.06707831472158432\n",
            "i 15\n",
            "epoch 36\n",
            " batch Loss train: 0.10435260087251663\n",
            "i 16\n",
            "epoch 36\n",
            " batch Loss train: 0.0871119499206543\n",
            "i 17\n",
            "epoch 36\n",
            " batch Loss train: 0.06152136251330376\n",
            "i 18\n",
            "epoch 36\n",
            " batch Loss train: 0.05977417528629303\n",
            "i 19\n",
            "epoch 36\n",
            " batch Loss train: 0.06139695271849632\n",
            "i 20\n",
            "epoch 36\n",
            " batch Loss train: 0.05738241970539093\n",
            "i 21\n",
            "epoch 36\n",
            " batch Loss train: 0.07600824534893036\n",
            "i 22\n",
            "epoch 36\n",
            " batch Loss train: 0.05446430295705795\n",
            "i 23\n",
            "epoch 36\n",
            " batch Loss train: 0.08059398829936981\n",
            "i 24\n",
            "epoch 36\n",
            " batch Loss train: 0.07339146733283997\n",
            "i 25\n",
            "epoch 36\n",
            " batch Loss train: 0.0683349221944809\n",
            "i 26\n",
            "epoch 36\n",
            " batch Loss train: 0.06824494153261185\n",
            "i 27\n",
            "epoch 36\n",
            " batch Loss train: 0.08252715319395065\n",
            "i 28\n",
            "epoch 36\n",
            " batch Loss train: 0.07582249492406845\n",
            "i 29\n",
            "epoch 36\n",
            " batch Loss train: 0.07524359226226807\n",
            "i 30\n",
            "epoch 36\n",
            " batch Loss train: 0.06598702073097229\n",
            "i 31\n",
            "epoch 36\n",
            " batch Loss train: 0.05514880642294884\n",
            "i 32\n",
            "epoch 36\n",
            " batch Loss train: 0.06502410024404526\n",
            "i 33\n",
            "epoch 36\n",
            " batch Loss train: 0.08600562065839767\n",
            "i 34\n",
            "epoch 36\n",
            " batch Loss train: 0.061008013784885406\n",
            "i 35\n",
            "epoch 36\n",
            " batch Loss train: 0.07408387213945389\n",
            "i 36\n",
            "epoch 36\n",
            " batch Loss train: 0.0762314572930336\n",
            "i 37\n",
            "epoch 36\n",
            " batch Loss train: 0.10205810517072678\n",
            "i 38\n",
            "epoch 36\n",
            " batch Loss train: 0.0705823004245758\n",
            "i 39\n",
            "epoch 36\n",
            " batch Loss train: 0.05655151978135109\n",
            "i 40\n",
            "epoch 36\n",
            " batch Loss train: 0.07864401489496231\n",
            "i 41\n",
            "epoch 36\n",
            " batch Loss train: 0.06533215194940567\n",
            "i 42\n",
            "epoch 36\n",
            " batch Loss train: 0.07011646032333374\n",
            "i 43\n",
            "epoch 36\n",
            " batch Loss train: 0.06682923436164856\n",
            "i 44\n",
            "epoch 36\n",
            " batch Loss train: 0.06815841048955917\n",
            "i 45\n",
            "epoch 36\n",
            " batch Loss train: 0.0766657292842865\n",
            "i 46\n",
            "epoch 36\n",
            " batch Loss train: 0.045815419405698776\n",
            "i 47\n",
            "epoch 36\n",
            " batch Loss train: 0.07409732043743134\n",
            "i 48\n",
            "epoch 36\n",
            " batch Loss train: 0.07475794851779938\n",
            "i 49\n",
            "epoch 36\n",
            " batch Loss train: 0.07614469528198242\n",
            "i 50\n",
            "epoch 36\n",
            " batch Loss train: 0.06739281117916107\n",
            "i 51\n",
            "epoch 36\n",
            " batch Loss train: 0.0633411779999733\n",
            "i 52\n",
            "epoch 36\n",
            " batch Loss train: 0.05757764354348183\n",
            "i 53\n",
            "epoch 36\n",
            " batch Loss train: 0.061337392777204514\n",
            "i 54\n",
            "epoch 36\n",
            " batch Loss train: 0.07518955320119858\n",
            "i 55\n",
            "epoch 36\n",
            " batch Loss train: 0.06765933334827423\n",
            "i 56\n",
            "epoch 36\n",
            " batch Loss train: 0.05851581320166588\n",
            "i 57\n",
            "epoch 36\n",
            " batch Loss train: 0.07095178961753845\n",
            "i 58\n",
            "epoch 36\n",
            " batch Loss train: 0.066860631108284\n",
            "i 59\n",
            "epoch 36\n",
            " batch Loss train: 0.06880087405443192\n",
            "i 60\n",
            "epoch 36\n",
            " batch Loss train: 0.0847083330154419\n",
            "i 61\n",
            "epoch 36\n",
            " batch Loss train: 0.07764333486557007\n",
            "i 62\n",
            "epoch 36\n",
            " batch Loss train: 0.06335033476352692\n",
            "i 63\n",
            "epoch 36\n",
            " batch Loss train: 0.08462480455636978\n",
            "i 64\n",
            "epoch 36\n",
            " batch Loss train: 0.06998929381370544\n",
            "i 65\n",
            "epoch 36\n",
            " batch Loss train: 0.06808963418006897\n",
            "i 66\n",
            "epoch 36\n",
            " batch Loss train: 0.06634462624788284\n",
            "i 67\n",
            "epoch 36\n",
            " batch Loss train: 0.063452810049057\n",
            "i 68\n",
            "epoch 36\n",
            " batch Loss train: 0.06035182997584343\n",
            "i 69\n",
            "epoch 36\n",
            " batch Loss train: 0.08290180563926697\n",
            "i 70\n",
            "epoch 36\n",
            " batch Loss train: 0.08668047934770584\n",
            "i 71\n",
            "epoch 36\n",
            " batch Loss train: 0.05995273217558861\n",
            "i 72\n",
            "epoch 36\n",
            " batch Loss train: 0.08594868332147598\n",
            "i 73\n",
            "epoch 36\n",
            " batch Loss train: 0.06626930087804794\n",
            "i 74\n",
            "epoch 36\n",
            " batch Loss train: 0.07157178968191147\n",
            "i 75\n",
            "epoch 36\n",
            " batch Loss train: 0.09311079978942871\n",
            "i 76\n",
            "epoch 36\n",
            " batch Loss train: 0.08328703790903091\n",
            "i 77\n",
            "epoch 36\n",
            " batch Loss train: 0.07946337014436722\n",
            "i 78\n",
            "epoch 36\n",
            " batch Loss train: 0.0793621838092804\n",
            "i 79\n",
            "epoch 36\n",
            " batch Loss train: 0.07243793457746506\n",
            "i 80\n",
            "epoch 36\n",
            " batch Loss train: 0.051532477140426636\n",
            "i 81\n",
            "epoch 36\n",
            " batch Loss train: 0.06349654495716095\n",
            "i 82\n",
            "epoch 36\n",
            " batch Loss train: 0.06388497352600098\n",
            "i 83\n",
            "epoch 36\n",
            " batch Loss train: 0.07579397410154343\n",
            "i 84\n",
            "epoch 36\n",
            " batch Loss train: 0.05963357165455818\n",
            "i 85\n",
            "epoch 36\n",
            " batch Loss train: 0.055074166506528854\n",
            "i 86\n",
            "epoch 36\n",
            " batch Loss train: 0.07904380559921265\n",
            "i 87\n",
            "epoch 36\n",
            " batch Loss train: 0.07739175856113434\n",
            "i 88\n",
            "epoch 36\n",
            " batch Loss train: 0.06158353388309479\n",
            "i 89\n",
            "epoch 36\n",
            " batch Loss train: 0.07685603946447372\n",
            "i 90\n",
            "epoch 36\n",
            " batch Loss train: 0.059957355260849\n",
            "i 91\n",
            "epoch 36\n",
            " batch Loss train: 0.07851606607437134\n",
            "i 92\n",
            "epoch 36\n",
            " batch Loss train: 0.08306819945573807\n",
            "i 93\n",
            "epoch 36\n",
            " batch Loss train: 0.056249335408210754\n",
            "i 94\n",
            "epoch 36\n",
            " batch Loss train: 0.06156294792890549\n",
            "i 95\n",
            "epoch 36\n",
            " batch Loss train: 0.06155722960829735\n",
            "i 96\n",
            "epoch 36\n",
            " batch Loss train: 0.07488428801298141\n",
            "i 97\n",
            "epoch 36\n",
            " batch Loss train: 0.07846762984991074\n",
            "i 98\n",
            "epoch 36\n",
            " batch Loss train: 0.07484553009271622\n",
            "i 99\n",
            "epoch 36\n",
            " batch Loss train: 0.11639861762523651\n",
            "i 100\n",
            "epoch 36\n",
            " batch Loss train: 0.07502394169569016\n",
            "i 101\n",
            "epoch 36\n",
            " batch Loss train: 0.06437241286039352\n",
            "i 102\n",
            "epoch 36\n",
            " batch Loss train: 0.07556743174791336\n",
            "i 103\n",
            "epoch 36\n",
            " batch Loss train: 0.05437221750617027\n",
            "i 104\n",
            "epoch 36\n",
            " batch Loss train: 0.08259417116641998\n",
            "i 105\n",
            "epoch 36\n",
            " batch Loss train: 0.11370231211185455\n",
            "i 106\n",
            "epoch 36\n",
            " batch Loss train: 0.09417987614870071\n",
            "i 107\n",
            "epoch 36\n",
            " batch Loss train: 0.08562804758548737\n",
            "i 108\n",
            "epoch 36\n",
            " batch Loss train: 0.0697673037648201\n",
            "i 109\n",
            "epoch 36\n",
            " batch Loss train: 0.06955757737159729\n",
            "i 110\n",
            "epoch 36\n",
            " batch Loss train: 0.06631974130868912\n",
            "i 111\n",
            "epoch 36\n",
            " batch Loss train: 0.08738605678081512\n",
            "i 112\n",
            "epoch 36\n",
            " batch Loss train: 0.065127432346344\n",
            "i 113\n",
            "epoch 36\n",
            " batch Loss train: 0.08072788268327713\n",
            "i 114\n",
            "epoch 36\n",
            " batch Loss train: 0.07596834748983383\n",
            "i 115\n",
            "epoch 36\n",
            " batch Loss train: 0.07318268716335297\n",
            "i 116\n",
            "epoch 36\n",
            " batch Loss train: 0.0621420219540596\n",
            "i 117\n",
            "epoch 36\n",
            " batch Loss train: 0.05815145745873451\n",
            "i 118\n",
            "epoch 36\n",
            " batch Loss train: 0.07156825810670853\n",
            "i 119\n",
            "epoch 36\n",
            " batch Loss train: 0.054167017340660095\n",
            "i 120\n",
            "epoch 36\n",
            " batch Loss train: 0.06931237876415253\n",
            "i 121\n",
            "epoch 36\n",
            " batch Loss train: 0.09226924180984497\n",
            "i 122\n",
            "epoch 36\n",
            " batch Loss train: 0.06178386136889458\n",
            "i 123\n",
            "epoch 36\n",
            " batch Loss train: 0.06037285178899765\n",
            "i 124\n",
            "epoch 36\n",
            " batch Loss train: 0.0752335786819458\n",
            "i 125\n",
            "epoch 36\n",
            " batch Loss train: 0.07741573452949524\n",
            "i 126\n",
            "epoch 36\n",
            " batch Loss train: 0.07087888568639755\n",
            "i 127\n",
            "epoch 36\n",
            " batch Loss train: 0.07973378896713257\n",
            "i 128\n",
            "epoch 36\n",
            " batch Loss train: 0.07584845274686813\n",
            "i 129\n",
            "epoch 36\n",
            " batch Loss train: 0.07487980276346207\n",
            "i 130\n",
            "epoch 36\n",
            " batch Loss train: 0.06149434670805931\n",
            "i 131\n",
            "epoch 36\n",
            " batch Loss train: 0.06620167940855026\n",
            "i 132\n",
            "epoch 36\n",
            " batch Loss train: 0.06998307257890701\n",
            "i 133\n",
            "epoch 36\n",
            " batch Loss train: 0.06441956013441086\n",
            "i 134\n",
            "epoch 36\n",
            " batch Loss train: 0.06399678438901901\n",
            "i 135\n",
            "epoch 36\n",
            " batch Loss train: 0.055302880704402924\n",
            "i 136\n",
            "epoch 36\n",
            " batch Loss train: 0.07297977060079575\n",
            "i 137\n",
            "epoch 36\n",
            " batch Loss train: 0.08560416847467422\n",
            "i 138\n",
            "epoch 36\n",
            " batch Loss train: 0.06143491715192795\n",
            "i 139\n",
            "epoch 36\n",
            " batch Loss train: 0.08579898625612259\n",
            "i 140\n",
            "epoch 36\n",
            " batch Loss train: 0.059133321046829224\n",
            "i 141\n",
            "epoch 36\n",
            " batch Loss train: 0.06106966733932495\n",
            "i 142\n",
            "epoch 36\n",
            " batch Loss train: 0.07984349876642227\n",
            "i 143\n",
            "epoch 36\n",
            " batch Loss train: 0.0709008052945137\n",
            "i 144\n",
            "epoch 36\n",
            " batch Loss train: 0.05234142020344734\n",
            "i 145\n",
            "epoch 36\n",
            " batch Loss train: 0.07780367136001587\n",
            "i 146\n",
            "epoch 36\n",
            " batch Loss train: 0.05960898473858833\n",
            "i 147\n",
            "epoch 36\n",
            " batch Loss train: 0.07817066460847855\n",
            "i 148\n",
            "epoch 36\n",
            " batch Loss train: 0.08103112131357193\n",
            "i 149\n",
            "epoch 36\n",
            " batch Loss train: 0.05882732570171356\n",
            "i 150\n",
            "epoch 36\n",
            " batch Loss train: 0.06040577217936516\n",
            "i 151\n",
            "epoch 36\n",
            " batch Loss train: 0.10728525370359421\n",
            "i 152\n",
            "epoch 36\n",
            " batch Loss train: 0.06462754309177399\n",
            "i 153\n",
            "epoch 36\n",
            " batch Loss train: 0.0832328051328659\n",
            "i 154\n",
            "epoch 36\n",
            " batch Loss train: 0.07682464271783829\n",
            "i 155\n",
            "epoch 36\n",
            " batch Loss train: 0.06813263148069382\n",
            "i 156\n",
            "epoch 36\n",
            " batch Loss train: 0.08800360560417175\n",
            "i 157\n",
            "epoch 36\n",
            " batch Loss train: 0.07695990055799484\n",
            "i 158\n",
            "epoch 36\n",
            " batch Loss train: 0.061660006642341614\n",
            "i 159\n",
            "epoch 36\n",
            " batch Loss train: 0.07666809856891632\n",
            "i 160\n",
            "epoch 36\n",
            " batch Loss train: 0.06770078837871552\n",
            "i 161\n",
            "epoch 36\n",
            " batch Loss train: 0.07689154148101807\n",
            "i 162\n",
            "epoch 36\n",
            " batch Loss train: 0.05050187557935715\n",
            "i 163\n",
            "epoch 36\n",
            " batch Loss train: 0.07319280505180359\n",
            "i 164\n",
            "epoch 36\n",
            " batch Loss train: 0.09021952748298645\n",
            "i 165\n",
            "epoch 36\n",
            " batch Loss train: 0.09258617460727692\n",
            "i 166\n",
            "epoch 36\n",
            " batch Loss train: 0.06879256665706635\n",
            "i 167\n",
            "epoch 36\n",
            " batch Loss train: 0.0835556909441948\n",
            "i 168\n",
            "epoch 36\n",
            " batch Loss train: 0.07998330891132355\n",
            "i 169\n",
            "epoch 36\n",
            " batch Loss train: 0.07075919955968857\n",
            "i 170\n",
            "epoch 36\n",
            " batch Loss train: 0.06718476861715317\n",
            "i 171\n",
            "epoch 36\n",
            " batch Loss train: 0.07027383148670197\n",
            "i 172\n",
            "epoch 36\n",
            " batch Loss train: 0.0706510990858078\n",
            "i 173\n",
            "epoch 36\n",
            " batch Loss train: 0.06304701417684555\n",
            "i 174\n",
            "epoch 36\n",
            " batch Loss train: 0.07158054411411285\n",
            "i 175\n",
            "epoch 36\n",
            " batch Loss train: 0.07406612485647202\n",
            "i 176\n",
            "epoch 36\n",
            " batch Loss train: 0.0864398255944252\n",
            "i 177\n",
            "epoch 36\n",
            " batch Loss train: 0.06323259323835373\n",
            "i 178\n",
            "epoch 36\n",
            " batch Loss train: 0.05373010039329529\n",
            "i 179\n",
            "epoch 36\n",
            " batch Loss train: 0.09102856367826462\n",
            "i 180\n",
            "epoch 36\n",
            " batch Loss train: 0.07493088394403458\n",
            "i 181\n",
            "epoch 36\n",
            " batch Loss train: 0.06547058373689651\n",
            "i 182\n",
            "epoch 36\n",
            " batch Loss train: 0.07877875119447708\n",
            "i 183\n",
            "epoch 36\n",
            " batch Loss train: 0.05778356269001961\n",
            "i 184\n",
            "epoch 36\n",
            " batch Loss train: 0.09598542749881744\n",
            "i 185\n",
            "epoch 36\n",
            " batch Loss train: 0.09745718538761139\n",
            "i 186\n",
            "epoch 36\n",
            " batch Loss train: 0.08638416975736618\n",
            "i 187\n",
            "epoch 36\n",
            " batch Loss train: 0.06990893185138702\n",
            "i 188\n",
            "epoch 36\n",
            " batch Loss train: 0.09383188933134079\n",
            "i 189\n",
            "epoch 36\n",
            " batch Loss train: 0.0664132758975029\n",
            "i 190\n",
            "epoch 36\n",
            " batch Loss train: 0.0728551596403122\n",
            "i 191\n",
            "epoch 36\n",
            " batch Loss train: 0.08217950910329819\n",
            "i 192\n",
            "epoch 36\n",
            " batch Loss train: 0.09873633831739426\n",
            "i 193\n",
            "epoch 36\n",
            " batch Loss train: 0.0801275297999382\n",
            "i 194\n",
            "epoch 36\n",
            " batch Loss train: 0.0880202129483223\n",
            "i 195\n",
            "epoch 36\n",
            " batch Loss train: 0.08541840314865112\n",
            "i 196\n",
            "epoch 36\n",
            " batch Loss train: 0.07711402326822281\n",
            "i 197\n",
            "epoch 36\n",
            " batch Loss train: 0.06139940768480301\n",
            "i 198\n",
            "epoch 36\n",
            " batch Loss train: 0.07801037281751633\n",
            "i 199\n",
            "epoch 36\n",
            " batch Loss train: 0.07922612875699997\n",
            "i 200\n",
            "epoch 36\n",
            " batch Loss train: 0.07039368152618408\n",
            "i 201\n",
            "epoch 36\n",
            " batch Loss train: 0.07996098697185516\n",
            "i 202\n",
            "epoch 36\n",
            " batch Loss train: 0.0961887389421463\n",
            "i 203\n",
            "epoch 36\n",
            " batch Loss train: 0.07701506465673447\n",
            "i 204\n",
            "epoch 36\n",
            " batch Loss train: 0.09953240305185318\n",
            "i 205\n",
            "epoch 36\n",
            " batch Loss train: 0.06506767123937607\n",
            "i 206\n",
            "epoch 36\n",
            " batch Loss train: 0.06602656841278076\n",
            "i 207\n",
            "epoch 36\n",
            " batch Loss train: 0.08534436672925949\n",
            "i 208\n",
            "epoch 36\n",
            " batch Loss train: 0.05944907292723656\n",
            "i 209\n",
            "epoch 36\n",
            " batch Loss train: 0.07633913308382034\n",
            "i 210\n",
            "epoch 36\n",
            " batch Loss train: 0.07765357196331024\n",
            "i 211\n",
            "epoch 36\n",
            " batch Loss train: 0.0803375393152237\n",
            "i 212\n",
            "epoch 36\n",
            " batch Loss train: 0.08405435085296631\n",
            "i 213\n",
            "epoch 36\n",
            " batch Loss train: 0.10554330796003342\n",
            "i 214\n",
            "epoch 36\n",
            " batch Loss train: 0.08121475577354431\n",
            "i 215\n",
            "epoch 36\n",
            " batch Loss train: 0.10553794354200363\n",
            "i 216\n",
            "epoch 36\n",
            " batch Loss train: 0.07330599427223206\n",
            "i 217\n",
            "epoch 36\n",
            " batch Loss train: 0.08561854064464569\n",
            "i 218\n",
            "epoch 36\n",
            " batch Loss train: 0.07846395671367645\n",
            "i 219\n",
            "epoch 36\n",
            " batch Loss train: 0.07402186095714569\n",
            "i 220\n",
            "epoch 36\n",
            " batch Loss train: 0.06174037232995033\n",
            "i 221\n",
            "epoch 36\n",
            " batch Loss train: 0.08484040945768356\n",
            "i 222\n",
            "epoch 36\n",
            " batch Loss train: 0.08299780637025833\n",
            "i 223\n",
            "epoch 36\n",
            " batch Loss train: 0.07863001525402069\n",
            "i 224\n",
            "epoch 36\n",
            " batch Loss train: 0.052237074822187424\n",
            "i 225\n",
            "epoch 36\n",
            " batch Loss train: 0.06933269649744034\n",
            "i 226\n",
            "epoch 36\n",
            " batch Loss train: 0.07206221669912338\n",
            "i 227\n",
            "epoch 36\n",
            " batch Loss train: 0.05835681036114693\n",
            "i 228\n",
            "epoch 36\n",
            " batch Loss train: 0.07474908977746964\n",
            "i 229\n",
            "epoch 36\n",
            " batch Loss train: 0.06352144479751587\n",
            "i 230\n",
            "epoch 36\n",
            " batch Loss train: 0.07795006036758423\n",
            "i 231\n",
            "epoch 36\n",
            " batch Loss train: 0.07074862718582153\n",
            "i 232\n",
            "epoch 36\n",
            " batch Loss train: 0.09232208877801895\n",
            "i 233\n",
            "epoch 36\n",
            " batch Loss train: 0.0682329535484314\n",
            "i 234\n",
            "epoch 36\n",
            " batch Loss train: 0.08044459670782089\n",
            "i 235\n",
            "epoch 36\n",
            " batch Loss train: 0.09502825886011124\n",
            "i 236\n",
            "epoch 36\n",
            " batch Loss train: 0.09651584923267365\n",
            "i 237\n",
            "epoch 36\n",
            " batch Loss train: 0.07604498416185379\n",
            "i 238\n",
            "epoch 36\n",
            " batch Loss train: 0.07122613489627838\n",
            "i 239\n",
            "epoch 36\n",
            " batch Loss train: 0.06207181513309479\n",
            "i 240\n",
            "epoch 36\n",
            " batch Loss train: 0.08339730650186539\n",
            "i 241\n",
            "epoch 36\n",
            " batch Loss train: 0.06294045597314835\n",
            "i 242\n",
            "epoch 36\n",
            " batch Loss train: 0.0802910253405571\n",
            "i 243\n",
            "epoch 36\n",
            " batch Loss train: 0.08789057284593582\n",
            "i 244\n",
            "epoch 36\n",
            " batch Loss train: 0.07157862931489944\n",
            "i 245\n",
            "epoch 36\n",
            " batch Loss train: 0.06840941309928894\n",
            "i 246\n",
            "epoch 36\n",
            " batch Loss train: 0.07459288835525513\n",
            "i 247\n",
            "epoch 36\n",
            " batch Loss train: 0.07422444969415665\n",
            "i 248\n",
            "epoch 36\n",
            " batch Loss train: 0.0637073963880539\n",
            "i 249\n",
            "epoch 36\n",
            " batch Loss train: 0.1040632426738739\n",
            "i 250\n",
            "epoch 36\n",
            " batch Loss train: 0.06718699634075165\n",
            "i 251\n",
            "epoch 36\n",
            " batch Loss train: 0.06703725457191467\n",
            "i 252\n",
            "epoch 36\n",
            " batch Loss train: 0.07755538076162338\n",
            "i 253\n",
            "epoch 36\n",
            " batch Loss train: 0.09499706327915192\n",
            "i 254\n",
            "epoch 36\n",
            " batch Loss train: 0.07567652314901352\n",
            "i 255\n",
            "epoch 36\n",
            " batch Loss train: 0.06853596121072769\n",
            "i 256\n",
            "epoch 36\n",
            " batch Loss train: 0.06514677405357361\n",
            "i 257\n",
            "epoch 36\n",
            " batch Loss train: 0.07706180214881897\n",
            "i 258\n",
            "epoch 36\n",
            " batch Loss train: 0.09501130878925323\n",
            "i 259\n",
            "epoch 36\n",
            " batch Loss train: 0.07680478692054749\n",
            "i 260\n",
            "epoch 36\n",
            " batch Loss train: 0.06199607253074646\n",
            "i 261\n",
            "epoch 36\n",
            " batch Loss train: 0.07084928452968597\n",
            "i 262\n",
            "epoch 36\n",
            " batch Loss train: 0.07824340462684631\n",
            "i 263\n",
            "epoch 36\n",
            " batch Loss train: 0.07380048930644989\n",
            "i 264\n",
            "epoch 36\n",
            " batch Loss train: 0.10418354719877243\n",
            "i 265\n",
            "epoch 36\n",
            " batch Loss train: 0.06635230779647827\n",
            "i 266\n",
            "epoch 36\n",
            " batch Loss train: 0.06409560889005661\n",
            "i 267\n",
            "epoch 36\n",
            " batch Loss train: 0.09159921854734421\n",
            "i 268\n",
            "epoch 36\n",
            " batch Loss train: 0.07804089784622192\n",
            "i 269\n",
            "epoch 36\n",
            " batch Loss train: 0.06711068749427795\n",
            "i 270\n",
            "epoch 36\n",
            " batch Loss train: 0.08464282751083374\n",
            "i 271\n",
            "epoch 36\n",
            " batch Loss train: 0.08657389879226685\n",
            "i 272\n",
            "epoch 36\n",
            " batch Loss train: 0.0618215911090374\n",
            "i 273\n",
            "epoch 36\n",
            " batch Loss train: 0.06214980036020279\n",
            "i 274\n",
            "epoch 36\n",
            " batch Loss train: 0.06447963416576385\n",
            "i 275\n",
            "epoch 36\n",
            " batch Loss train: 0.09292000532150269\n",
            "i 276\n",
            "epoch 36\n",
            " batch Loss train: 0.1219770684838295\n",
            "i 277\n",
            "epoch 36\n",
            " batch Loss train: 0.08344141393899918\n",
            "i 278\n",
            "epoch 36\n",
            " batch Loss train: 0.09028053283691406\n",
            "i 279\n",
            "epoch 36\n",
            " batch Loss train: 0.0659957081079483\n",
            "i 280\n",
            "epoch 36\n",
            " batch Loss train: 0.06329505145549774\n",
            "i 281\n",
            "epoch 36\n",
            " batch Loss train: 0.1051921546459198\n",
            "i 282\n",
            "epoch 36\n",
            " batch Loss train: 0.07939386367797852\n",
            "i 283\n",
            "epoch 36\n",
            " batch Loss train: 0.08177848905324936\n",
            "i 284\n",
            "epoch 36\n",
            " batch Loss train: 0.0951409712433815\n",
            "i 285\n",
            "epoch 36\n",
            " batch Loss train: 0.09095809608697891\n",
            "i 286\n",
            "epoch 36\n",
            " batch Loss train: 0.09031502157449722\n",
            "i 287\n",
            "epoch 36\n",
            " batch Loss train: 0.07187279313802719\n",
            "i 288\n",
            "epoch 36\n",
            " batch Loss train: 0.07418623566627502\n",
            "i 289\n",
            "epoch 36\n",
            " batch Loss train: 0.07655797898769379\n",
            "i 290\n",
            "epoch 36\n",
            " batch Loss train: 0.08951204270124435\n",
            "i 291\n",
            "epoch 36\n",
            " batch Loss train: 0.08410411328077316\n",
            "i 292\n",
            "epoch 36\n",
            " batch Loss train: 0.06581804156303406\n",
            "i 293\n",
            "epoch 36\n",
            " batch Loss train: 0.08660634607076645\n",
            "i 294\n",
            "epoch 36\n",
            " batch Loss train: 0.08960342407226562\n",
            "i 295\n",
            "epoch 36\n",
            " batch Loss train: 0.07268350571393967\n",
            "i 296\n",
            "epoch 36\n",
            " batch Loss train: 0.07050837576389313\n",
            "i 297\n",
            "epoch 36\n",
            " batch Loss train: 0.06395948678255081\n",
            "i 298\n",
            "epoch 36\n",
            " batch Loss train: 0.07700170576572418\n",
            "i 299\n",
            "epoch 36\n",
            " batch Loss train: 0.08694056421518326\n",
            "i 300\n",
            "epoch 36\n",
            " batch Loss train: 0.06517850607633591\n",
            "i 301\n",
            "epoch 36\n",
            " batch Loss train: 0.08475092798471451\n",
            "i 302\n",
            "epoch 36\n",
            " batch Loss train: 0.08846922218799591\n",
            "i 303\n",
            "epoch 36\n",
            " batch Loss train: 0.07065537571907043\n",
            "i 304\n",
            "epoch 36\n",
            " batch Loss train: 0.06368858367204666\n",
            "i 305\n",
            "epoch 36\n",
            " batch Loss train: 0.06406127661466599\n",
            "i 306\n",
            "epoch 36\n",
            " batch Loss train: 0.0719962790608406\n",
            "i 307\n",
            "epoch 36\n",
            " batch Loss train: 0.060853153467178345\n",
            "i 308\n",
            "epoch 36\n",
            " batch Loss train: 0.06506864726543427\n",
            "i 309\n",
            "epoch 36\n",
            " batch Loss train: 0.082900770008564\n",
            "i 310\n",
            "epoch 36\n",
            " batch Loss train: 0.07541448622941971\n",
            "i 311\n",
            "epoch 36\n",
            " batch Loss train: 0.0664675310254097\n",
            "i 312\n",
            "epoch 36\n",
            " batch Loss train: 0.06691575795412064\n",
            "i 313\n",
            "epoch 36\n",
            " batch Loss train: 0.11780494451522827\n",
            "i 314\n",
            "epoch 36\n",
            " batch Loss train: 0.07624133676290512\n",
            "i 315\n",
            "epoch 36\n",
            " batch Loss train: 0.08555595576763153\n",
            "i 316\n",
            "epoch 36\n",
            " batch Loss train: 0.07622773200273514\n",
            "i 317\n",
            "epoch 36\n",
            " batch Loss train: 0.08522703498601913\n",
            "i 318\n",
            "epoch 36\n",
            " batch Loss train: 0.07105226814746857\n",
            "i 319\n",
            "epoch 36\n",
            " batch Loss train: 0.06637205928564072\n",
            "i 320\n",
            "epoch 36\n",
            " batch Loss train: 0.06749314069747925\n",
            "i 321\n",
            "epoch 36\n",
            " batch Loss train: 0.07916082441806793\n",
            "i 322\n",
            "epoch 36\n",
            " batch Loss train: 0.054361481219530106\n",
            "i 323\n",
            "epoch 36\n",
            " batch Loss train: 0.07914085686206818\n",
            "i 324\n",
            "epoch 36\n",
            " batch Loss train: 0.0888725221157074\n",
            "i 325\n",
            "epoch 36\n",
            " batch Loss train: 0.09476541727781296\n",
            "i 326\n",
            "epoch 36\n",
            " batch Loss train: 0.07277536392211914\n",
            "i 327\n",
            "epoch 36\n",
            " batch Loss train: 0.07128383964300156\n",
            "i 328\n",
            "epoch 36\n",
            " batch Loss train: 0.06510653346776962\n",
            "i 329\n",
            "epoch 36\n",
            " batch Loss train: 0.06175384670495987\n",
            "i 330\n",
            "epoch 36\n",
            " batch Loss train: 0.07790457457304001\n",
            "i 331\n",
            "epoch 36\n",
            " batch Loss train: 0.071677565574646\n",
            "i 332\n",
            "epoch 36\n",
            " batch Loss train: 0.07719726115465164\n",
            "i 333\n",
            "epoch 36\n",
            " batch Loss train: 0.07370654493570328\n",
            "i 334\n",
            "epoch 36\n",
            " batch Loss train: 0.07434351742267609\n",
            "i 335\n",
            "epoch 36\n",
            " batch Loss train: 0.06562342494726181\n",
            "i 336\n",
            "epoch 36\n",
            " batch Loss train: 0.06053166836500168\n",
            "i 337\n",
            "epoch 36\n",
            " batch Loss train: 0.05608825758099556\n",
            "i 338\n",
            "epoch 36\n",
            " batch Loss train: 0.06850095093250275\n",
            "i 339\n",
            "epoch 36\n",
            " batch Loss train: 0.06470031291246414\n",
            "i 340\n",
            "epoch 36\n",
            " batch Loss train: 0.07134026288986206\n",
            "i 341\n",
            "epoch 36\n",
            " batch Loss train: 0.11219170689582825\n",
            "i 342\n",
            "epoch 36\n",
            " batch Loss train: 0.09377110749483109\n",
            "i 343\n",
            "epoch 36\n",
            " batch Loss train: 0.10657906532287598\n",
            "i 344\n",
            "epoch 36\n",
            " batch Loss train: 0.08044315874576569\n",
            "i 345\n",
            "epoch 36\n",
            " batch Loss train: 0.08710553497076035\n",
            "i 346\n",
            "epoch 36\n",
            " batch Loss train: 0.07005365937948227\n",
            "i 347\n",
            "epoch 36\n",
            " batch Loss train: 0.06400375068187714\n",
            "i 348\n",
            "epoch 36\n",
            " batch Loss train: 0.0656665712594986\n",
            "i 349\n",
            "epoch 36\n",
            " batch Loss train: 0.057578541338443756\n",
            "i 350\n",
            "epoch 36\n",
            " batch Loss train: 0.07853378355503082\n",
            "i 351\n",
            "epoch 36\n",
            " batch Loss train: 0.07538046687841415\n",
            "i 352\n",
            "epoch 36\n",
            " batch Loss train: 0.09002139419317245\n",
            "i 353\n",
            "epoch 36\n",
            " batch Loss train: 0.08345967531204224\n",
            "i 354\n",
            "epoch 36\n",
            " batch Loss train: 0.09066535532474518\n",
            "i 355\n",
            "epoch 36\n",
            " batch Loss train: 0.07983256876468658\n",
            "i 356\n",
            "epoch 36\n",
            " batch Loss train: 0.06461672484874725\n",
            "i 357\n",
            "epoch 36\n",
            " batch Loss train: 0.09460575133562088\n",
            "i 358\n",
            "epoch 36\n",
            " batch Loss train: 0.08064153045415878\n",
            "i 359\n",
            "epoch 36\n",
            " batch Loss train: 0.0839577466249466\n",
            "i 360\n",
            "epoch 36\n",
            " batch Loss train: 0.08028129488229752\n",
            "i 361\n",
            "epoch 36\n",
            " batch Loss train: 0.08460932224988937\n",
            "i 362\n",
            "epoch 36\n",
            " batch Loss train: 0.09393740445375443\n",
            "i 363\n",
            "epoch 36\n",
            " batch Loss train: 0.07622143626213074\n",
            "i 364\n",
            "epoch 36\n",
            " batch Loss train: 0.08381017297506332\n",
            "i 365\n",
            "epoch 36\n",
            " batch Loss train: 0.07900319248437881\n",
            "i 366\n",
            "epoch 36\n",
            " batch Loss train: 0.08553194999694824\n",
            "i 367\n",
            "epoch 36\n",
            " batch Loss train: 0.06361768394708633\n",
            "i 368\n",
            "epoch 36\n",
            " batch Loss train: 0.0889890119433403\n",
            "i 369\n",
            "epoch 36\n",
            " batch Loss train: 0.07173270732164383\n",
            "i 370\n",
            "epoch 36\n",
            " batch Loss train: 0.07252882421016693\n",
            "i 371\n",
            "epoch 36\n",
            " batch Loss train: 0.05904402211308479\n",
            "i 372\n",
            "epoch 36\n",
            " batch Loss train: 0.07714565843343735\n",
            "i 373\n",
            "epoch 36\n",
            " batch Loss train: 0.08900397270917892\n",
            "i 374\n",
            "epoch 36\n",
            " batch Loss train: 0.08097165077924728\n",
            "i 375\n",
            "epoch 36\n",
            " batch Loss train: 0.07532792538404465\n",
            "i 376\n",
            "epoch 36\n",
            " batch Loss train: 0.06230946257710457\n",
            "i 377\n",
            "epoch 36\n",
            " batch Loss train: 0.06402301788330078\n",
            "i 378\n",
            "epoch 36\n",
            " batch Loss train: 0.0669311136007309\n",
            "i 379\n",
            "epoch 36\n",
            " batch Loss train: 0.06995224207639694\n",
            "i 380\n",
            "epoch 36\n",
            " batch Loss train: 0.082095205783844\n",
            "i 381\n",
            "epoch 36\n",
            " batch Loss train: 0.10670308023691177\n",
            "i 382\n",
            "epoch 36\n",
            " batch Loss train: 0.07736197859048843\n",
            "i 383\n",
            "epoch 36\n",
            " batch Loss train: 0.07623300701379776\n",
            "i 384\n",
            "epoch 36\n",
            " batch Loss train: 0.07858692109584808\n",
            "i 385\n",
            "epoch 36\n",
            " batch Loss train: 0.09849938005208969\n",
            "i 386\n",
            "epoch 36\n",
            " batch Loss train: 0.0986071452498436\n",
            "i 387\n",
            "epoch 36\n",
            " batch Loss train: 0.07434436678886414\n",
            "i 388\n",
            "epoch 36\n",
            " batch Loss train: 0.07600105553865433\n",
            "i 389\n",
            "epoch 36\n",
            " batch Loss train: 0.09552190452814102\n",
            "i 390\n",
            "epoch 36\n",
            " batch Loss train: 0.054882295429706573\n",
            "i 391\n",
            "epoch 36\n",
            " batch Loss train: 0.06407395005226135\n",
            "i 392\n",
            "epoch 36\n",
            " batch Loss train: 0.10433723032474518\n",
            "i 393\n",
            "epoch 36\n",
            " batch Loss train: 0.0734691470861435\n",
            "i 394\n",
            "epoch 36\n",
            " batch Loss train: 0.09063038975000381\n",
            "i 395\n",
            "epoch 36\n",
            " batch Loss train: 0.07162147015333176\n",
            "i 396\n",
            "epoch 36\n",
            " batch Loss train: 0.09816139936447144\n",
            "i 397\n",
            "epoch 36\n",
            " batch Loss train: 0.061539940536022186\n",
            "i 398\n",
            "epoch 36\n",
            " batch Loss train: 0.05958142504096031\n",
            "i 399\n",
            "epoch 36\n",
            " batch Loss train: 0.060756538063287735\n",
            "i 400\n",
            "epoch 36\n",
            " batch Loss train: 0.07260512560606003\n",
            "i 401\n",
            "epoch 36\n",
            " batch Loss train: 0.09661167114973068\n",
            "i 402\n",
            "epoch 36\n",
            " batch Loss train: 0.08384612947702408\n",
            "i 403\n",
            "epoch 36\n",
            " batch Loss train: 0.0859159380197525\n",
            "i 404\n",
            "epoch 36\n",
            " batch Loss train: 0.09719565510749817\n",
            "i 405\n",
            "epoch 36\n",
            " batch Loss train: 0.07498572766780853\n",
            "i 406\n",
            "epoch 36\n",
            " batch Loss train: 0.08646758645772934\n",
            "i 407\n",
            "epoch 36\n",
            " batch Loss train: 0.06019885092973709\n",
            "i 408\n",
            "epoch 36\n",
            " batch Loss train: 0.09839469939470291\n",
            "i 409\n",
            "epoch 36\n",
            " batch Loss train: 0.058572109788656235\n",
            "i 410\n",
            "epoch 36\n",
            " batch Loss train: 0.06398078799247742\n",
            "i 411\n",
            "epoch 36\n",
            " batch Loss train: 0.07733966410160065\n",
            "i 412\n",
            "epoch 36\n",
            " batch Loss train: 0.0794232040643692\n",
            "i 413\n",
            "epoch 36\n",
            " batch Loss train: 0.07419298589229584\n",
            "i 414\n",
            "epoch 36\n",
            " batch Loss train: 0.08093701303005219\n",
            "i 415\n",
            "epoch 36\n",
            " batch Loss train: 0.06380722671747208\n",
            "i 416\n",
            "epoch 36\n",
            " batch Loss train: 0.05745474994182587\n",
            "i 417\n",
            "epoch 36\n",
            " batch Loss train: 0.09659000486135483\n",
            "i 418\n",
            "epoch 36\n",
            " batch Loss train: 0.08951625227928162\n",
            "i 419\n",
            "epoch 36\n",
            " batch Loss train: 0.09694154560565948\n",
            "i 420\n",
            "epoch 36\n",
            " batch Loss train: 0.08756522834300995\n",
            "i 421\n",
            "epoch 36\n",
            " batch Loss train: 0.07115460187196732\n",
            "i 422\n",
            "epoch 36\n",
            " batch Loss train: 0.10639530420303345\n",
            "i 423\n",
            "epoch 36\n",
            " batch Loss train: 0.09264865517616272\n",
            "i 424\n",
            "epoch 36\n",
            " batch Loss train: 0.08094367384910583\n",
            "i 425\n",
            "epoch 36\n",
            " batch Loss train: 0.06920737028121948\n",
            "i 426\n",
            "epoch 36\n",
            " batch Loss train: 0.0891549214720726\n",
            "i 427\n",
            "epoch 36\n",
            " batch Loss train: 0.0807158574461937\n",
            "i 428\n",
            "epoch 36\n",
            " batch Loss train: 0.08063560724258423\n",
            "i 429\n",
            "epoch 36\n",
            " batch Loss train: 0.10315851867198944\n",
            "i 430\n",
            "epoch 36\n",
            " batch Loss train: 0.06710196286439896\n",
            "i 431\n",
            "epoch 36\n",
            " batch Loss train: 0.076091468334198\n",
            "i 432\n",
            "epoch 36\n",
            " batch Loss train: 0.06343335658311844\n",
            "i 433\n",
            "epoch 36\n",
            " batch Loss train: 0.09137913584709167\n",
            "i 434\n",
            "epoch 36\n",
            " batch Loss train: 0.08575326949357986\n",
            "i 435\n",
            "epoch 36\n",
            " batch Loss train: 0.11420869827270508\n",
            "i 436\n",
            "epoch 36\n",
            " batch Loss train: 0.07156463712453842\n",
            "i 437\n",
            "epoch 36\n",
            " batch Loss train: 0.06785235553979874\n",
            "i 438\n",
            "epoch 36\n",
            " batch Loss train: 0.08714951574802399\n",
            "i 439\n",
            "epoch 36\n",
            " batch Loss train: 0.07002267241477966\n",
            "i 440\n",
            "epoch 36\n",
            " batch Loss train: 0.0868413969874382\n",
            "i 441\n",
            "epoch 36\n",
            " batch Loss train: 0.08228355646133423\n",
            "i 442\n",
            "epoch 36\n",
            " batch Loss train: 0.056722741574048996\n",
            "i 443\n",
            "epoch 36\n",
            " batch Loss train: 0.06835711002349854\n",
            "i 444\n",
            "epoch 36\n",
            " batch Loss train: 0.0683545172214508\n",
            "i 445\n",
            "epoch 36\n",
            " batch Loss train: 0.07479824870824814\n",
            "total epoch Loss train: tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 37\n",
            " batch Loss train: 0.048343200236558914\n",
            "i 1\n",
            "epoch 37\n",
            " batch Loss train: 0.04999134689569473\n",
            "i 2\n",
            "epoch 37\n",
            " batch Loss train: 0.05995285138487816\n",
            "i 3\n",
            "epoch 37\n",
            " batch Loss train: 0.05884856358170509\n",
            "i 4\n",
            "epoch 37\n",
            " batch Loss train: 0.05507561191916466\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 37\n",
            " batch Loss train: 0.06440165638923645\n",
            "i 6\n",
            "epoch 37\n",
            " batch Loss train: 0.06209155172109604\n",
            "i 7\n",
            "epoch 37\n",
            " batch Loss train: 0.07744200527667999\n",
            "i 8\n",
            "epoch 37\n",
            " batch Loss train: 0.09203527867794037\n",
            "i 9\n",
            "epoch 37\n",
            " batch Loss train: 0.06791818141937256\n",
            "i 10\n",
            "epoch 37\n",
            " batch Loss train: 0.07255711406469345\n",
            "i 11\n",
            "epoch 37\n",
            " batch Loss train: 0.06987844407558441\n",
            "i 12\n",
            "epoch 37\n",
            " batch Loss train: 0.07380315661430359\n",
            "i 13\n",
            "epoch 37\n",
            " batch Loss train: 0.05502351373434067\n",
            "i 14\n",
            "epoch 37\n",
            " batch Loss train: 0.06104952096939087\n",
            "i 15\n",
            "epoch 37\n",
            " batch Loss train: 0.06850754469633102\n",
            "i 16\n",
            "epoch 37\n",
            " batch Loss train: 0.05498264729976654\n",
            "i 17\n",
            "epoch 37\n",
            " batch Loss train: 0.06278635561466217\n",
            "i 18\n",
            "epoch 37\n",
            " batch Loss train: 0.0503082200884819\n",
            "i 19\n",
            "epoch 37\n",
            " batch Loss train: 0.08348836749792099\n",
            "i 20\n",
            "epoch 37\n",
            " batch Loss train: 0.06137657165527344\n",
            "i 21\n",
            "epoch 37\n",
            " batch Loss train: 0.05718701332807541\n",
            "i 22\n",
            "epoch 37\n",
            " batch Loss train: 0.0767158791422844\n",
            "i 23\n",
            "epoch 37\n",
            " batch Loss train: 0.06134815514087677\n",
            "i 24\n",
            "epoch 37\n",
            " batch Loss train: 0.08758474886417389\n",
            "i 25\n",
            "epoch 37\n",
            " batch Loss train: 0.0830332562327385\n",
            "i 26\n",
            "epoch 37\n",
            " batch Loss train: 0.07201413810253143\n",
            "i 27\n",
            "epoch 37\n",
            " batch Loss train: 0.08619926124811172\n",
            "i 28\n",
            "epoch 37\n",
            " batch Loss train: 0.06493257731199265\n",
            "i 29\n",
            "epoch 37\n",
            " batch Loss train: 0.05211688578128815\n",
            "i 30\n",
            "epoch 37\n",
            " batch Loss train: 0.06218060106039047\n",
            "i 31\n",
            "epoch 37\n",
            " batch Loss train: 0.0766962319612503\n",
            "i 32\n",
            "epoch 37\n",
            " batch Loss train: 0.08366920799016953\n",
            "i 33\n",
            "epoch 37\n",
            " batch Loss train: 0.06315205246210098\n",
            "i 34\n",
            "epoch 37\n",
            " batch Loss train: 0.05893045663833618\n",
            "i 35\n",
            "epoch 37\n",
            " batch Loss train: 0.05004998296499252\n",
            "i 36\n",
            "epoch 37\n",
            " batch Loss train: 0.06648459285497665\n",
            "i 37\n",
            "epoch 37\n",
            " batch Loss train: 0.06205896660685539\n",
            "i 38\n",
            "epoch 37\n",
            " batch Loss train: 0.06012877821922302\n",
            "i 39\n",
            "epoch 37\n",
            " batch Loss train: 0.062379125505685806\n",
            "i 40\n",
            "epoch 37\n",
            " batch Loss train: 0.0644802525639534\n",
            "i 41\n",
            "epoch 37\n",
            " batch Loss train: 0.059964753687381744\n",
            "i 42\n",
            "epoch 37\n",
            " batch Loss train: 0.0538998506963253\n",
            "i 43\n",
            "epoch 37\n",
            " batch Loss train: 0.0533117949962616\n",
            "i 44\n",
            "epoch 37\n",
            " batch Loss train: 0.04611479490995407\n",
            "i 45\n",
            "epoch 37\n",
            " batch Loss train: 0.0749746635556221\n",
            "i 46\n",
            "epoch 37\n",
            " batch Loss train: 0.08272141218185425\n",
            "i 47\n",
            "epoch 37\n",
            " batch Loss train: 0.08650846779346466\n",
            "i 48\n",
            "epoch 37\n",
            " batch Loss train: 0.06723123043775558\n",
            "i 49\n",
            "epoch 37\n",
            " batch Loss train: 0.06700385361909866\n",
            "i 50\n",
            "epoch 37\n",
            " batch Loss train: 0.051020339131355286\n",
            "i 51\n",
            "epoch 37\n",
            " batch Loss train: 0.05135028064250946\n",
            "i 52\n",
            "epoch 37\n",
            " batch Loss train: 0.07124915719032288\n",
            "i 53\n",
            "epoch 37\n",
            " batch Loss train: 0.05775079131126404\n",
            "i 54\n",
            "epoch 37\n",
            " batch Loss train: 0.0902705192565918\n",
            "i 55\n",
            "epoch 37\n",
            " batch Loss train: 0.071937195956707\n",
            "i 56\n",
            "epoch 37\n",
            " batch Loss train: 0.06350861489772797\n",
            "i 57\n",
            "epoch 37\n",
            " batch Loss train: 0.07109203934669495\n",
            "i 58\n",
            "epoch 37\n",
            " batch Loss train: 0.05595895275473595\n",
            "i 59\n",
            "epoch 37\n",
            " batch Loss train: 0.04965497553348541\n",
            "i 60\n",
            "epoch 37\n",
            " batch Loss train: 0.054005593061447144\n",
            "i 61\n",
            "epoch 37\n",
            " batch Loss train: 0.07068812847137451\n",
            "i 62\n",
            "epoch 37\n",
            " batch Loss train: 0.06309723109006882\n",
            "i 63\n",
            "epoch 37\n",
            " batch Loss train: 0.057115066796541214\n",
            "i 64\n",
            "epoch 37\n",
            " batch Loss train: 0.06557375937700272\n",
            "i 65\n",
            "epoch 37\n",
            " batch Loss train: 0.06362840533256531\n",
            "i 66\n",
            "epoch 37\n",
            " batch Loss train: 0.07147202640771866\n",
            "i 67\n",
            "epoch 37\n",
            " batch Loss train: 0.09492602199316025\n",
            "i 68\n",
            "epoch 37\n",
            " batch Loss train: 0.07792066782712936\n",
            "i 69\n",
            "epoch 37\n",
            " batch Loss train: 0.06294398009777069\n",
            "i 70\n",
            "epoch 37\n",
            " batch Loss train: 0.06602584570646286\n",
            "i 71\n",
            "epoch 37\n",
            " batch Loss train: 0.0696503147482872\n",
            "i 72\n",
            "epoch 37\n",
            " batch Loss train: 0.05360504984855652\n",
            "i 73\n",
            "epoch 37\n",
            " batch Loss train: 0.07282012701034546\n",
            "i 74\n",
            "epoch 37\n",
            " batch Loss train: 0.06156463921070099\n",
            "i 75\n",
            "epoch 37\n",
            " batch Loss train: 0.07274683564901352\n",
            "i 76\n",
            "epoch 37\n",
            " batch Loss train: 0.06722055375576019\n",
            "i 77\n",
            "epoch 37\n",
            " batch Loss train: 0.06646700948476791\n",
            "i 78\n",
            "epoch 37\n",
            " batch Loss train: 0.08724217861890793\n",
            "i 79\n",
            "epoch 37\n",
            " batch Loss train: 0.06818793714046478\n",
            "i 80\n",
            "epoch 37\n",
            " batch Loss train: 0.059905391186475754\n",
            "i 81\n",
            "epoch 37\n",
            " batch Loss train: 0.0702386423945427\n",
            "i 82\n",
            "epoch 37\n",
            " batch Loss train: 0.06580910086631775\n",
            "i 83\n",
            "epoch 37\n",
            " batch Loss train: 0.07254226505756378\n",
            "i 84\n",
            "epoch 37\n",
            " batch Loss train: 0.07482264935970306\n",
            "i 85\n",
            "epoch 37\n",
            " batch Loss train: 0.06672956049442291\n",
            "i 86\n",
            "epoch 37\n",
            " batch Loss train: 0.05046072229743004\n",
            "i 87\n",
            "epoch 37\n",
            " batch Loss train: 0.07089966535568237\n",
            "i 88\n",
            "epoch 37\n",
            " batch Loss train: 0.05726997181773186\n",
            "i 89\n",
            "epoch 37\n",
            " batch Loss train: 0.06678159534931183\n",
            "i 90\n",
            "epoch 37\n",
            " batch Loss train: 0.07088816165924072\n",
            "i 91\n",
            "epoch 37\n",
            " batch Loss train: 0.08827922493219376\n",
            "i 92\n",
            "epoch 37\n",
            " batch Loss train: 0.07550975680351257\n",
            "i 93\n",
            "epoch 37\n",
            " batch Loss train: 0.09635758399963379\n",
            "i 94\n",
            "epoch 37\n",
            " batch Loss train: 0.054195620119571686\n",
            "i 95\n",
            "epoch 37\n",
            " batch Loss train: 0.061183761805295944\n",
            "i 96\n",
            "epoch 37\n",
            " batch Loss train: 0.06550028175115585\n",
            "i 97\n",
            "epoch 37\n",
            " batch Loss train: 0.06113998964428902\n",
            "i 98\n",
            "epoch 37\n",
            " batch Loss train: 0.06550053507089615\n",
            "i 99\n",
            "epoch 37\n",
            " batch Loss train: 0.0718679130077362\n",
            "i 100\n",
            "epoch 37\n",
            " batch Loss train: 0.06746087223291397\n",
            "i 101\n",
            "epoch 37\n",
            " batch Loss train: 0.08953513205051422\n",
            "i 102\n",
            "epoch 37\n",
            " batch Loss train: 0.05753059685230255\n",
            "i 103\n",
            "epoch 37\n",
            " batch Loss train: 0.06771872937679291\n",
            "i 104\n",
            "epoch 37\n",
            " batch Loss train: 0.06937818229198456\n",
            "i 105\n",
            "epoch 37\n",
            " batch Loss train: 0.06547750532627106\n",
            "i 106\n",
            "epoch 37\n",
            " batch Loss train: 0.11078547686338425\n",
            "i 107\n",
            "epoch 37\n",
            " batch Loss train: 0.05605638772249222\n",
            "i 108\n",
            "epoch 37\n",
            " batch Loss train: 0.07127230614423752\n",
            "i 109\n",
            "epoch 37\n",
            " batch Loss train: 0.04528064280748367\n",
            "i 110\n",
            "epoch 37\n",
            " batch Loss train: 0.07385925203561783\n",
            "i 111\n",
            "epoch 37\n",
            " batch Loss train: 0.0696796402335167\n",
            "i 112\n",
            "epoch 37\n",
            " batch Loss train: 0.06501387059688568\n",
            "i 113\n",
            "epoch 37\n",
            " batch Loss train: 0.07083545625209808\n",
            "i 114\n",
            "epoch 37\n",
            " batch Loss train: 0.0667179599404335\n",
            "i 115\n",
            "epoch 37\n",
            " batch Loss train: 0.06685727834701538\n",
            "i 116\n",
            "epoch 37\n",
            " batch Loss train: 0.04985901713371277\n",
            "i 117\n",
            "epoch 37\n",
            " batch Loss train: 0.0632878839969635\n",
            "i 118\n",
            "epoch 37\n",
            " batch Loss train: 0.086784727871418\n",
            "i 119\n",
            "epoch 37\n",
            " batch Loss train: 0.07197857648134232\n",
            "i 120\n",
            "epoch 37\n",
            " batch Loss train: 0.06673340499401093\n",
            "i 121\n",
            "epoch 37\n",
            " batch Loss train: 0.08664198219776154\n",
            "i 122\n",
            "epoch 37\n",
            " batch Loss train: 0.08158484846353531\n",
            "i 123\n",
            "epoch 37\n",
            " batch Loss train: 0.06344319134950638\n",
            "i 124\n",
            "epoch 37\n",
            " batch Loss train: 0.07349169999361038\n",
            "i 125\n",
            "epoch 37\n",
            " batch Loss train: 0.055822283029556274\n",
            "i 126\n",
            "epoch 37\n",
            " batch Loss train: 0.09947986155748367\n",
            "i 127\n",
            "epoch 37\n",
            " batch Loss train: 0.06518280506134033\n",
            "i 128\n",
            "epoch 37\n",
            " batch Loss train: 0.056782472878694534\n",
            "i 129\n",
            "epoch 37\n",
            " batch Loss train: 0.06604663282632828\n",
            "i 130\n",
            "epoch 37\n",
            " batch Loss train: 0.07331390678882599\n",
            "i 131\n",
            "epoch 37\n",
            " batch Loss train: 0.07744669169187546\n",
            "i 132\n",
            "epoch 37\n",
            " batch Loss train: 0.07324112951755524\n",
            "i 133\n",
            "epoch 37\n",
            " batch Loss train: 0.06980308890342712\n",
            "i 134\n",
            "epoch 37\n",
            " batch Loss train: 0.0679720938205719\n",
            "i 135\n",
            "epoch 37\n",
            " batch Loss train: 0.04901149868965149\n",
            "i 136\n",
            "epoch 37\n",
            " batch Loss train: 0.07890041917562485\n",
            "i 137\n",
            "epoch 37\n",
            " batch Loss train: 0.03853294253349304\n",
            "i 138\n",
            "epoch 37\n",
            " batch Loss train: 0.05886398255825043\n",
            "i 139\n",
            "epoch 37\n",
            " batch Loss train: 0.07932727783918381\n",
            "i 140\n",
            "epoch 37\n",
            " batch Loss train: 0.08579929918050766\n",
            "i 141\n",
            "epoch 37\n",
            " batch Loss train: 0.06524550914764404\n",
            "i 142\n",
            "epoch 37\n",
            " batch Loss train: 0.08903104066848755\n",
            "i 143\n",
            "epoch 37\n",
            " batch Loss train: 0.0627109706401825\n",
            "i 144\n",
            "epoch 37\n",
            " batch Loss train: 0.07452964037656784\n",
            "i 145\n",
            "epoch 37\n",
            " batch Loss train: 0.06177761033177376\n",
            "i 146\n",
            "epoch 37\n",
            " batch Loss train: 0.0515962578356266\n",
            "i 147\n",
            "epoch 37\n",
            " batch Loss train: 0.059443868696689606\n",
            "i 148\n",
            "epoch 37\n",
            " batch Loss train: 0.046755172312259674\n",
            "i 149\n",
            "epoch 37\n",
            " batch Loss train: 0.06092159450054169\n",
            "i 150\n",
            "epoch 37\n",
            " batch Loss train: 0.06871728599071503\n",
            "i 151\n",
            "epoch 37\n",
            " batch Loss train: 0.07317116856575012\n",
            "i 152\n",
            "epoch 37\n",
            " batch Loss train: 0.05170079693198204\n",
            "i 153\n",
            "epoch 37\n",
            " batch Loss train: 0.09178390353918076\n",
            "i 154\n",
            "epoch 37\n",
            " batch Loss train: 0.07658092677593231\n",
            "i 155\n",
            "epoch 37\n",
            " batch Loss train: 0.05841096118092537\n",
            "i 156\n",
            "epoch 37\n",
            " batch Loss train: 0.05775702744722366\n",
            "i 157\n",
            "epoch 37\n",
            " batch Loss train: 0.0822642371058464\n",
            "i 158\n",
            "epoch 37\n",
            " batch Loss train: 0.09222734719514847\n",
            "i 159\n",
            "epoch 37\n",
            " batch Loss train: 0.060713231563568115\n",
            "i 160\n",
            "epoch 37\n",
            " batch Loss train: 0.062185969203710556\n",
            "i 161\n",
            "epoch 37\n",
            " batch Loss train: 0.05724214389920235\n",
            "i 162\n",
            "epoch 37\n",
            " batch Loss train: 0.0886089876294136\n",
            "i 163\n",
            "epoch 37\n",
            " batch Loss train: 0.058767661452293396\n",
            "i 164\n",
            "epoch 37\n",
            " batch Loss train: 0.07242465019226074\n",
            "i 165\n",
            "epoch 37\n",
            " batch Loss train: 0.0610593743622303\n",
            "i 166\n",
            "epoch 37\n",
            " batch Loss train: 0.05399308353662491\n",
            "i 167\n",
            "epoch 37\n",
            " batch Loss train: 0.07469205558300018\n",
            "i 168\n",
            "epoch 37\n",
            " batch Loss train: 0.06925389170646667\n",
            "i 169\n",
            "epoch 37\n",
            " batch Loss train: 0.06009912118315697\n",
            "i 170\n",
            "epoch 37\n",
            " batch Loss train: 0.06370003521442413\n",
            "i 171\n",
            "epoch 37\n",
            " batch Loss train: 0.08101365715265274\n",
            "i 172\n",
            "epoch 37\n",
            " batch Loss train: 0.04990283399820328\n",
            "i 173\n",
            "epoch 37\n",
            " batch Loss train: 0.06147531419992447\n",
            "i 174\n",
            "epoch 37\n",
            " batch Loss train: 0.06187627092003822\n",
            "i 175\n",
            "epoch 37\n",
            " batch Loss train: 0.059407226741313934\n",
            "i 176\n",
            "epoch 37\n",
            " batch Loss train: 0.06560590863227844\n",
            "i 177\n",
            "epoch 37\n",
            " batch Loss train: 0.05252911150455475\n",
            "i 178\n",
            "epoch 37\n",
            " batch Loss train: 0.06483212858438492\n",
            "i 179\n",
            "epoch 37\n",
            " batch Loss train: 0.08649592101573944\n",
            "i 180\n",
            "epoch 37\n",
            " batch Loss train: 0.06925337016582489\n",
            "i 181\n",
            "epoch 37\n",
            " batch Loss train: 0.07558344304561615\n",
            "i 182\n",
            "epoch 37\n",
            " batch Loss train: 0.06891477108001709\n",
            "i 183\n",
            "epoch 37\n",
            " batch Loss train: 0.07516413927078247\n",
            "i 184\n",
            "epoch 37\n",
            " batch Loss train: 0.06256153434515\n",
            "i 185\n",
            "epoch 37\n",
            " batch Loss train: 0.05930178239941597\n",
            "i 186\n",
            "epoch 37\n",
            " batch Loss train: 0.06974068284034729\n",
            "i 187\n",
            "epoch 37\n",
            " batch Loss train: 0.07034442573785782\n",
            "i 188\n",
            "epoch 37\n",
            " batch Loss train: 0.06977634131908417\n",
            "i 189\n",
            "epoch 37\n",
            " batch Loss train: 0.07105450332164764\n",
            "i 190\n",
            "epoch 37\n",
            " batch Loss train: 0.05395947024226189\n",
            "i 191\n",
            "epoch 37\n",
            " batch Loss train: 0.08276640623807907\n",
            "i 192\n",
            "epoch 37\n",
            " batch Loss train: 0.09836195409297943\n",
            "i 193\n",
            "epoch 37\n",
            " batch Loss train: 0.05062534287571907\n",
            "i 194\n",
            "epoch 37\n",
            " batch Loss train: 0.06563545018434525\n",
            "i 195\n",
            "epoch 37\n",
            " batch Loss train: 0.07038175314664841\n",
            "i 196\n",
            "epoch 37\n",
            " batch Loss train: 0.07245421409606934\n",
            "i 197\n",
            "epoch 37\n",
            " batch Loss train: 0.06595174968242645\n",
            "i 198\n",
            "epoch 37\n",
            " batch Loss train: 0.0552334226667881\n",
            "i 199\n",
            "epoch 37\n",
            " batch Loss train: 0.09019064158201218\n",
            "i 200\n",
            "epoch 37\n",
            " batch Loss train: 0.09020324796438217\n",
            "i 201\n",
            "epoch 37\n",
            " batch Loss train: 0.0750269666314125\n",
            "i 202\n",
            "epoch 37\n",
            " batch Loss train: 0.08120030164718628\n",
            "i 203\n",
            "epoch 37\n",
            " batch Loss train: 0.08781945705413818\n",
            "i 204\n",
            "epoch 37\n",
            " batch Loss train: 0.06943254172801971\n",
            "i 205\n",
            "epoch 37\n",
            " batch Loss train: 0.06856995075941086\n",
            "i 206\n",
            "epoch 37\n",
            " batch Loss train: 0.08660254627466202\n",
            "i 207\n",
            "epoch 37\n",
            " batch Loss train: 0.07916319370269775\n",
            "i 208\n",
            "epoch 37\n",
            " batch Loss train: 0.059465307742357254\n",
            "i 209\n",
            "epoch 37\n",
            " batch Loss train: 0.0760149136185646\n",
            "i 210\n",
            "epoch 37\n",
            " batch Loss train: 0.05904718115925789\n",
            "i 211\n",
            "epoch 37\n",
            " batch Loss train: 0.06340358406305313\n",
            "i 212\n",
            "epoch 37\n",
            " batch Loss train: 0.06855323910713196\n",
            "i 213\n",
            "epoch 37\n",
            " batch Loss train: 0.046201836317777634\n",
            "i 214\n",
            "epoch 37\n",
            " batch Loss train: 0.06739436089992523\n",
            "i 215\n",
            "epoch 37\n",
            " batch Loss train: 0.07163543254137039\n",
            "i 216\n",
            "epoch 37\n",
            " batch Loss train: 0.07575485110282898\n",
            "i 217\n",
            "epoch 37\n",
            " batch Loss train: 0.06355806440114975\n",
            "i 218\n",
            "epoch 37\n",
            " batch Loss train: 0.049391962587833405\n",
            "i 219\n",
            "epoch 37\n",
            " batch Loss train: 0.06760342419147491\n",
            "i 220\n",
            "epoch 37\n",
            " batch Loss train: 0.064420185983181\n",
            "i 221\n",
            "epoch 37\n",
            " batch Loss train: 0.06922562420368195\n",
            "i 222\n",
            "epoch 37\n",
            " batch Loss train: 0.06066827476024628\n",
            "i 223\n",
            "epoch 37\n",
            " batch Loss train: 0.06841988861560822\n",
            "i 224\n",
            "epoch 37\n",
            " batch Loss train: 0.0809415653347969\n",
            "i 225\n",
            "epoch 37\n",
            " batch Loss train: 0.08021344244480133\n",
            "i 226\n",
            "epoch 37\n",
            " batch Loss train: 0.05823361128568649\n",
            "i 227\n",
            "epoch 37\n",
            " batch Loss train: 0.08822577446699142\n",
            "i 228\n",
            "epoch 37\n",
            " batch Loss train: 0.07202963531017303\n",
            "i 229\n",
            "epoch 37\n",
            " batch Loss train: 0.08142328262329102\n",
            "i 230\n",
            "epoch 37\n",
            " batch Loss train: 0.08453739434480667\n",
            "i 231\n",
            "epoch 37\n",
            " batch Loss train: 0.05604277178645134\n",
            "i 232\n",
            "epoch 37\n",
            " batch Loss train: 0.0695318803191185\n",
            "i 233\n",
            "epoch 37\n",
            " batch Loss train: 0.094518281519413\n",
            "i 234\n",
            "epoch 37\n",
            " batch Loss train: 0.07164902240037918\n",
            "i 235\n",
            "epoch 37\n",
            " batch Loss train: 0.073529914021492\n",
            "i 236\n",
            "epoch 37\n",
            " batch Loss train: 0.07606974244117737\n",
            "i 237\n",
            "epoch 37\n",
            " batch Loss train: 0.06419120728969574\n",
            "i 238\n",
            "epoch 37\n",
            " batch Loss train: 0.06229275092482567\n",
            "i 239\n",
            "epoch 37\n",
            " batch Loss train: 0.0573393851518631\n",
            "i 240\n",
            "epoch 37\n",
            " batch Loss train: 0.06049228459596634\n",
            "i 241\n",
            "epoch 37\n",
            " batch Loss train: 0.07751601189374924\n",
            "i 242\n",
            "epoch 37\n",
            " batch Loss train: 0.060780592262744904\n",
            "i 243\n",
            "epoch 37\n",
            " batch Loss train: 0.07314717769622803\n",
            "i 244\n",
            "epoch 37\n",
            " batch Loss train: 0.08981823921203613\n",
            "i 245\n",
            "epoch 37\n",
            " batch Loss train: 0.06922929733991623\n",
            "i 246\n",
            "epoch 37\n",
            " batch Loss train: 0.06986098736524582\n",
            "i 247\n",
            "epoch 37\n",
            " batch Loss train: 0.06661015003919601\n",
            "i 248\n",
            "epoch 37\n",
            " batch Loss train: 0.06847579032182693\n",
            "i 249\n",
            "epoch 37\n",
            " batch Loss train: 0.09859321266412735\n",
            "i 250\n",
            "epoch 37\n",
            " batch Loss train: 0.07399488985538483\n",
            "i 251\n",
            "epoch 37\n",
            " batch Loss train: 0.05137435719370842\n",
            "i 252\n",
            "epoch 37\n",
            " batch Loss train: 0.08750796318054199\n",
            "i 253\n",
            "epoch 37\n",
            " batch Loss train: 0.07869721949100494\n",
            "i 254\n",
            "epoch 37\n",
            " batch Loss train: 0.06547010689973831\n",
            "i 255\n",
            "epoch 37\n",
            " batch Loss train: 0.08519583195447922\n",
            "i 256\n",
            "epoch 37\n",
            " batch Loss train: 0.0827249139547348\n",
            "i 257\n",
            "epoch 37\n",
            " batch Loss train: 0.10188097506761551\n",
            "i 258\n",
            "epoch 37\n",
            " batch Loss train: 0.06822943687438965\n",
            "i 259\n",
            "epoch 37\n",
            " batch Loss train: 0.0654706358909607\n",
            "i 260\n",
            "epoch 37\n",
            " batch Loss train: 0.0601380430161953\n",
            "i 261\n",
            "epoch 37\n",
            " batch Loss train: 0.08251158893108368\n",
            "i 262\n",
            "epoch 37\n",
            " batch Loss train: 0.07412011176347733\n",
            "i 263\n",
            "epoch 37\n",
            " batch Loss train: 0.06351958215236664\n",
            "i 264\n",
            "epoch 37\n",
            " batch Loss train: 0.07402902096509933\n",
            "i 265\n",
            "epoch 37\n",
            " batch Loss train: 0.09355446696281433\n",
            "i 266\n",
            "epoch 37\n",
            " batch Loss train: 0.05798744410276413\n",
            "i 267\n",
            "epoch 37\n",
            " batch Loss train: 0.07564842700958252\n",
            "i 268\n",
            "epoch 37\n",
            " batch Loss train: 0.0724029541015625\n",
            "i 269\n",
            "epoch 37\n",
            " batch Loss train: 0.07157976180315018\n",
            "i 270\n",
            "epoch 37\n",
            " batch Loss train: 0.05745705962181091\n",
            "i 271\n",
            "epoch 37\n",
            " batch Loss train: 0.06781546026468277\n",
            "i 272\n",
            "epoch 37\n",
            " batch Loss train: 0.10285387933254242\n",
            "i 273\n",
            "epoch 37\n",
            " batch Loss train: 0.06997493654489517\n",
            "i 274\n",
            "epoch 37\n",
            " batch Loss train: 0.06385210901498795\n",
            "i 275\n",
            "epoch 37\n",
            " batch Loss train: 0.08514396846294403\n",
            "i 276\n",
            "epoch 37\n",
            " batch Loss train: 0.07660644501447678\n",
            "i 277\n",
            "epoch 37\n",
            " batch Loss train: 0.08144917339086533\n",
            "i 278\n",
            "epoch 37\n",
            " batch Loss train: 0.08838149905204773\n",
            "i 279\n",
            "epoch 37\n",
            " batch Loss train: 0.0771840363740921\n",
            "i 280\n",
            "epoch 37\n",
            " batch Loss train: 0.06427399069070816\n",
            "i 281\n",
            "epoch 37\n",
            " batch Loss train: 0.07229140400886536\n",
            "i 282\n",
            "epoch 37\n",
            " batch Loss train: 0.07466485351324081\n",
            "i 283\n",
            "epoch 37\n",
            " batch Loss train: 0.08336739242076874\n",
            "i 284\n",
            "epoch 37\n",
            " batch Loss train: 0.06579212844371796\n",
            "i 285\n",
            "epoch 37\n",
            " batch Loss train: 0.08356897532939911\n",
            "i 286\n",
            "epoch 37\n",
            " batch Loss train: 0.0644010379910469\n",
            "i 287\n",
            "epoch 37\n",
            " batch Loss train: 0.07697228342294693\n",
            "i 288\n",
            "epoch 37\n",
            " batch Loss train: 0.07547245174646378\n",
            "i 289\n",
            "epoch 37\n",
            " batch Loss train: 0.060104645788669586\n",
            "i 290\n",
            "epoch 37\n",
            " batch Loss train: 0.07957441359758377\n",
            "i 291\n",
            "epoch 37\n",
            " batch Loss train: 0.07379428297281265\n",
            "i 292\n",
            "epoch 37\n",
            " batch Loss train: 0.07099462300539017\n",
            "i 293\n",
            "epoch 37\n",
            " batch Loss train: 0.08046051859855652\n",
            "i 294\n",
            "epoch 37\n",
            " batch Loss train: 0.09141365438699722\n",
            "i 295\n",
            "epoch 37\n",
            " batch Loss train: 0.07636940479278564\n",
            "i 296\n",
            "epoch 37\n",
            " batch Loss train: 0.05384713038802147\n",
            "i 297\n",
            "epoch 37\n",
            " batch Loss train: 0.06695318967103958\n",
            "i 298\n",
            "epoch 37\n",
            " batch Loss train: 0.07312638312578201\n",
            "i 299\n",
            "epoch 37\n",
            " batch Loss train: 0.06873007118701935\n",
            "i 300\n",
            "epoch 37\n",
            " batch Loss train: 0.06865175813436508\n",
            "i 301\n",
            "epoch 37\n",
            " batch Loss train: 0.07378803193569183\n",
            "i 302\n",
            "epoch 37\n",
            " batch Loss train: 0.06626846641302109\n",
            "i 303\n",
            "epoch 37\n",
            " batch Loss train: 0.0629258006811142\n",
            "i 304\n",
            "epoch 37\n",
            " batch Loss train: 0.07124220579862595\n",
            "i 305\n",
            "epoch 37\n",
            " batch Loss train: 0.08838541805744171\n",
            "i 306\n",
            "epoch 37\n",
            " batch Loss train: 0.08492578566074371\n",
            "i 307\n",
            "epoch 37\n",
            " batch Loss train: 0.09849794209003448\n",
            "i 308\n",
            "epoch 37\n",
            " batch Loss train: 0.09923022985458374\n",
            "i 309\n",
            "epoch 37\n",
            " batch Loss train: 0.08339691162109375\n",
            "i 310\n",
            "epoch 37\n",
            " batch Loss train: 0.07621238380670547\n",
            "i 311\n",
            "epoch 37\n",
            " batch Loss train: 0.08481380343437195\n",
            "i 312\n",
            "epoch 37\n",
            " batch Loss train: 0.07744377106428146\n",
            "i 313\n",
            "epoch 37\n",
            " batch Loss train: 0.07235266268253326\n",
            "i 314\n",
            "epoch 37\n",
            " batch Loss train: 0.07566884905099869\n",
            "i 315\n",
            "epoch 37\n",
            " batch Loss train: 0.09352036565542221\n",
            "i 316\n",
            "epoch 37\n",
            " batch Loss train: 0.07192280888557434\n",
            "i 317\n",
            "epoch 37\n",
            " batch Loss train: 0.07495925575494766\n",
            "i 318\n",
            "epoch 37\n",
            " batch Loss train: 0.09009093791246414\n",
            "i 319\n",
            "epoch 37\n",
            " batch Loss train: 0.06969674676656723\n",
            "i 320\n",
            "epoch 37\n",
            " batch Loss train: 0.07943249493837357\n",
            "i 321\n",
            "epoch 37\n",
            " batch Loss train: 0.07609682530164719\n",
            "i 322\n",
            "epoch 37\n",
            " batch Loss train: 0.08678152412176132\n",
            "i 323\n",
            "epoch 37\n",
            " batch Loss train: 0.07342269271612167\n",
            "i 324\n",
            "epoch 37\n",
            " batch Loss train: 0.06511901319026947\n",
            "i 325\n",
            "epoch 37\n",
            " batch Loss train: 0.08412773162126541\n",
            "i 326\n",
            "epoch 37\n",
            " batch Loss train: 0.10822685062885284\n",
            "i 327\n",
            "epoch 37\n",
            " batch Loss train: 0.08080240339040756\n",
            "i 328\n",
            "epoch 37\n",
            " batch Loss train: 0.0686529353260994\n",
            "i 329\n",
            "epoch 37\n",
            " batch Loss train: 0.08195968717336655\n",
            "i 330\n",
            "epoch 37\n",
            " batch Loss train: 0.06577422469854355\n",
            "i 331\n",
            "epoch 37\n",
            " batch Loss train: 0.11344140022993088\n",
            "i 332\n",
            "epoch 37\n",
            " batch Loss train: 0.064150370657444\n",
            "i 333\n",
            "epoch 37\n",
            " batch Loss train: 0.0917498767375946\n",
            "i 334\n",
            "epoch 37\n",
            " batch Loss train: 0.07680338621139526\n",
            "i 335\n",
            "epoch 37\n",
            " batch Loss train: 0.08097715675830841\n",
            "i 336\n",
            "epoch 37\n",
            " batch Loss train: 0.06874459236860275\n",
            "i 337\n",
            "epoch 37\n",
            " batch Loss train: 0.07489071786403656\n",
            "i 338\n",
            "epoch 37\n",
            " batch Loss train: 0.07393242418766022\n",
            "i 339\n",
            "epoch 37\n",
            " batch Loss train: 0.09453972429037094\n",
            "i 340\n",
            "epoch 37\n",
            " batch Loss train: 0.062007609754800797\n",
            "i 341\n",
            "epoch 37\n",
            " batch Loss train: 0.11758067458868027\n",
            "i 342\n",
            "epoch 37\n",
            " batch Loss train: 0.07046285271644592\n",
            "i 343\n",
            "epoch 37\n",
            " batch Loss train: 0.06184576079249382\n",
            "i 344\n",
            "epoch 37\n",
            " batch Loss train: 0.06980811059474945\n",
            "i 345\n",
            "epoch 37\n",
            " batch Loss train: 0.07797006517648697\n",
            "i 346\n",
            "epoch 37\n",
            " batch Loss train: 0.10167628526687622\n",
            "i 347\n",
            "epoch 37\n",
            " batch Loss train: 0.10174620151519775\n",
            "i 348\n",
            "epoch 37\n",
            " batch Loss train: 0.06650348752737045\n",
            "i 349\n",
            "epoch 37\n",
            " batch Loss train: 0.07700838148593903\n",
            "i 350\n",
            "epoch 37\n",
            " batch Loss train: 0.06268147379159927\n",
            "i 351\n",
            "epoch 37\n",
            " batch Loss train: 0.08119774609804153\n",
            "i 352\n",
            "epoch 37\n",
            " batch Loss train: 0.07511798292398453\n",
            "i 353\n",
            "epoch 37\n",
            " batch Loss train: 0.0652339905500412\n",
            "i 354\n",
            "epoch 37\n",
            " batch Loss train: 0.06050518900156021\n",
            "i 355\n",
            "epoch 37\n",
            " batch Loss train: 0.10422112792730331\n",
            "i 356\n",
            "epoch 37\n",
            " batch Loss train: 0.09377709031105042\n",
            "i 357\n",
            "epoch 37\n",
            " batch Loss train: 0.07543893903493881\n",
            "i 358\n",
            "epoch 37\n",
            " batch Loss train: 0.07194282859563828\n",
            "i 359\n",
            "epoch 37\n",
            " batch Loss train: 0.08388561010360718\n",
            "i 360\n",
            "epoch 37\n",
            " batch Loss train: 0.07104302197694778\n",
            "i 361\n",
            "epoch 37\n",
            " batch Loss train: 0.09068787842988968\n",
            "i 362\n",
            "epoch 37\n",
            " batch Loss train: 0.06912123411893845\n",
            "i 363\n",
            "epoch 37\n",
            " batch Loss train: 0.13393498957157135\n",
            "i 364\n",
            "epoch 37\n",
            " batch Loss train: 0.08544178307056427\n",
            "i 365\n",
            "epoch 37\n",
            " batch Loss train: 0.07285517454147339\n",
            "i 366\n",
            "epoch 37\n",
            " batch Loss train: 0.09574912488460541\n",
            "i 367\n",
            "epoch 37\n",
            " batch Loss train: 0.09252190589904785\n",
            "i 368\n",
            "epoch 37\n",
            " batch Loss train: 0.06846955418586731\n",
            "i 369\n",
            "epoch 37\n",
            " batch Loss train: 0.08423306047916412\n",
            "i 370\n",
            "epoch 37\n",
            " batch Loss train: 0.08683750778436661\n",
            "i 371\n",
            "epoch 37\n",
            " batch Loss train: 0.07852514088153839\n",
            "i 372\n",
            "epoch 37\n",
            " batch Loss train: 0.09252560138702393\n",
            "i 373\n",
            "epoch 37\n",
            " batch Loss train: 0.09785110503435135\n",
            "i 374\n",
            "epoch 37\n",
            " batch Loss train: 0.07585100084543228\n",
            "i 375\n",
            "epoch 37\n",
            " batch Loss train: 0.06394413858652115\n",
            "i 376\n",
            "epoch 37\n",
            " batch Loss train: 0.05652501806616783\n",
            "i 377\n",
            "epoch 37\n",
            " batch Loss train: 0.1250004917383194\n",
            "i 378\n",
            "epoch 37\n",
            " batch Loss train: 0.06909197568893433\n",
            "i 379\n",
            "epoch 37\n",
            " batch Loss train: 0.11641747504472733\n",
            "i 380\n",
            "epoch 37\n",
            " batch Loss train: 0.06794633716344833\n",
            "i 381\n",
            "epoch 37\n",
            " batch Loss train: 0.08553074300289154\n",
            "i 382\n",
            "epoch 37\n",
            " batch Loss train: 0.07989615201950073\n",
            "i 383\n",
            "epoch 37\n",
            " batch Loss train: 0.09004463255405426\n",
            "i 384\n",
            "epoch 37\n",
            " batch Loss train: 0.07293105870485306\n",
            "i 385\n",
            "epoch 37\n",
            " batch Loss train: 0.09338384866714478\n",
            "i 386\n",
            "epoch 37\n",
            " batch Loss train: 0.07382778078317642\n",
            "i 387\n",
            "epoch 37\n",
            " batch Loss train: 0.07366789132356644\n",
            "i 388\n",
            "epoch 37\n",
            " batch Loss train: 0.07071996480226517\n",
            "i 389\n",
            "epoch 37\n",
            " batch Loss train: 0.061544355005025864\n",
            "i 390\n",
            "epoch 37\n",
            " batch Loss train: 0.11615630984306335\n",
            "i 391\n",
            "epoch 37\n",
            " batch Loss train: 0.09235160797834396\n",
            "i 392\n",
            "epoch 37\n",
            " batch Loss train: 0.060460321605205536\n",
            "i 393\n",
            "epoch 37\n",
            " batch Loss train: 0.11225734651088715\n",
            "i 394\n",
            "epoch 37\n",
            " batch Loss train: 0.06727848201990128\n",
            "i 395\n",
            "epoch 37\n",
            " batch Loss train: 0.07440877705812454\n",
            "i 396\n",
            "epoch 37\n",
            " batch Loss train: 0.08653456717729568\n",
            "i 397\n",
            "epoch 37\n",
            " batch Loss train: 0.06608030945062637\n",
            "i 398\n",
            "epoch 37\n",
            " batch Loss train: 0.07353969663381577\n",
            "i 399\n",
            "epoch 37\n",
            " batch Loss train: 0.08497516065835953\n",
            "i 400\n",
            "epoch 37\n",
            " batch Loss train: 0.1042461097240448\n",
            "i 401\n",
            "epoch 37\n",
            " batch Loss train: 0.09725958853960037\n",
            "i 402\n",
            "epoch 37\n",
            " batch Loss train: 0.0918932557106018\n",
            "i 403\n",
            "epoch 37\n",
            " batch Loss train: 0.061767578125\n",
            "i 404\n",
            "epoch 37\n",
            " batch Loss train: 0.09780815988779068\n",
            "i 405\n",
            "epoch 37\n",
            " batch Loss train: 0.07501132041215897\n",
            "i 406\n",
            "epoch 37\n",
            " batch Loss train: 0.056539542973041534\n",
            "i 407\n",
            "epoch 37\n",
            " batch Loss train: 0.10858651995658875\n",
            "i 408\n",
            "epoch 37\n",
            " batch Loss train: 0.08766505122184753\n",
            "i 409\n",
            "epoch 37\n",
            " batch Loss train: 0.07474995404481888\n",
            "i 410\n",
            "epoch 37\n",
            " batch Loss train: 0.08713087439537048\n",
            "i 411\n",
            "epoch 37\n",
            " batch Loss train: 0.088097482919693\n",
            "i 412\n",
            "epoch 37\n",
            " batch Loss train: 0.1290736049413681\n",
            "i 413\n",
            "epoch 37\n",
            " batch Loss train: 0.07550034672021866\n",
            "i 414\n",
            "epoch 37\n",
            " batch Loss train: 0.08738505095243454\n",
            "i 415\n",
            "epoch 37\n",
            " batch Loss train: 0.08982153236865997\n",
            "i 416\n",
            "epoch 37\n",
            " batch Loss train: 0.05712371692061424\n",
            "i 417\n",
            "epoch 37\n",
            " batch Loss train: 0.1080741435289383\n",
            "i 418\n",
            "epoch 37\n",
            " batch Loss train: 0.07197384536266327\n",
            "i 419\n",
            "epoch 37\n",
            " batch Loss train: 0.08136361092329025\n",
            "i 420\n",
            "epoch 37\n",
            " batch Loss train: 0.0852707028388977\n",
            "i 421\n",
            "epoch 37\n",
            " batch Loss train: 0.10221314430236816\n",
            "i 422\n",
            "epoch 37\n",
            " batch Loss train: 0.07560676336288452\n",
            "i 423\n",
            "epoch 37\n",
            " batch Loss train: 0.06524984538555145\n",
            "i 424\n",
            "epoch 37\n",
            " batch Loss train: 0.08307313174009323\n",
            "i 425\n",
            "epoch 37\n",
            " batch Loss train: 0.08931595087051392\n",
            "i 426\n",
            "epoch 37\n",
            " batch Loss train: 0.07505980879068375\n",
            "i 427\n",
            "epoch 37\n",
            " batch Loss train: 0.06785625219345093\n",
            "i 428\n",
            "epoch 37\n",
            " batch Loss train: 0.08358169347047806\n",
            "i 429\n",
            "epoch 37\n",
            " batch Loss train: 0.09375157952308655\n",
            "i 430\n",
            "epoch 37\n",
            " batch Loss train: 0.10494402050971985\n",
            "i 431\n",
            "epoch 37\n",
            " batch Loss train: 0.08719271421432495\n",
            "i 432\n",
            "epoch 37\n",
            " batch Loss train: 0.09083428233861923\n",
            "i 433\n",
            "epoch 37\n",
            " batch Loss train: 0.09783423691987991\n",
            "i 434\n",
            "epoch 37\n",
            " batch Loss train: 0.07886459678411484\n",
            "i 435\n",
            "epoch 37\n",
            " batch Loss train: 0.07816752791404724\n",
            "i 436\n",
            "epoch 37\n",
            " batch Loss train: 0.0892881378531456\n",
            "i 437\n",
            "epoch 37\n",
            " batch Loss train: 0.11508657038211823\n",
            "i 438\n",
            "epoch 37\n",
            " batch Loss train: 0.10879721492528915\n",
            "i 439\n",
            "epoch 37\n",
            " batch Loss train: 0.08290921896696091\n",
            "i 440\n",
            "epoch 37\n",
            " batch Loss train: 0.0827275738120079\n",
            "i 441\n",
            "epoch 37\n",
            " batch Loss train: 0.08317913860082626\n",
            "i 442\n",
            "epoch 37\n",
            " batch Loss train: 0.08629553020000458\n",
            "i 443\n",
            "epoch 37\n",
            " batch Loss train: 0.07530006021261215\n",
            "i 444\n",
            "epoch 37\n",
            " batch Loss train: 0.0963975265622139\n",
            "i 445\n",
            "epoch 37\n",
            " batch Loss train: 0.06890296190977097\n",
            "total epoch Loss train: tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 38\n",
            " batch Loss train: 0.07541711628437042\n",
            "i 1\n",
            "epoch 38\n",
            " batch Loss train: 0.06876314431428909\n",
            "i 2\n",
            "epoch 38\n",
            " batch Loss train: 0.062045179307460785\n",
            "i 3\n",
            "epoch 38\n",
            " batch Loss train: 0.08957920223474503\n",
            "i 4\n",
            "epoch 38\n",
            " batch Loss train: 0.07025045156478882\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 38\n",
            " batch Loss train: 0.07119713723659515\n",
            "i 6\n",
            "epoch 38\n",
            " batch Loss train: 0.06673600524663925\n",
            "i 7\n",
            "epoch 38\n",
            " batch Loss train: 0.05698151886463165\n",
            "i 8\n",
            "epoch 38\n",
            " batch Loss train: 0.08264164626598358\n",
            "i 9\n",
            "epoch 38\n",
            " batch Loss train: 0.05822056904435158\n",
            "i 10\n",
            "epoch 38\n",
            " batch Loss train: 0.09661649912595749\n",
            "i 11\n",
            "epoch 38\n",
            " batch Loss train: 0.06365904957056046\n",
            "i 12\n",
            "epoch 38\n",
            " batch Loss train: 0.11516101658344269\n",
            "i 13\n",
            "epoch 38\n",
            " batch Loss train: 0.04957905784249306\n",
            "i 14\n",
            "epoch 38\n",
            " batch Loss train: 0.07354772090911865\n",
            "i 15\n",
            "epoch 38\n",
            " batch Loss train: 0.07572612911462784\n",
            "i 16\n",
            "epoch 38\n",
            " batch Loss train: 0.05979248136281967\n",
            "i 17\n",
            "epoch 38\n",
            " batch Loss train: 0.07425511628389359\n",
            "i 18\n",
            "epoch 38\n",
            " batch Loss train: 0.07183374464511871\n",
            "i 19\n",
            "epoch 38\n",
            " batch Loss train: 0.06333578377962112\n",
            "i 20\n",
            "epoch 38\n",
            " batch Loss train: 0.060625672340393066\n",
            "i 21\n",
            "epoch 38\n",
            " batch Loss train: 0.07684431225061417\n",
            "i 22\n",
            "epoch 38\n",
            " batch Loss train: 0.06450367718935013\n",
            "i 23\n",
            "epoch 38\n",
            " batch Loss train: 0.08589113503694534\n",
            "i 24\n",
            "epoch 38\n",
            " batch Loss train: 0.06512399762868881\n",
            "i 25\n",
            "epoch 38\n",
            " batch Loss train: 0.05067538470029831\n",
            "i 26\n",
            "epoch 38\n",
            " batch Loss train: 0.07398675382137299\n",
            "i 27\n",
            "epoch 38\n",
            " batch Loss train: 0.07960370182991028\n",
            "i 28\n",
            "epoch 38\n",
            " batch Loss train: 0.06820261478424072\n",
            "i 29\n",
            "epoch 38\n",
            " batch Loss train: 0.051008012145757675\n",
            "i 30\n",
            "epoch 38\n",
            " batch Loss train: 0.07431072741746902\n",
            "i 31\n",
            "epoch 38\n",
            " batch Loss train: 0.06286368519067764\n",
            "i 32\n",
            "epoch 38\n",
            " batch Loss train: 0.0802047997713089\n",
            "i 33\n",
            "epoch 38\n",
            " batch Loss train: 0.06555183231830597\n",
            "i 34\n",
            "epoch 38\n",
            " batch Loss train: 0.07156863063573837\n",
            "i 35\n",
            "epoch 38\n",
            " batch Loss train: 0.05645999312400818\n",
            "i 36\n",
            "epoch 38\n",
            " batch Loss train: 0.09752989560365677\n",
            "i 37\n",
            "epoch 38\n",
            " batch Loss train: 0.08250872790813446\n",
            "i 38\n",
            "epoch 38\n",
            " batch Loss train: 0.06592006236314774\n",
            "i 39\n",
            "epoch 38\n",
            " batch Loss train: 0.09221597015857697\n",
            "i 40\n",
            "epoch 38\n",
            " batch Loss train: 0.06380754709243774\n",
            "i 41\n",
            "epoch 38\n",
            " batch Loss train: 0.061366964131593704\n",
            "i 42\n",
            "epoch 38\n",
            " batch Loss train: 0.05441730469465256\n",
            "i 43\n",
            "epoch 38\n",
            " batch Loss train: 0.06932514160871506\n",
            "i 44\n",
            "epoch 38\n",
            " batch Loss train: 0.06457246094942093\n",
            "i 45\n",
            "epoch 38\n",
            " batch Loss train: 0.049751218408346176\n",
            "i 46\n",
            "epoch 38\n",
            " batch Loss train: 0.07209808379411697\n",
            "i 47\n",
            "epoch 38\n",
            " batch Loss train: 0.06398598849773407\n",
            "i 48\n",
            "epoch 38\n",
            " batch Loss train: 0.057339515537023544\n",
            "i 49\n",
            "epoch 38\n",
            " batch Loss train: 0.062490396201610565\n",
            "i 50\n",
            "epoch 38\n",
            " batch Loss train: 0.10352667421102524\n",
            "i 51\n",
            "epoch 38\n",
            " batch Loss train: 0.06731803715229034\n",
            "i 52\n",
            "epoch 38\n",
            " batch Loss train: 0.05210207402706146\n",
            "i 53\n",
            "epoch 38\n",
            " batch Loss train: 0.07136791199445724\n",
            "i 54\n",
            "epoch 38\n",
            " batch Loss train: 0.08839921653270721\n",
            "i 55\n",
            "epoch 38\n",
            " batch Loss train: 0.061068709939718246\n",
            "i 56\n",
            "epoch 38\n",
            " batch Loss train: 0.06887297332286835\n",
            "i 57\n",
            "epoch 38\n",
            " batch Loss train: 0.08745191246271133\n",
            "i 58\n",
            "epoch 38\n",
            " batch Loss train: 0.0741889625787735\n",
            "i 59\n",
            "epoch 38\n",
            " batch Loss train: 0.08204822987318039\n",
            "i 60\n",
            "epoch 38\n",
            " batch Loss train: 0.06513767689466476\n",
            "i 61\n",
            "epoch 38\n",
            " batch Loss train: 0.045801788568496704\n",
            "i 62\n",
            "epoch 38\n",
            " batch Loss train: 0.05955532565712929\n",
            "i 63\n",
            "epoch 38\n",
            " batch Loss train: 0.10323541611433029\n",
            "i 64\n",
            "epoch 38\n",
            " batch Loss train: 0.06995327025651932\n",
            "i 65\n",
            "epoch 38\n",
            " batch Loss train: 0.06866677105426788\n",
            "i 66\n",
            "epoch 38\n",
            " batch Loss train: 0.07545818388462067\n",
            "i 67\n",
            "epoch 38\n",
            " batch Loss train: 0.060109905898571014\n",
            "i 68\n",
            "epoch 38\n",
            " batch Loss train: 0.07040874660015106\n",
            "i 69\n",
            "epoch 38\n",
            " batch Loss train: 0.07419158518314362\n",
            "i 70\n",
            "epoch 38\n",
            " batch Loss train: 0.05713130161166191\n",
            "i 71\n",
            "epoch 38\n",
            " batch Loss train: 0.060923561453819275\n",
            "i 72\n",
            "epoch 38\n",
            " batch Loss train: 0.09689619392156601\n",
            "i 73\n",
            "epoch 38\n",
            " batch Loss train: 0.07629532366991043\n",
            "i 74\n",
            "epoch 38\n",
            " batch Loss train: 0.05272804573178291\n",
            "i 75\n",
            "epoch 38\n",
            " batch Loss train: 0.06838177889585495\n",
            "i 76\n",
            "epoch 38\n",
            " batch Loss train: 0.06031332165002823\n",
            "i 77\n",
            "epoch 38\n",
            " batch Loss train: 0.0805017352104187\n",
            "i 78\n",
            "epoch 38\n",
            " batch Loss train: 0.06479079276323318\n",
            "i 79\n",
            "epoch 38\n",
            " batch Loss train: 0.06623465567827225\n",
            "i 80\n",
            "epoch 38\n",
            " batch Loss train: 0.07981345057487488\n",
            "i 81\n",
            "epoch 38\n",
            " batch Loss train: 0.07683730870485306\n",
            "i 82\n",
            "epoch 38\n",
            " batch Loss train: 0.0711689293384552\n",
            "i 83\n",
            "epoch 38\n",
            " batch Loss train: 0.07206606864929199\n",
            "i 84\n",
            "epoch 38\n",
            " batch Loss train: 0.06315739452838898\n",
            "i 85\n",
            "epoch 38\n",
            " batch Loss train: 0.09381229430437088\n",
            "i 86\n",
            "epoch 38\n",
            " batch Loss train: 0.07517261803150177\n",
            "i 87\n",
            "epoch 38\n",
            " batch Loss train: 0.0881420224905014\n",
            "i 88\n",
            "epoch 38\n",
            " batch Loss train: 0.07647599279880524\n",
            "i 89\n",
            "epoch 38\n",
            " batch Loss train: 0.07196483016014099\n",
            "i 90\n",
            "epoch 38\n",
            " batch Loss train: 0.08754482120275497\n",
            "i 91\n",
            "epoch 38\n",
            " batch Loss train: 0.07516933232545853\n",
            "i 92\n",
            "epoch 38\n",
            " batch Loss train: 0.0847972109913826\n",
            "i 93\n",
            "epoch 38\n",
            " batch Loss train: 0.062164485454559326\n",
            "i 94\n",
            "epoch 38\n",
            " batch Loss train: 0.07710235565900803\n",
            "i 95\n",
            "epoch 38\n",
            " batch Loss train: 0.07293029129505157\n",
            "i 96\n",
            "epoch 38\n",
            " batch Loss train: 0.06504659354686737\n",
            "i 97\n",
            "epoch 38\n",
            " batch Loss train: 0.0798054188489914\n",
            "i 98\n",
            "epoch 38\n",
            " batch Loss train: 0.07848592102527618\n",
            "i 99\n",
            "epoch 38\n",
            " batch Loss train: 0.07686816155910492\n",
            "i 100\n",
            "epoch 38\n",
            " batch Loss train: 0.06724178791046143\n",
            "i 101\n",
            "epoch 38\n",
            " batch Loss train: 0.05228539928793907\n",
            "i 102\n",
            "epoch 38\n",
            " batch Loss train: 0.06316986680030823\n",
            "i 103\n",
            "epoch 38\n",
            " batch Loss train: 0.07781890034675598\n",
            "i 104\n",
            "epoch 38\n",
            " batch Loss train: 0.07539685070514679\n",
            "i 105\n",
            "epoch 38\n",
            " batch Loss train: 0.06694345921278\n",
            "i 106\n",
            "epoch 38\n",
            " batch Loss train: 0.08029983937740326\n",
            "i 107\n",
            "epoch 38\n",
            " batch Loss train: 0.059165749698877335\n",
            "i 108\n",
            "epoch 38\n",
            " batch Loss train: 0.07587387412786484\n",
            "i 109\n",
            "epoch 38\n",
            " batch Loss train: 0.06766358017921448\n",
            "i 110\n",
            "epoch 38\n",
            " batch Loss train: 0.08671648055315018\n",
            "i 111\n",
            "epoch 38\n",
            " batch Loss train: 0.07606691867113113\n",
            "i 112\n",
            "epoch 38\n",
            " batch Loss train: 0.08592357486486435\n",
            "i 113\n",
            "epoch 38\n",
            " batch Loss train: 0.07569057494401932\n",
            "i 114\n",
            "epoch 38\n",
            " batch Loss train: 0.06570200622081757\n",
            "i 115\n",
            "epoch 38\n",
            " batch Loss train: 0.07395239919424057\n",
            "i 116\n",
            "epoch 38\n",
            " batch Loss train: 0.08352236449718475\n",
            "i 117\n",
            "epoch 38\n",
            " batch Loss train: 0.08298390358686447\n",
            "i 118\n",
            "epoch 38\n",
            " batch Loss train: 0.08631812036037445\n",
            "i 119\n",
            "epoch 38\n",
            " batch Loss train: 0.060169219970703125\n",
            "i 120\n",
            "epoch 38\n",
            " batch Loss train: 0.09186547249555588\n",
            "i 121\n",
            "epoch 38\n",
            " batch Loss train: 0.06082470342516899\n",
            "i 122\n",
            "epoch 38\n",
            " batch Loss train: 0.03714478388428688\n",
            "i 123\n",
            "epoch 38\n",
            " batch Loss train: 0.07407121360301971\n",
            "i 124\n",
            "epoch 38\n",
            " batch Loss train: 0.06518065184354782\n",
            "i 125\n",
            "epoch 38\n",
            " batch Loss train: 0.0843714103102684\n",
            "i 126\n",
            "epoch 38\n",
            " batch Loss train: 0.07462552189826965\n",
            "i 127\n",
            "epoch 38\n",
            " batch Loss train: 0.06916628032922745\n",
            "i 128\n",
            "epoch 38\n",
            " batch Loss train: 0.062477558851242065\n",
            "i 129\n",
            "epoch 38\n",
            " batch Loss train: 0.06420442461967468\n",
            "i 130\n",
            "epoch 38\n",
            " batch Loss train: 0.0771515965461731\n",
            "i 131\n",
            "epoch 38\n",
            " batch Loss train: 0.0802229568362236\n",
            "i 132\n",
            "epoch 38\n",
            " batch Loss train: 0.06259936094284058\n",
            "i 133\n",
            "epoch 38\n",
            " batch Loss train: 0.0486518032848835\n",
            "i 134\n",
            "epoch 38\n",
            " batch Loss train: 0.04069564864039421\n",
            "i 135\n",
            "epoch 38\n",
            " batch Loss train: 0.04628465697169304\n",
            "i 136\n",
            "epoch 38\n",
            " batch Loss train: 0.06551149487495422\n",
            "i 137\n",
            "epoch 38\n",
            " batch Loss train: 0.07485520839691162\n",
            "i 138\n",
            "epoch 38\n",
            " batch Loss train: 0.08254421502351761\n",
            "i 139\n",
            "epoch 38\n",
            " batch Loss train: 0.07193192094564438\n",
            "i 140\n",
            "epoch 38\n",
            " batch Loss train: 0.059701524674892426\n",
            "i 141\n",
            "epoch 38\n",
            " batch Loss train: 0.06319186091423035\n",
            "i 142\n",
            "epoch 38\n",
            " batch Loss train: 0.07076025009155273\n",
            "i 143\n",
            "epoch 38\n",
            " batch Loss train: 0.05247565731406212\n",
            "i 144\n",
            "epoch 38\n",
            " batch Loss train: 0.06880252063274384\n",
            "i 145\n",
            "epoch 38\n",
            " batch Loss train: 0.05995277315378189\n",
            "i 146\n",
            "epoch 38\n",
            " batch Loss train: 0.0684535801410675\n",
            "i 147\n",
            "epoch 38\n",
            " batch Loss train: 0.062309302389621735\n",
            "i 148\n",
            "epoch 38\n",
            " batch Loss train: 0.06064825505018234\n",
            "i 149\n",
            "epoch 38\n",
            " batch Loss train: 0.06799588352441788\n",
            "i 150\n",
            "epoch 38\n",
            " batch Loss train: 0.09424524009227753\n",
            "i 151\n",
            "epoch 38\n",
            " batch Loss train: 0.06369634717702866\n",
            "i 152\n",
            "epoch 38\n",
            " batch Loss train: 0.06818472594022751\n",
            "i 153\n",
            "epoch 38\n",
            " batch Loss train: 0.07027541846036911\n",
            "i 154\n",
            "epoch 38\n",
            " batch Loss train: 0.08936820179224014\n",
            "i 155\n",
            "epoch 38\n",
            " batch Loss train: 0.06540601700544357\n",
            "i 156\n",
            "epoch 38\n",
            " batch Loss train: 0.061063263565301895\n",
            "i 157\n",
            "epoch 38\n",
            " batch Loss train: 0.06100061163306236\n",
            "i 158\n",
            "epoch 38\n",
            " batch Loss train: 0.06884802132844925\n",
            "i 159\n",
            "epoch 38\n",
            " batch Loss train: 0.0598100982606411\n",
            "i 160\n",
            "epoch 38\n",
            " batch Loss train: 0.05874062329530716\n",
            "i 161\n",
            "epoch 38\n",
            " batch Loss train: 0.07053860276937485\n",
            "i 162\n",
            "epoch 38\n",
            " batch Loss train: 0.08667498081922531\n",
            "i 163\n",
            "epoch 38\n",
            " batch Loss train: 0.05165538936853409\n",
            "i 164\n",
            "epoch 38\n",
            " batch Loss train: 0.0777193158864975\n",
            "i 165\n",
            "epoch 38\n",
            " batch Loss train: 0.070356085896492\n",
            "i 166\n",
            "epoch 38\n",
            " batch Loss train: 0.0835791528224945\n",
            "i 167\n",
            "epoch 38\n",
            " batch Loss train: 0.07087148725986481\n",
            "i 168\n",
            "epoch 38\n",
            " batch Loss train: 0.06846249103546143\n",
            "i 169\n",
            "epoch 38\n",
            " batch Loss train: 0.062313683331012726\n",
            "i 170\n",
            "epoch 38\n",
            " batch Loss train: 0.0730413943529129\n",
            "i 171\n",
            "epoch 38\n",
            " batch Loss train: 0.08801401406526566\n",
            "i 172\n",
            "epoch 38\n",
            " batch Loss train: 0.06509944051504135\n",
            "i 173\n",
            "epoch 38\n",
            " batch Loss train: 0.08566833287477493\n",
            "i 174\n",
            "epoch 38\n",
            " batch Loss train: 0.09070675075054169\n",
            "i 175\n",
            "epoch 38\n",
            " batch Loss train: 0.07052688300609589\n",
            "i 176\n",
            "epoch 38\n",
            " batch Loss train: 0.0696118101477623\n",
            "i 177\n",
            "epoch 38\n",
            " batch Loss train: 0.0690622478723526\n",
            "i 178\n",
            "epoch 38\n",
            " batch Loss train: 0.05727824568748474\n",
            "i 179\n",
            "epoch 38\n",
            " batch Loss train: 0.06673019379377365\n",
            "i 180\n",
            "epoch 38\n",
            " batch Loss train: 0.06444763392210007\n",
            "i 181\n",
            "epoch 38\n",
            " batch Loss train: 0.0666985809803009\n",
            "i 182\n",
            "epoch 38\n",
            " batch Loss train: 0.061267901211977005\n",
            "i 183\n",
            "epoch 38\n",
            " batch Loss train: 0.06942788511514664\n",
            "i 184\n",
            "epoch 38\n",
            " batch Loss train: 0.06825809925794601\n",
            "i 185\n",
            "epoch 38\n",
            " batch Loss train: 0.0523671917617321\n",
            "i 186\n",
            "epoch 38\n",
            " batch Loss train: 0.08447347581386566\n",
            "i 187\n",
            "epoch 38\n",
            " batch Loss train: 0.06969255954027176\n",
            "i 188\n",
            "epoch 38\n",
            " batch Loss train: 0.07871587574481964\n",
            "i 189\n",
            "epoch 38\n",
            " batch Loss train: 0.06763257086277008\n",
            "i 190\n",
            "epoch 38\n",
            " batch Loss train: 0.05061082914471626\n",
            "i 191\n",
            "epoch 38\n",
            " batch Loss train: 0.07670850306749344\n",
            "i 192\n",
            "epoch 38\n",
            " batch Loss train: 0.06805340945720673\n",
            "i 193\n",
            "epoch 38\n",
            " batch Loss train: 0.08383835107088089\n",
            "i 194\n",
            "epoch 38\n",
            " batch Loss train: 0.09859541803598404\n",
            "i 195\n",
            "epoch 38\n",
            " batch Loss train: 0.0887284204363823\n",
            "i 196\n",
            "epoch 38\n",
            " batch Loss train: 0.0930996835231781\n",
            "i 197\n",
            "epoch 38\n",
            " batch Loss train: 0.07448064535856247\n",
            "i 198\n",
            "epoch 38\n",
            " batch Loss train: 0.10026104748249054\n",
            "i 199\n",
            "epoch 38\n",
            " batch Loss train: 0.0765843614935875\n",
            "i 200\n",
            "epoch 38\n",
            " batch Loss train: 0.08010070025920868\n",
            "i 201\n",
            "epoch 38\n",
            " batch Loss train: 0.0626821368932724\n",
            "i 202\n",
            "epoch 38\n",
            " batch Loss train: 0.07102134078741074\n",
            "i 203\n",
            "epoch 38\n",
            " batch Loss train: 0.05968179181218147\n",
            "i 204\n",
            "epoch 38\n",
            " batch Loss train: 0.07808534801006317\n",
            "i 205\n",
            "epoch 38\n",
            " batch Loss train: 0.07115630060434341\n",
            "i 206\n",
            "epoch 38\n",
            " batch Loss train: 0.07354386150836945\n",
            "i 207\n",
            "epoch 38\n",
            " batch Loss train: 0.07947554439306259\n",
            "i 208\n",
            "epoch 38\n",
            " batch Loss train: 0.06895127147436142\n",
            "i 209\n",
            "epoch 38\n",
            " batch Loss train: 0.05087747424840927\n",
            "i 210\n",
            "epoch 38\n",
            " batch Loss train: 0.08024544268846512\n",
            "i 211\n",
            "epoch 38\n",
            " batch Loss train: 0.06419339776039124\n",
            "i 212\n",
            "epoch 38\n",
            " batch Loss train: 0.0656495988368988\n",
            "i 213\n",
            "epoch 38\n",
            " batch Loss train: 0.06864088773727417\n",
            "i 214\n",
            "epoch 38\n",
            " batch Loss train: 0.06265479326248169\n",
            "i 215\n",
            "epoch 38\n",
            " batch Loss train: 0.050575267523527145\n",
            "i 216\n",
            "epoch 38\n",
            " batch Loss train: 0.06924154609441757\n",
            "i 217\n",
            "epoch 38\n",
            " batch Loss train: 0.057283636182546616\n",
            "i 218\n",
            "epoch 38\n",
            " batch Loss train: 0.09611760079860687\n",
            "i 219\n",
            "epoch 38\n",
            " batch Loss train: 0.0591263510286808\n",
            "i 220\n",
            "epoch 38\n",
            " batch Loss train: 0.05434379354119301\n",
            "i 221\n",
            "epoch 38\n",
            " batch Loss train: 0.08434585481882095\n",
            "i 222\n",
            "epoch 38\n",
            " batch Loss train: 0.058637842535972595\n",
            "i 223\n",
            "epoch 38\n",
            " batch Loss train: 0.07473891228437424\n",
            "i 224\n",
            "epoch 38\n",
            " batch Loss train: 0.09170828759670258\n",
            "i 225\n",
            "epoch 38\n",
            " batch Loss train: 0.06455466151237488\n",
            "i 226\n",
            "epoch 38\n",
            " batch Loss train: 0.0682477355003357\n",
            "i 227\n",
            "epoch 38\n",
            " batch Loss train: 0.0649823546409607\n",
            "i 228\n",
            "epoch 38\n",
            " batch Loss train: 0.06341490149497986\n",
            "i 229\n",
            "epoch 38\n",
            " batch Loss train: 0.081361785531044\n",
            "i 230\n",
            "epoch 38\n",
            " batch Loss train: 0.07255035638809204\n",
            "i 231\n",
            "epoch 38\n",
            " batch Loss train: 0.06848294287919998\n",
            "i 232\n",
            "epoch 38\n",
            " batch Loss train: 0.0812031701207161\n",
            "i 233\n",
            "epoch 38\n",
            " batch Loss train: 0.07516219466924667\n",
            "i 234\n",
            "epoch 38\n",
            " batch Loss train: 0.07744215428829193\n",
            "i 235\n",
            "epoch 38\n",
            " batch Loss train: 0.08139286935329437\n",
            "i 236\n",
            "epoch 38\n",
            " batch Loss train: 0.0692780613899231\n",
            "i 237\n",
            "epoch 38\n",
            " batch Loss train: 0.06786701083183289\n",
            "i 238\n",
            "epoch 38\n",
            " batch Loss train: 0.05868609622120857\n",
            "i 239\n",
            "epoch 38\n",
            " batch Loss train: 0.06313523650169373\n",
            "i 240\n",
            "epoch 38\n",
            " batch Loss train: 0.07110629975795746\n",
            "i 241\n",
            "epoch 38\n",
            " batch Loss train: 0.07206614315509796\n",
            "i 242\n",
            "epoch 38\n",
            " batch Loss train: 0.07809381932020187\n",
            "i 243\n",
            "epoch 38\n",
            " batch Loss train: 0.05058642104268074\n",
            "i 244\n",
            "epoch 38\n",
            " batch Loss train: 0.07428039610385895\n",
            "i 245\n",
            "epoch 38\n",
            " batch Loss train: 0.07959970086812973\n",
            "i 246\n",
            "epoch 38\n",
            " batch Loss train: 0.08832614868879318\n",
            "i 247\n",
            "epoch 38\n",
            " batch Loss train: 0.07257730513811111\n",
            "i 248\n",
            "epoch 38\n",
            " batch Loss train: 0.06795535236597061\n",
            "i 249\n",
            "epoch 38\n",
            " batch Loss train: 0.0736309364438057\n",
            "i 250\n",
            "epoch 38\n",
            " batch Loss train: 0.06193946674466133\n",
            "i 251\n",
            "epoch 38\n",
            " batch Loss train: 0.08503352850675583\n",
            "i 252\n",
            "epoch 38\n",
            " batch Loss train: 0.07945860922336578\n",
            "i 253\n",
            "epoch 38\n",
            " batch Loss train: 0.07447008043527603\n",
            "i 254\n",
            "epoch 38\n",
            " batch Loss train: 0.07546661049127579\n",
            "i 255\n",
            "epoch 38\n",
            " batch Loss train: 0.07663583010435104\n",
            "i 256\n",
            "epoch 38\n",
            " batch Loss train: 0.06919486075639725\n",
            "i 257\n",
            "epoch 38\n",
            " batch Loss train: 0.06370829045772552\n",
            "i 258\n",
            "epoch 38\n",
            " batch Loss train: 0.06980332732200623\n",
            "i 259\n",
            "epoch 38\n",
            " batch Loss train: 0.07308902591466904\n",
            "i 260\n",
            "epoch 38\n",
            " batch Loss train: 0.08510314673185349\n",
            "i 261\n",
            "epoch 38\n",
            " batch Loss train: 0.059571415185928345\n",
            "i 262\n",
            "epoch 38\n",
            " batch Loss train: 0.06312406808137894\n",
            "i 263\n",
            "epoch 38\n",
            " batch Loss train: 0.07557769119739532\n",
            "i 264\n",
            "epoch 38\n",
            " batch Loss train: 0.059940654784440994\n",
            "i 265\n",
            "epoch 38\n",
            " batch Loss train: 0.061431318521499634\n",
            "i 266\n",
            "epoch 38\n",
            " batch Loss train: 0.08265809714794159\n",
            "i 267\n",
            "epoch 38\n",
            " batch Loss train: 0.06864959746599197\n",
            "i 268\n",
            "epoch 38\n",
            " batch Loss train: 0.07188855856657028\n",
            "i 269\n",
            "epoch 38\n",
            " batch Loss train: 0.06997686624526978\n",
            "i 270\n",
            "epoch 38\n",
            " batch Loss train: 0.08067350834608078\n",
            "i 271\n",
            "epoch 38\n",
            " batch Loss train: 0.08013427257537842\n",
            "i 272\n",
            "epoch 38\n",
            " batch Loss train: 0.09577099978923798\n",
            "i 273\n",
            "epoch 38\n",
            " batch Loss train: 0.0664776861667633\n",
            "i 274\n",
            "epoch 38\n",
            " batch Loss train: 0.07077120244503021\n",
            "i 275\n",
            "epoch 38\n",
            " batch Loss train: 0.07525651156902313\n",
            "i 276\n",
            "epoch 38\n",
            " batch Loss train: 0.06573482602834702\n",
            "i 277\n",
            "epoch 38\n",
            " batch Loss train: 0.08296922594308853\n",
            "i 278\n",
            "epoch 38\n",
            " batch Loss train: 0.072150819003582\n",
            "i 279\n",
            "epoch 38\n",
            " batch Loss train: 0.08951283246278763\n",
            "i 280\n",
            "epoch 38\n",
            " batch Loss train: 0.08962833881378174\n",
            "i 281\n",
            "epoch 38\n",
            " batch Loss train: 0.06988836079835892\n",
            "i 282\n",
            "epoch 38\n",
            " batch Loss train: 0.08974198251962662\n",
            "i 283\n",
            "epoch 38\n",
            " batch Loss train: 0.08059976249933243\n",
            "i 284\n",
            "epoch 38\n",
            " batch Loss train: 0.0762045755982399\n",
            "i 285\n",
            "epoch 38\n",
            " batch Loss train: 0.05968010425567627\n",
            "i 286\n",
            "epoch 38\n",
            " batch Loss train: 0.077264204621315\n",
            "i 287\n",
            "epoch 38\n",
            " batch Loss train: 0.06203757971525192\n",
            "i 288\n",
            "epoch 38\n",
            " batch Loss train: 0.09130033850669861\n",
            "i 289\n",
            "epoch 38\n",
            " batch Loss train: 0.06980165839195251\n",
            "i 290\n",
            "epoch 38\n",
            " batch Loss train: 0.10121910274028778\n",
            "i 291\n",
            "epoch 38\n",
            " batch Loss train: 0.05973327159881592\n",
            "i 292\n",
            "epoch 38\n",
            " batch Loss train: 0.07552224397659302\n",
            "i 293\n",
            "epoch 38\n",
            " batch Loss train: 0.07148634642362595\n",
            "i 294\n",
            "epoch 38\n",
            " batch Loss train: 0.06202300637960434\n",
            "i 295\n",
            "epoch 38\n",
            " batch Loss train: 0.06996537744998932\n",
            "i 296\n",
            "epoch 38\n",
            " batch Loss train: 0.08241820335388184\n",
            "i 297\n",
            "epoch 38\n",
            " batch Loss train: 0.062024571001529694\n",
            "i 298\n",
            "epoch 38\n",
            " batch Loss train: 0.06058875471353531\n",
            "i 299\n",
            "epoch 38\n",
            " batch Loss train: 0.10454678535461426\n",
            "i 300\n",
            "epoch 38\n",
            " batch Loss train: 0.05639822781085968\n",
            "i 301\n",
            "epoch 38\n",
            " batch Loss train: 0.0791604071855545\n",
            "i 302\n",
            "epoch 38\n",
            " batch Loss train: 0.06522857397794724\n",
            "i 303\n",
            "epoch 38\n",
            " batch Loss train: 0.08678993582725525\n",
            "i 304\n",
            "epoch 38\n",
            " batch Loss train: 0.08873055130243301\n",
            "i 305\n",
            "epoch 38\n",
            " batch Loss train: 0.06839760392904282\n",
            "i 306\n",
            "epoch 38\n",
            " batch Loss train: 0.061636313796043396\n",
            "i 307\n",
            "epoch 38\n",
            " batch Loss train: 0.06219680979847908\n",
            "i 308\n",
            "epoch 38\n",
            " batch Loss train: 0.09279298782348633\n",
            "i 309\n",
            "epoch 38\n",
            " batch Loss train: 0.0766238123178482\n",
            "i 310\n",
            "epoch 38\n",
            " batch Loss train: 0.08071301877498627\n",
            "i 311\n",
            "epoch 38\n",
            " batch Loss train: 0.09083044528961182\n",
            "i 312\n",
            "epoch 38\n",
            " batch Loss train: 0.10629279166460037\n",
            "i 313\n",
            "epoch 38\n",
            " batch Loss train: 0.07353945821523666\n",
            "i 314\n",
            "epoch 38\n",
            " batch Loss train: 0.08186262845993042\n",
            "i 315\n",
            "epoch 38\n",
            " batch Loss train: 0.08010932058095932\n",
            "i 316\n",
            "epoch 38\n",
            " batch Loss train: 0.08785485476255417\n",
            "i 317\n",
            "epoch 38\n",
            " batch Loss train: 0.05940709263086319\n",
            "i 318\n",
            "epoch 38\n",
            " batch Loss train: 0.06402044743299484\n",
            "i 319\n",
            "epoch 38\n",
            " batch Loss train: 0.09331022202968597\n",
            "i 320\n",
            "epoch 38\n",
            " batch Loss train: 0.05090665817260742\n",
            "i 321\n",
            "epoch 38\n",
            " batch Loss train: 0.10257721692323685\n",
            "i 322\n",
            "epoch 38\n",
            " batch Loss train: 0.08141543716192245\n",
            "i 323\n",
            "epoch 38\n",
            " batch Loss train: 0.08171076327562332\n",
            "i 324\n",
            "epoch 38\n",
            " batch Loss train: 0.060638427734375\n",
            "i 325\n",
            "epoch 38\n",
            " batch Loss train: 0.06210638955235481\n",
            "i 326\n",
            "epoch 38\n",
            " batch Loss train: 0.12924806773662567\n",
            "i 327\n",
            "epoch 38\n",
            " batch Loss train: 0.07323353737592697\n",
            "i 328\n",
            "epoch 38\n",
            " batch Loss train: 0.06521579623222351\n",
            "i 329\n",
            "epoch 38\n",
            " batch Loss train: 0.07071220874786377\n",
            "i 330\n",
            "epoch 38\n",
            " batch Loss train: 0.11113817989826202\n",
            "i 331\n",
            "epoch 38\n",
            " batch Loss train: 0.07461465150117874\n",
            "i 332\n",
            "epoch 38\n",
            " batch Loss train: 0.08573135733604431\n",
            "i 333\n",
            "epoch 38\n",
            " batch Loss train: 0.07859921455383301\n",
            "i 334\n",
            "epoch 38\n",
            " batch Loss train: 0.08578607439994812\n",
            "i 335\n",
            "epoch 38\n",
            " batch Loss train: 0.08825555443763733\n",
            "i 336\n",
            "epoch 38\n",
            " batch Loss train: 0.0811413824558258\n",
            "i 337\n",
            "epoch 38\n",
            " batch Loss train: 0.08765353262424469\n",
            "i 338\n",
            "epoch 38\n",
            " batch Loss train: 0.09094098955392838\n",
            "i 339\n",
            "epoch 38\n",
            " batch Loss train: 0.07035870105028152\n",
            "i 340\n",
            "epoch 38\n",
            " batch Loss train: 0.07356341928243637\n",
            "i 341\n",
            "epoch 38\n",
            " batch Loss train: 0.07807651162147522\n",
            "i 342\n",
            "epoch 38\n",
            " batch Loss train: 0.08095138520002365\n",
            "i 343\n",
            "epoch 38\n",
            " batch Loss train: 0.060025207698345184\n",
            "i 344\n",
            "epoch 38\n",
            " batch Loss train: 0.06382289528846741\n",
            "i 345\n",
            "epoch 38\n",
            " batch Loss train: 0.06521609425544739\n",
            "i 346\n",
            "epoch 38\n",
            " batch Loss train: 0.07987631112337112\n",
            "i 347\n",
            "epoch 38\n",
            " batch Loss train: 0.05868424102663994\n",
            "i 348\n",
            "epoch 38\n",
            " batch Loss train: 0.06649409979581833\n",
            "i 349\n",
            "epoch 38\n",
            " batch Loss train: 0.09970436245203018\n",
            "i 350\n",
            "epoch 38\n",
            " batch Loss train: 0.0764968991279602\n",
            "i 351\n",
            "epoch 38\n",
            " batch Loss train: 0.06777137517929077\n",
            "i 352\n",
            "epoch 38\n",
            " batch Loss train: 0.07385266572237015\n",
            "i 353\n",
            "epoch 38\n",
            " batch Loss train: 0.08469768613576889\n",
            "i 354\n",
            "epoch 38\n",
            " batch Loss train: 0.08476275205612183\n",
            "i 355\n",
            "epoch 38\n",
            " batch Loss train: 0.06383929401636124\n",
            "i 356\n",
            "epoch 38\n",
            " batch Loss train: 0.07273461669683456\n",
            "i 357\n",
            "epoch 38\n",
            " batch Loss train: 0.10658993571996689\n",
            "i 358\n",
            "epoch 38\n",
            " batch Loss train: 0.09389353543519974\n",
            "i 359\n",
            "epoch 38\n",
            " batch Loss train: 0.06038080155849457\n",
            "i 360\n",
            "epoch 38\n",
            " batch Loss train: 0.06181596592068672\n",
            "i 361\n",
            "epoch 38\n",
            " batch Loss train: 0.09920429438352585\n",
            "i 362\n",
            "epoch 38\n",
            " batch Loss train: 0.08377253264188766\n",
            "i 363\n",
            "epoch 38\n",
            " batch Loss train: 0.07354307174682617\n",
            "i 364\n",
            "epoch 38\n",
            " batch Loss train: 0.09362361580133438\n",
            "i 365\n",
            "epoch 38\n",
            " batch Loss train: 0.07029932737350464\n",
            "i 366\n",
            "epoch 38\n",
            " batch Loss train: 0.07138641923666\n",
            "i 367\n",
            "epoch 38\n",
            " batch Loss train: 0.07663729786872864\n",
            "i 368\n",
            "epoch 38\n",
            " batch Loss train: 0.052808068692684174\n",
            "i 369\n",
            "epoch 38\n",
            " batch Loss train: 0.06285034120082855\n",
            "i 370\n",
            "epoch 38\n",
            " batch Loss train: 0.08706018328666687\n",
            "i 371\n",
            "epoch 38\n",
            " batch Loss train: 0.08589968085289001\n",
            "i 372\n",
            "epoch 38\n",
            " batch Loss train: 0.06764347851276398\n",
            "i 373\n",
            "epoch 38\n",
            " batch Loss train: 0.07610484957695007\n",
            "i 374\n",
            "epoch 38\n",
            " batch Loss train: 0.11300162225961685\n",
            "i 375\n",
            "epoch 38\n",
            " batch Loss train: 0.08394165337085724\n",
            "i 376\n",
            "epoch 38\n",
            " batch Loss train: 0.10185804963111877\n",
            "i 377\n",
            "epoch 38\n",
            " batch Loss train: 0.08561906218528748\n",
            "i 378\n",
            "epoch 38\n",
            " batch Loss train: 0.06819834560155869\n",
            "i 379\n",
            "epoch 38\n",
            " batch Loss train: 0.05978158861398697\n",
            "i 380\n",
            "epoch 38\n",
            " batch Loss train: 0.08941588550806046\n",
            "i 381\n",
            "epoch 38\n",
            " batch Loss train: 0.10086987912654877\n",
            "i 382\n",
            "epoch 38\n",
            " batch Loss train: 0.0814591571688652\n",
            "i 383\n",
            "epoch 38\n",
            " batch Loss train: 0.08993296325206757\n",
            "i 384\n",
            "epoch 38\n",
            " batch Loss train: 0.07442141324281693\n",
            "i 385\n",
            "epoch 38\n",
            " batch Loss train: 0.06914548575878143\n",
            "i 386\n",
            "epoch 38\n",
            " batch Loss train: 0.06501639634370804\n",
            "i 387\n",
            "epoch 38\n",
            " batch Loss train: 0.11582094430923462\n",
            "i 388\n",
            "epoch 38\n",
            " batch Loss train: 0.07065419107675552\n",
            "i 389\n",
            "epoch 38\n",
            " batch Loss train: 0.0534382127225399\n",
            "i 390\n",
            "epoch 38\n",
            " batch Loss train: 0.0823424831032753\n",
            "i 391\n",
            "epoch 38\n",
            " batch Loss train: 0.07976123690605164\n",
            "i 392\n",
            "epoch 38\n",
            " batch Loss train: 0.06810486316680908\n",
            "i 393\n",
            "epoch 38\n",
            " batch Loss train: 0.06172024458646774\n",
            "i 394\n",
            "epoch 38\n",
            " batch Loss train: 0.0705200731754303\n",
            "i 395\n",
            "epoch 38\n",
            " batch Loss train: 0.08394177258014679\n",
            "i 396\n",
            "epoch 38\n",
            " batch Loss train: 0.10546109080314636\n",
            "i 397\n",
            "epoch 38\n",
            " batch Loss train: 0.10923080891370773\n",
            "i 398\n",
            "epoch 38\n",
            " batch Loss train: 0.09920108318328857\n",
            "i 399\n",
            "epoch 38\n",
            " batch Loss train: 0.08447717130184174\n",
            "i 400\n",
            "epoch 38\n",
            " batch Loss train: 0.07509283721446991\n",
            "i 401\n",
            "epoch 38\n",
            " batch Loss train: 0.060240089893341064\n",
            "i 402\n",
            "epoch 38\n",
            " batch Loss train: 0.07919425517320633\n",
            "i 403\n",
            "epoch 38\n",
            " batch Loss train: 0.09764810651540756\n",
            "i 404\n",
            "epoch 38\n",
            " batch Loss train: 0.06216075271368027\n",
            "i 405\n",
            "epoch 38\n",
            " batch Loss train: 0.0663987547159195\n",
            "i 406\n",
            "epoch 38\n",
            " batch Loss train: 0.07319928705692291\n",
            "i 407\n",
            "epoch 38\n",
            " batch Loss train: 0.10872431844472885\n",
            "i 408\n",
            "epoch 38\n",
            " batch Loss train: 0.08329161256551743\n",
            "i 409\n",
            "epoch 38\n",
            " batch Loss train: 0.09574343264102936\n",
            "i 410\n",
            "epoch 38\n",
            " batch Loss train: 0.07754039764404297\n",
            "i 411\n",
            "epoch 38\n",
            " batch Loss train: 0.10089005529880524\n",
            "i 412\n",
            "epoch 38\n",
            " batch Loss train: 0.08465199917554855\n",
            "i 413\n",
            "epoch 38\n",
            " batch Loss train: 0.08617862313985825\n",
            "i 414\n",
            "epoch 38\n",
            " batch Loss train: 0.09714701026678085\n",
            "i 415\n",
            "epoch 38\n",
            " batch Loss train: 0.06641135364770889\n",
            "i 416\n",
            "epoch 38\n",
            " batch Loss train: 0.07318063825368881\n",
            "i 417\n",
            "epoch 38\n",
            " batch Loss train: 0.077389195561409\n",
            "i 418\n",
            "epoch 38\n",
            " batch Loss train: 0.0643681138753891\n",
            "i 419\n",
            "epoch 38\n",
            " batch Loss train: 0.09380412846803665\n",
            "i 420\n",
            "epoch 38\n",
            " batch Loss train: 0.06834821403026581\n",
            "i 421\n",
            "epoch 38\n",
            " batch Loss train: 0.08327070623636246\n",
            "i 422\n",
            "epoch 38\n",
            " batch Loss train: 0.07192137837409973\n",
            "i 423\n",
            "epoch 38\n",
            " batch Loss train: 0.09494490921497345\n",
            "i 424\n",
            "epoch 38\n",
            " batch Loss train: 0.0676189586520195\n",
            "i 425\n",
            "epoch 38\n",
            " batch Loss train: 0.06911614537239075\n",
            "i 426\n",
            "epoch 38\n",
            " batch Loss train: 0.09163501113653183\n",
            "i 427\n",
            "epoch 38\n",
            " batch Loss train: 0.06117027625441551\n",
            "i 428\n",
            "epoch 38\n",
            " batch Loss train: 0.07558850198984146\n",
            "i 429\n",
            "epoch 38\n",
            " batch Loss train: 0.058222364634275436\n",
            "i 430\n",
            "epoch 38\n",
            " batch Loss train: 0.0526249073445797\n",
            "i 431\n",
            "epoch 38\n",
            " batch Loss train: 0.07900096476078033\n",
            "i 432\n",
            "epoch 38\n",
            " batch Loss train: 0.08369344472885132\n",
            "i 433\n",
            "epoch 38\n",
            " batch Loss train: 0.08679798990488052\n",
            "i 434\n",
            "epoch 38\n",
            " batch Loss train: 0.09037893265485764\n",
            "i 435\n",
            "epoch 38\n",
            " batch Loss train: 0.08969990909099579\n",
            "i 436\n",
            "epoch 38\n",
            " batch Loss train: 0.08411135524511337\n",
            "i 437\n",
            "epoch 38\n",
            " batch Loss train: 0.05297461152076721\n",
            "i 438\n",
            "epoch 38\n",
            " batch Loss train: 0.06298793852329254\n",
            "i 439\n",
            "epoch 38\n",
            " batch Loss train: 0.07681804150342941\n",
            "i 440\n",
            "epoch 38\n",
            " batch Loss train: 0.08428680896759033\n",
            "i 441\n",
            "epoch 38\n",
            " batch Loss train: 0.07479459047317505\n",
            "i 442\n",
            "epoch 38\n",
            " batch Loss train: 0.09907545894384384\n",
            "i 443\n",
            "epoch 38\n",
            " batch Loss train: 0.07531057298183441\n",
            "i 444\n",
            "epoch 38\n",
            " batch Loss train: 0.06983690708875656\n",
            "i 445\n",
            "epoch 38\n",
            " batch Loss train: 0.10809633880853653\n",
            "total epoch Loss train: tensor(0.1081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 39\n",
            " batch Loss train: 0.07712075114250183\n",
            "i 1\n",
            "epoch 39\n",
            " batch Loss train: 0.05325328931212425\n",
            "i 2\n",
            "epoch 39\n",
            " batch Loss train: 0.06902442127466202\n",
            "i 3\n",
            "epoch 39\n",
            " batch Loss train: 0.08249296247959137\n",
            "i 4\n",
            "epoch 39\n",
            " batch Loss train: 0.060021523386240005\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 39\n",
            " batch Loss train: 0.05855626240372658\n",
            "i 6\n",
            "epoch 39\n",
            " batch Loss train: 0.06373461335897446\n",
            "i 7\n",
            "epoch 39\n",
            " batch Loss train: 0.0649111270904541\n",
            "i 8\n",
            "epoch 39\n",
            " batch Loss train: 0.07198148220777512\n",
            "i 9\n",
            "epoch 39\n",
            " batch Loss train: 0.05523471161723137\n",
            "i 10\n",
            "epoch 39\n",
            " batch Loss train: 0.055424198508262634\n",
            "i 11\n",
            "epoch 39\n",
            " batch Loss train: 0.06026478484272957\n",
            "i 12\n",
            "epoch 39\n",
            " batch Loss train: 0.07245011627674103\n",
            "i 13\n",
            "epoch 39\n",
            " batch Loss train: 0.06089167669415474\n",
            "i 14\n",
            "epoch 39\n",
            " batch Loss train: 0.06403791904449463\n",
            "i 15\n",
            "epoch 39\n",
            " batch Loss train: 0.06071283668279648\n",
            "i 16\n",
            "epoch 39\n",
            " batch Loss train: 0.07083941251039505\n",
            "i 17\n",
            "epoch 39\n",
            " batch Loss train: 0.05975694581866264\n",
            "i 18\n",
            "epoch 39\n",
            " batch Loss train: 0.07666882127523422\n",
            "i 19\n",
            "epoch 39\n",
            " batch Loss train: 0.09649235755205154\n",
            "i 20\n",
            "epoch 39\n",
            " batch Loss train: 0.0557711087167263\n",
            "i 21\n",
            "epoch 39\n",
            " batch Loss train: 0.05655496194958687\n",
            "i 22\n",
            "epoch 39\n",
            " batch Loss train: 0.07201242446899414\n",
            "i 23\n",
            "epoch 39\n",
            " batch Loss train: 0.060094673186540604\n",
            "i 24\n",
            "epoch 39\n",
            " batch Loss train: 0.06765422224998474\n",
            "i 25\n",
            "epoch 39\n",
            " batch Loss train: 0.05649426952004433\n",
            "i 26\n",
            "epoch 39\n",
            " batch Loss train: 0.07104761898517609\n",
            "i 27\n",
            "epoch 39\n",
            " batch Loss train: 0.07480227947235107\n",
            "i 28\n",
            "epoch 39\n",
            " batch Loss train: 0.0784960463643074\n",
            "i 29\n",
            "epoch 39\n",
            " batch Loss train: 0.055502716451883316\n",
            "i 30\n",
            "epoch 39\n",
            " batch Loss train: 0.05994091182947159\n",
            "i 31\n",
            "epoch 39\n",
            " batch Loss train: 0.06010498106479645\n",
            "i 32\n",
            "epoch 39\n",
            " batch Loss train: 0.06585239619016647\n",
            "i 33\n",
            "epoch 39\n",
            " batch Loss train: 0.05973166599869728\n",
            "i 34\n",
            "epoch 39\n",
            " batch Loss train: 0.05642781779170036\n",
            "i 35\n",
            "epoch 39\n",
            " batch Loss train: 0.05820373445749283\n",
            "i 36\n",
            "epoch 39\n",
            " batch Loss train: 0.05708952620625496\n",
            "i 37\n",
            "epoch 39\n",
            " batch Loss train: 0.06241040304303169\n",
            "i 38\n",
            "epoch 39\n",
            " batch Loss train: 0.07083591818809509\n",
            "i 39\n",
            "epoch 39\n",
            " batch Loss train: 0.07604854553937912\n",
            "i 40\n",
            "epoch 39\n",
            " batch Loss train: 0.06212741509079933\n",
            "i 41\n",
            "epoch 39\n",
            " batch Loss train: 0.053426485508680344\n",
            "i 42\n",
            "epoch 39\n",
            " batch Loss train: 0.05521261319518089\n",
            "i 43\n",
            "epoch 39\n",
            " batch Loss train: 0.0595310777425766\n",
            "i 44\n",
            "epoch 39\n",
            " batch Loss train: 0.0566243901848793\n",
            "i 45\n",
            "epoch 39\n",
            " batch Loss train: 0.07790996134281158\n",
            "i 46\n",
            "epoch 39\n",
            " batch Loss train: 0.0618772879242897\n",
            "i 47\n",
            "epoch 39\n",
            " batch Loss train: 0.07278287410736084\n",
            "i 48\n",
            "epoch 39\n",
            " batch Loss train: 0.07411480695009232\n",
            "i 49\n",
            "epoch 39\n",
            " batch Loss train: 0.06395459920167923\n",
            "i 50\n",
            "epoch 39\n",
            " batch Loss train: 0.0682118609547615\n",
            "i 51\n",
            "epoch 39\n",
            " batch Loss train: 0.07408235967159271\n",
            "i 52\n",
            "epoch 39\n",
            " batch Loss train: 0.06808000802993774\n",
            "i 53\n",
            "epoch 39\n",
            " batch Loss train: 0.06480512768030167\n",
            "i 54\n",
            "epoch 39\n",
            " batch Loss train: 0.05418768897652626\n",
            "i 55\n",
            "epoch 39\n",
            " batch Loss train: 0.06997257471084595\n",
            "i 56\n",
            "epoch 39\n",
            " batch Loss train: 0.07881058752536774\n",
            "i 57\n",
            "epoch 39\n",
            " batch Loss train: 0.06554196774959564\n",
            "i 58\n",
            "epoch 39\n",
            " batch Loss train: 0.0702725276350975\n",
            "i 59\n",
            "epoch 39\n",
            " batch Loss train: 0.06413670629262924\n",
            "i 60\n",
            "epoch 39\n",
            " batch Loss train: 0.06354862451553345\n",
            "i 61\n",
            "epoch 39\n",
            " batch Loss train: 0.059998828917741776\n",
            "i 62\n",
            "epoch 39\n",
            " batch Loss train: 0.061039526015520096\n",
            "i 63\n",
            "epoch 39\n",
            " batch Loss train: 0.04610762372612953\n",
            "i 64\n",
            "epoch 39\n",
            " batch Loss train: 0.06571710854768753\n",
            "i 65\n",
            "epoch 39\n",
            " batch Loss train: 0.07149386405944824\n",
            "i 66\n",
            "epoch 39\n",
            " batch Loss train: 0.0533839650452137\n",
            "i 67\n",
            "epoch 39\n",
            " batch Loss train: 0.06123238429427147\n",
            "i 68\n",
            "epoch 39\n",
            " batch Loss train: 0.07814471423625946\n",
            "i 69\n",
            "epoch 39\n",
            " batch Loss train: 0.0639728382229805\n",
            "i 70\n",
            "epoch 39\n",
            " batch Loss train: 0.05639544501900673\n",
            "i 71\n",
            "epoch 39\n",
            " batch Loss train: 0.0724261924624443\n",
            "i 72\n",
            "epoch 39\n",
            " batch Loss train: 0.09897911548614502\n",
            "i 73\n",
            "epoch 39\n",
            " batch Loss train: 0.06662856042385101\n",
            "i 74\n",
            "epoch 39\n",
            " batch Loss train: 0.0718747153878212\n",
            "i 75\n",
            "epoch 39\n",
            " batch Loss train: 0.082800954580307\n",
            "i 76\n",
            "epoch 39\n",
            " batch Loss train: 0.05362612009048462\n",
            "i 77\n",
            "epoch 39\n",
            " batch Loss train: 0.07136958092451096\n",
            "i 78\n",
            "epoch 39\n",
            " batch Loss train: 0.08279925584793091\n",
            "i 79\n",
            "epoch 39\n",
            " batch Loss train: 0.0763883888721466\n",
            "i 80\n",
            "epoch 39\n",
            " batch Loss train: 0.05324305593967438\n",
            "i 81\n",
            "epoch 39\n",
            " batch Loss train: 0.07314060628414154\n",
            "i 82\n",
            "epoch 39\n",
            " batch Loss train: 0.05926134064793587\n",
            "i 83\n",
            "epoch 39\n",
            " batch Loss train: 0.08091951906681061\n",
            "i 84\n",
            "epoch 39\n",
            " batch Loss train: 0.07368795573711395\n",
            "i 85\n",
            "epoch 39\n",
            " batch Loss train: 0.07167544215917587\n",
            "i 86\n",
            "epoch 39\n",
            " batch Loss train: 0.06825824826955795\n",
            "i 87\n",
            "epoch 39\n",
            " batch Loss train: 0.049010418355464935\n",
            "i 88\n",
            "epoch 39\n",
            " batch Loss train: 0.0682930201292038\n",
            "i 89\n",
            "epoch 39\n",
            " batch Loss train: 0.05956519395112991\n",
            "i 90\n",
            "epoch 39\n",
            " batch Loss train: 0.08305977284908295\n",
            "i 91\n",
            "epoch 39\n",
            " batch Loss train: 0.052876025438308716\n",
            "i 92\n",
            "epoch 39\n",
            " batch Loss train: 0.06036961451172829\n",
            "i 93\n",
            "epoch 39\n",
            " batch Loss train: 0.08221639692783356\n",
            "i 94\n",
            "epoch 39\n",
            " batch Loss train: 0.06331887096166611\n",
            "i 95\n",
            "epoch 39\n",
            " batch Loss train: 0.06861765682697296\n",
            "i 96\n",
            "epoch 39\n",
            " batch Loss train: 0.07020682096481323\n",
            "i 97\n",
            "epoch 39\n",
            " batch Loss train: 0.07834554463624954\n",
            "i 98\n",
            "epoch 39\n",
            " batch Loss train: 0.06559432297945023\n",
            "i 99\n",
            "epoch 39\n",
            " batch Loss train: 0.07994946092367172\n",
            "i 100\n",
            "epoch 39\n",
            " batch Loss train: 0.0620247982442379\n",
            "i 101\n",
            "epoch 39\n",
            " batch Loss train: 0.06149264797568321\n",
            "i 102\n",
            "epoch 39\n",
            " batch Loss train: 0.060256607830524445\n",
            "i 103\n",
            "epoch 39\n",
            " batch Loss train: 0.08204204589128494\n",
            "i 104\n",
            "epoch 39\n",
            " batch Loss train: 0.05614930018782616\n",
            "i 105\n",
            "epoch 39\n",
            " batch Loss train: 0.04522353783249855\n",
            "i 106\n",
            "epoch 39\n",
            " batch Loss train: 0.07214704900979996\n",
            "i 107\n",
            "epoch 39\n",
            " batch Loss train: 0.07913871109485626\n",
            "i 108\n",
            "epoch 39\n",
            " batch Loss train: 0.055451422929763794\n",
            "i 109\n",
            "epoch 39\n",
            " batch Loss train: 0.057452939450740814\n",
            "i 110\n",
            "epoch 39\n",
            " batch Loss train: 0.054937008768320084\n",
            "i 111\n",
            "epoch 39\n",
            " batch Loss train: 0.05024782195687294\n",
            "i 112\n",
            "epoch 39\n",
            " batch Loss train: 0.07145583629608154\n",
            "i 113\n",
            "epoch 39\n",
            " batch Loss train: 0.06495591998100281\n",
            "i 114\n",
            "epoch 39\n",
            " batch Loss train: 0.05302707105875015\n",
            "i 115\n",
            "epoch 39\n",
            " batch Loss train: 0.08257599920034409\n",
            "i 116\n",
            "epoch 39\n",
            " batch Loss train: 0.08496424555778503\n",
            "i 117\n",
            "epoch 39\n",
            " batch Loss train: 0.06630143523216248\n",
            "i 118\n",
            "epoch 39\n",
            " batch Loss train: 0.05579845607280731\n",
            "i 119\n",
            "epoch 39\n",
            " batch Loss train: 0.08584105968475342\n",
            "i 120\n",
            "epoch 39\n",
            " batch Loss train: 0.06673446297645569\n",
            "i 121\n",
            "epoch 39\n",
            " batch Loss train: 0.06265772134065628\n",
            "i 122\n",
            "epoch 39\n",
            " batch Loss train: 0.07110721617937088\n",
            "i 123\n",
            "epoch 39\n",
            " batch Loss train: 0.0511857345700264\n",
            "i 124\n",
            "epoch 39\n",
            " batch Loss train: 0.058320119976997375\n",
            "i 125\n",
            "epoch 39\n",
            " batch Loss train: 0.06972942501306534\n",
            "i 126\n",
            "epoch 39\n",
            " batch Loss train: 0.08453839272260666\n",
            "i 127\n",
            "epoch 39\n",
            " batch Loss train: 0.0800192728638649\n",
            "i 128\n",
            "epoch 39\n",
            " batch Loss train: 0.06545477360486984\n",
            "i 129\n",
            "epoch 39\n",
            " batch Loss train: 0.0561835840344429\n",
            "i 130\n",
            "epoch 39\n",
            " batch Loss train: 0.06459411978721619\n",
            "i 131\n",
            "epoch 39\n",
            " batch Loss train: 0.0624866746366024\n",
            "i 132\n",
            "epoch 39\n",
            " batch Loss train: 0.056788232177495956\n",
            "i 133\n",
            "epoch 39\n",
            " batch Loss train: 0.05074337497353554\n",
            "i 134\n",
            "epoch 39\n",
            " batch Loss train: 0.05874437466263771\n",
            "i 135\n",
            "epoch 39\n",
            " batch Loss train: 0.06527488678693771\n",
            "i 136\n",
            "epoch 39\n",
            " batch Loss train: 0.05499958619475365\n",
            "i 137\n",
            "epoch 39\n",
            " batch Loss train: 0.052596818655729294\n",
            "i 138\n",
            "epoch 39\n",
            " batch Loss train: 0.0708407536149025\n",
            "i 139\n",
            "epoch 39\n",
            " batch Loss train: 0.0619756244122982\n",
            "i 140\n",
            "epoch 39\n",
            " batch Loss train: 0.05316248908638954\n",
            "i 141\n",
            "epoch 39\n",
            " batch Loss train: 0.0506034791469574\n",
            "i 142\n",
            "epoch 39\n",
            " batch Loss train: 0.054607026278972626\n",
            "i 143\n",
            "epoch 39\n",
            " batch Loss train: 0.05650174245238304\n",
            "i 144\n",
            "epoch 39\n",
            " batch Loss train: 0.06396729499101639\n",
            "i 145\n",
            "epoch 39\n",
            " batch Loss train: 0.07720965147018433\n",
            "i 146\n",
            "epoch 39\n",
            " batch Loss train: 0.06998266279697418\n",
            "i 147\n",
            "epoch 39\n",
            " batch Loss train: 0.07555679231882095\n",
            "i 148\n",
            "epoch 39\n",
            " batch Loss train: 0.053603336215019226\n",
            "i 149\n",
            "epoch 39\n",
            " batch Loss train: 0.07449954003095627\n",
            "i 150\n",
            "epoch 39\n",
            " batch Loss train: 0.05131951719522476\n",
            "i 151\n",
            "epoch 39\n",
            " batch Loss train: 0.07791684567928314\n",
            "i 152\n",
            "epoch 39\n",
            " batch Loss train: 0.061736755073070526\n",
            "i 153\n",
            "epoch 39\n",
            " batch Loss train: 0.07161054015159607\n",
            "i 154\n",
            "epoch 39\n",
            " batch Loss train: 0.05502786859869957\n",
            "i 155\n",
            "epoch 39\n",
            " batch Loss train: 0.057183995842933655\n",
            "i 156\n",
            "epoch 39\n",
            " batch Loss train: 0.05944966897368431\n",
            "i 157\n",
            "epoch 39\n",
            " batch Loss train: 0.07600881159305573\n",
            "i 158\n",
            "epoch 39\n",
            " batch Loss train: 0.06761997193098068\n",
            "i 159\n",
            "epoch 39\n",
            " batch Loss train: 0.06107903644442558\n",
            "i 160\n",
            "epoch 39\n",
            " batch Loss train: 0.07789906859397888\n",
            "i 161\n",
            "epoch 39\n",
            " batch Loss train: 0.05835607647895813\n",
            "i 162\n",
            "epoch 39\n",
            " batch Loss train: 0.07393954694271088\n",
            "i 163\n",
            "epoch 39\n",
            " batch Loss train: 0.06877898424863815\n",
            "i 164\n",
            "epoch 39\n",
            " batch Loss train: 0.07285875827074051\n",
            "i 165\n",
            "epoch 39\n",
            " batch Loss train: 0.04760027676820755\n",
            "i 166\n",
            "epoch 39\n",
            " batch Loss train: 0.06813932955265045\n",
            "i 167\n",
            "epoch 39\n",
            " batch Loss train: 0.06517058610916138\n",
            "i 168\n",
            "epoch 39\n",
            " batch Loss train: 0.06621412932872772\n",
            "i 169\n",
            "epoch 39\n",
            " batch Loss train: 0.08777356892824173\n",
            "i 170\n",
            "epoch 39\n",
            " batch Loss train: 0.07867328077554703\n",
            "i 171\n",
            "epoch 39\n",
            " batch Loss train: 0.05086573585867882\n",
            "i 172\n",
            "epoch 39\n",
            " batch Loss train: 0.06516829133033752\n",
            "i 173\n",
            "epoch 39\n",
            " batch Loss train: 0.0750151127576828\n",
            "i 174\n",
            "epoch 39\n",
            " batch Loss train: 0.058697983622550964\n",
            "i 175\n",
            "epoch 39\n",
            " batch Loss train: 0.05304752662777901\n",
            "i 176\n",
            "epoch 39\n",
            " batch Loss train: 0.07519084960222244\n",
            "i 177\n",
            "epoch 39\n",
            " batch Loss train: 0.06169026345014572\n",
            "i 178\n",
            "epoch 39\n",
            " batch Loss train: 0.05940351262688637\n",
            "i 179\n",
            "epoch 39\n",
            " batch Loss train: 0.06446409970521927\n",
            "i 180\n",
            "epoch 39\n",
            " batch Loss train: 0.0592057965695858\n",
            "i 181\n",
            "epoch 39\n",
            " batch Loss train: 0.05927400290966034\n",
            "i 182\n",
            "epoch 39\n",
            " batch Loss train: 0.06951167434453964\n",
            "i 183\n",
            "epoch 39\n",
            " batch Loss train: 0.06010504439473152\n",
            "i 184\n",
            "epoch 39\n",
            " batch Loss train: 0.06148273125290871\n",
            "i 185\n",
            "epoch 39\n",
            " batch Loss train: 0.059488486498594284\n",
            "i 186\n",
            "epoch 39\n",
            " batch Loss train: 0.05099708214402199\n",
            "i 187\n",
            "epoch 39\n",
            " batch Loss train: 0.04720132425427437\n",
            "i 188\n",
            "epoch 39\n",
            " batch Loss train: 0.06956788897514343\n",
            "i 189\n",
            "epoch 39\n",
            " batch Loss train: 0.06583927571773529\n",
            "i 190\n",
            "epoch 39\n",
            " batch Loss train: 0.07208429276943207\n",
            "i 191\n",
            "epoch 39\n",
            " batch Loss train: 0.07521689683198929\n",
            "i 192\n",
            "epoch 39\n",
            " batch Loss train: 0.0695100948214531\n",
            "i 193\n",
            "epoch 39\n",
            " batch Loss train: 0.056863270699977875\n",
            "i 194\n",
            "epoch 39\n",
            " batch Loss train: 0.08760926872491837\n",
            "i 195\n",
            "epoch 39\n",
            " batch Loss train: 0.06799300760030746\n",
            "i 196\n",
            "epoch 39\n",
            " batch Loss train: 0.10305577516555786\n",
            "i 197\n",
            "epoch 39\n",
            " batch Loss train: 0.06979124993085861\n",
            "i 198\n",
            "epoch 39\n",
            " batch Loss train: 0.05370795726776123\n",
            "i 199\n",
            "epoch 39\n",
            " batch Loss train: 0.0730937197804451\n",
            "i 200\n",
            "epoch 39\n",
            " batch Loss train: 0.060307908803224564\n",
            "i 201\n",
            "epoch 39\n",
            " batch Loss train: 0.058981046080589294\n",
            "i 202\n",
            "epoch 39\n",
            " batch Loss train: 0.06398919969797134\n",
            "i 203\n",
            "epoch 39\n",
            " batch Loss train: 0.05553565174341202\n",
            "i 204\n",
            "epoch 39\n",
            " batch Loss train: 0.06875012814998627\n",
            "i 205\n",
            "epoch 39\n",
            " batch Loss train: 0.06398548930883408\n",
            "i 206\n",
            "epoch 39\n",
            " batch Loss train: 0.06721557676792145\n",
            "i 207\n",
            "epoch 39\n",
            " batch Loss train: 0.07227238267660141\n",
            "i 208\n",
            "epoch 39\n",
            " batch Loss train: 0.07206163555383682\n",
            "i 209\n",
            "epoch 39\n",
            " batch Loss train: 0.06760219484567642\n",
            "i 210\n",
            "epoch 39\n",
            " batch Loss train: 0.0653134137392044\n",
            "i 211\n",
            "epoch 39\n",
            " batch Loss train: 0.07439766824245453\n",
            "i 212\n",
            "epoch 39\n",
            " batch Loss train: 0.057682037353515625\n",
            "i 213\n",
            "epoch 39\n",
            " batch Loss train: 0.06927001476287842\n",
            "i 214\n",
            "epoch 39\n",
            " batch Loss train: 0.061218369752168655\n",
            "i 215\n",
            "epoch 39\n",
            " batch Loss train: 0.0634678304195404\n",
            "i 216\n",
            "epoch 39\n",
            " batch Loss train: 0.05552784726023674\n",
            "i 217\n",
            "epoch 39\n",
            " batch Loss train: 0.05798213556408882\n",
            "i 218\n",
            "epoch 39\n",
            " batch Loss train: 0.07501077651977539\n",
            "i 219\n",
            "epoch 39\n",
            " batch Loss train: 0.0849829614162445\n",
            "i 220\n",
            "epoch 39\n",
            " batch Loss train: 0.06670284271240234\n",
            "i 221\n",
            "epoch 39\n",
            " batch Loss train: 0.07932053506374359\n",
            "i 222\n",
            "epoch 39\n",
            " batch Loss train: 0.06458313763141632\n",
            "i 223\n",
            "epoch 39\n",
            " batch Loss train: 0.04915372282266617\n",
            "i 224\n",
            "epoch 39\n",
            " batch Loss train: 0.09764877706766129\n",
            "i 225\n",
            "epoch 39\n",
            " batch Loss train: 0.08136549592018127\n",
            "i 226\n",
            "epoch 39\n",
            " batch Loss train: 0.05386750400066376\n",
            "i 227\n",
            "epoch 39\n",
            " batch Loss train: 0.07860972732305527\n",
            "i 228\n",
            "epoch 39\n",
            " batch Loss train: 0.06230440363287926\n",
            "i 229\n",
            "epoch 39\n",
            " batch Loss train: 0.07242117822170258\n",
            "i 230\n",
            "epoch 39\n",
            " batch Loss train: 0.07630344480276108\n",
            "i 231\n",
            "epoch 39\n",
            " batch Loss train: 0.08152719587087631\n",
            "i 232\n",
            "epoch 39\n",
            " batch Loss train: 0.08179817348718643\n",
            "i 233\n",
            "epoch 39\n",
            " batch Loss train: 0.05035759508609772\n",
            "i 234\n",
            "epoch 39\n",
            " batch Loss train: 0.06173344701528549\n",
            "i 235\n",
            "epoch 39\n",
            " batch Loss train: 0.09720367193222046\n",
            "i 236\n",
            "epoch 39\n",
            " batch Loss train: 0.07132451981306076\n",
            "i 237\n",
            "epoch 39\n",
            " batch Loss train: 0.07579115778207779\n",
            "i 238\n",
            "epoch 39\n",
            " batch Loss train: 0.06054487079381943\n",
            "i 239\n",
            "epoch 39\n",
            " batch Loss train: 0.07531847059726715\n",
            "i 240\n",
            "epoch 39\n",
            " batch Loss train: 0.08329033851623535\n",
            "i 241\n",
            "epoch 39\n",
            " batch Loss train: 0.06311873346567154\n",
            "i 242\n",
            "epoch 39\n",
            " batch Loss train: 0.0579688623547554\n",
            "i 243\n",
            "epoch 39\n",
            " batch Loss train: 0.060834918171167374\n",
            "i 244\n",
            "epoch 39\n",
            " batch Loss train: 0.0696108266711235\n",
            "i 245\n",
            "epoch 39\n",
            " batch Loss train: 0.07210434973239899\n",
            "i 246\n",
            "epoch 39\n",
            " batch Loss train: 0.07094775140285492\n",
            "i 247\n",
            "epoch 39\n",
            " batch Loss train: 0.07334457337856293\n",
            "i 248\n",
            "epoch 39\n",
            " batch Loss train: 0.07843052595853806\n",
            "i 249\n",
            "epoch 39\n",
            " batch Loss train: 0.0624600350856781\n",
            "i 250\n",
            "epoch 39\n",
            " batch Loss train: 0.05512954294681549\n",
            "i 251\n",
            "epoch 39\n",
            " batch Loss train: 0.06145034357905388\n",
            "i 252\n",
            "epoch 39\n",
            " batch Loss train: 0.06997065991163254\n",
            "i 253\n",
            "epoch 39\n",
            " batch Loss train: 0.07394886761903763\n",
            "i 254\n",
            "epoch 39\n",
            " batch Loss train: 0.06718490272760391\n",
            "i 255\n",
            "epoch 39\n",
            " batch Loss train: 0.08409170806407928\n",
            "i 256\n",
            "epoch 39\n",
            " batch Loss train: 0.05837409570813179\n",
            "i 257\n",
            "epoch 39\n",
            " batch Loss train: 0.053335122764110565\n",
            "i 258\n",
            "epoch 39\n",
            " batch Loss train: 0.08044809103012085\n",
            "i 259\n",
            "epoch 39\n",
            " batch Loss train: 0.0645396038889885\n",
            "i 260\n",
            "epoch 39\n",
            " batch Loss train: 0.07663554698228836\n",
            "i 261\n",
            "epoch 39\n",
            " batch Loss train: 0.07778042554855347\n",
            "i 262\n",
            "epoch 39\n",
            " batch Loss train: 0.07109776884317398\n",
            "i 263\n",
            "epoch 39\n",
            " batch Loss train: 0.07503737509250641\n",
            "i 264\n",
            "epoch 39\n",
            " batch Loss train: 0.10431936383247375\n",
            "i 265\n",
            "epoch 39\n",
            " batch Loss train: 0.08360863476991653\n",
            "i 266\n",
            "epoch 39\n",
            " batch Loss train: 0.05366353690624237\n",
            "i 267\n",
            "epoch 39\n",
            " batch Loss train: 0.0715576559305191\n",
            "i 268\n",
            "epoch 39\n",
            " batch Loss train: 0.08805488795042038\n",
            "i 269\n",
            "epoch 39\n",
            " batch Loss train: 0.06675338745117188\n",
            "i 270\n",
            "epoch 39\n",
            " batch Loss train: 0.08843459188938141\n",
            "i 271\n",
            "epoch 39\n",
            " batch Loss train: 0.07172740250825882\n",
            "i 272\n",
            "epoch 39\n",
            " batch Loss train: 0.0668741837143898\n",
            "i 273\n",
            "epoch 39\n",
            " batch Loss train: 0.06450463831424713\n",
            "i 274\n",
            "epoch 39\n",
            " batch Loss train: 0.05940251797437668\n",
            "i 275\n",
            "epoch 39\n",
            " batch Loss train: 0.08225003629922867\n",
            "i 276\n",
            "epoch 39\n",
            " batch Loss train: 0.07146325707435608\n",
            "i 277\n",
            "epoch 39\n",
            " batch Loss train: 0.08628997206687927\n",
            "i 278\n",
            "epoch 39\n",
            " batch Loss train: 0.061974380165338516\n",
            "i 279\n",
            "epoch 39\n",
            " batch Loss train: 0.0779978558421135\n",
            "i 280\n",
            "epoch 39\n",
            " batch Loss train: 0.04845703765749931\n",
            "i 281\n",
            "epoch 39\n",
            " batch Loss train: 0.08160079270601273\n",
            "i 282\n",
            "epoch 39\n",
            " batch Loss train: 0.05079784244298935\n",
            "i 283\n",
            "epoch 39\n",
            " batch Loss train: 0.04932805895805359\n",
            "i 284\n",
            "epoch 39\n",
            " batch Loss train: 0.08701927959918976\n",
            "i 285\n",
            "epoch 39\n",
            " batch Loss train: 0.07942027598619461\n",
            "i 286\n",
            "epoch 39\n",
            " batch Loss train: 0.05652957037091255\n",
            "i 287\n",
            "epoch 39\n",
            " batch Loss train: 0.07182055711746216\n",
            "i 288\n",
            "epoch 39\n",
            " batch Loss train: 0.11949101090431213\n",
            "i 289\n",
            "epoch 39\n",
            " batch Loss train: 0.06531473994255066\n",
            "i 290\n",
            "epoch 39\n",
            " batch Loss train: 0.07499393820762634\n",
            "i 291\n",
            "epoch 39\n",
            " batch Loss train: 0.06982537358999252\n",
            "i 292\n",
            "epoch 39\n",
            " batch Loss train: 0.07600583881139755\n",
            "i 293\n",
            "epoch 39\n",
            " batch Loss train: 0.08903467655181885\n",
            "i 294\n",
            "epoch 39\n",
            " batch Loss train: 0.07288108766078949\n",
            "i 295\n",
            "epoch 39\n",
            " batch Loss train: 0.06479315459728241\n",
            "i 296\n",
            "epoch 39\n",
            " batch Loss train: 0.07655445486307144\n",
            "i 297\n",
            "epoch 39\n",
            " batch Loss train: 0.05694832280278206\n",
            "i 298\n",
            "epoch 39\n",
            " batch Loss train: 0.07222094386816025\n",
            "i 299\n",
            "epoch 39\n",
            " batch Loss train: 0.05694311484694481\n",
            "i 300\n",
            "epoch 39\n",
            " batch Loss train: 0.05913136154413223\n",
            "i 301\n",
            "epoch 39\n",
            " batch Loss train: 0.060817744582891464\n",
            "i 302\n",
            "epoch 39\n",
            " batch Loss train: 0.08096558600664139\n",
            "i 303\n",
            "epoch 39\n",
            " batch Loss train: 0.07278242707252502\n",
            "i 304\n",
            "epoch 39\n",
            " batch Loss train: 0.07227854430675507\n",
            "i 305\n",
            "epoch 39\n",
            " batch Loss train: 0.0471017025411129\n",
            "i 306\n",
            "epoch 39\n",
            " batch Loss train: 0.06970150023698807\n",
            "i 307\n",
            "epoch 39\n",
            " batch Loss train: 0.0800265297293663\n",
            "i 308\n",
            "epoch 39\n",
            " batch Loss train: 0.09828928858041763\n",
            "i 309\n",
            "epoch 39\n",
            " batch Loss train: 0.09585898369550705\n",
            "i 310\n",
            "epoch 39\n",
            " batch Loss train: 0.06343834847211838\n",
            "i 311\n",
            "epoch 39\n",
            " batch Loss train: 0.06377256661653519\n",
            "i 312\n",
            "epoch 39\n",
            " batch Loss train: 0.06815645843744278\n",
            "i 313\n",
            "epoch 39\n",
            " batch Loss train: 0.08382881432771683\n",
            "i 314\n",
            "epoch 39\n",
            " batch Loss train: 0.07294604182243347\n",
            "i 315\n",
            "epoch 39\n",
            " batch Loss train: 0.06761221587657928\n",
            "i 316\n",
            "epoch 39\n",
            " batch Loss train: 0.0570950023829937\n",
            "i 317\n",
            "epoch 39\n",
            " batch Loss train: 0.06331843137741089\n",
            "i 318\n",
            "epoch 39\n",
            " batch Loss train: 0.05745274946093559\n",
            "i 319\n",
            "epoch 39\n",
            " batch Loss train: 0.08718050271272659\n",
            "i 320\n",
            "epoch 39\n",
            " batch Loss train: 0.06311143934726715\n",
            "i 321\n",
            "epoch 39\n",
            " batch Loss train: 0.12708133459091187\n",
            "i 322\n",
            "epoch 39\n",
            " batch Loss train: 0.0724603533744812\n",
            "i 323\n",
            "epoch 39\n",
            " batch Loss train: 0.06576644629240036\n",
            "i 324\n",
            "epoch 39\n",
            " batch Loss train: 0.055549103766679764\n",
            "i 325\n",
            "epoch 39\n",
            " batch Loss train: 0.08266965299844742\n",
            "i 326\n",
            "epoch 39\n",
            " batch Loss train: 0.05375521257519722\n",
            "i 327\n",
            "epoch 39\n",
            " batch Loss train: 0.0822213813662529\n",
            "i 328\n",
            "epoch 39\n",
            " batch Loss train: 0.06033625826239586\n",
            "i 329\n",
            "epoch 39\n",
            " batch Loss train: 0.07066671550273895\n",
            "i 330\n",
            "epoch 39\n",
            " batch Loss train: 0.08888839930295944\n",
            "i 331\n",
            "epoch 39\n",
            " batch Loss train: 0.08419517427682877\n",
            "i 332\n",
            "epoch 39\n",
            " batch Loss train: 0.05968058481812477\n",
            "i 333\n",
            "epoch 39\n",
            " batch Loss train: 0.08417337387800217\n",
            "i 334\n",
            "epoch 39\n",
            " batch Loss train: 0.07760775834321976\n",
            "i 335\n",
            "epoch 39\n",
            " batch Loss train: 0.06253775954246521\n",
            "i 336\n",
            "epoch 39\n",
            " batch Loss train: 0.0692809596657753\n",
            "i 337\n",
            "epoch 39\n",
            " batch Loss train: 0.08188338577747345\n",
            "i 338\n",
            "epoch 39\n",
            " batch Loss train: 0.07981984317302704\n",
            "i 339\n",
            "epoch 39\n",
            " batch Loss train: 0.06874213367700577\n",
            "i 340\n",
            "epoch 39\n",
            " batch Loss train: 0.08952932804822922\n",
            "i 341\n",
            "epoch 39\n",
            " batch Loss train: 0.06710286438465118\n",
            "i 342\n",
            "epoch 39\n",
            " batch Loss train: 0.074846051633358\n",
            "i 343\n",
            "epoch 39\n",
            " batch Loss train: 0.058389656245708466\n",
            "i 344\n",
            "epoch 39\n",
            " batch Loss train: 0.09150149673223495\n",
            "i 345\n",
            "epoch 39\n",
            " batch Loss train: 0.054829925298690796\n",
            "i 346\n",
            "epoch 39\n",
            " batch Loss train: 0.06447760760784149\n",
            "i 347\n",
            "epoch 39\n",
            " batch Loss train: 0.05321907624602318\n",
            "i 348\n",
            "epoch 39\n",
            " batch Loss train: 0.07770867645740509\n",
            "i 349\n",
            "epoch 39\n",
            " batch Loss train: 0.06817381083965302\n",
            "i 350\n",
            "epoch 39\n",
            " batch Loss train: 0.1048983782529831\n",
            "i 351\n",
            "epoch 39\n",
            " batch Loss train: 0.06823403388261795\n",
            "i 352\n",
            "epoch 39\n",
            " batch Loss train: 0.07821807265281677\n",
            "i 353\n",
            "epoch 39\n",
            " batch Loss train: 0.07060306519269943\n",
            "i 354\n",
            "epoch 39\n",
            " batch Loss train: 0.06841956079006195\n",
            "i 355\n",
            "epoch 39\n",
            " batch Loss train: 0.06304178386926651\n",
            "i 356\n",
            "epoch 39\n",
            " batch Loss train: 0.07285074889659882\n",
            "i 357\n",
            "epoch 39\n",
            " batch Loss train: 0.07818271219730377\n",
            "i 358\n",
            "epoch 39\n",
            " batch Loss train: 0.07041633874177933\n",
            "i 359\n",
            "epoch 39\n",
            " batch Loss train: 0.08832070976495743\n",
            "i 360\n",
            "epoch 39\n",
            " batch Loss train: 0.05462868884205818\n",
            "i 361\n",
            "epoch 39\n",
            " batch Loss train: 0.08544909954071045\n",
            "i 362\n",
            "epoch 39\n",
            " batch Loss train: 0.06683295965194702\n",
            "i 363\n",
            "epoch 39\n",
            " batch Loss train: 0.0811031237244606\n",
            "i 364\n",
            "epoch 39\n",
            " batch Loss train: 0.07020612061023712\n",
            "i 365\n",
            "epoch 39\n",
            " batch Loss train: 0.0999993309378624\n",
            "i 366\n",
            "epoch 39\n",
            " batch Loss train: 0.07582145184278488\n",
            "i 367\n",
            "epoch 39\n",
            " batch Loss train: 0.07220805436372757\n",
            "i 368\n",
            "epoch 39\n",
            " batch Loss train: 0.06078273430466652\n",
            "i 369\n",
            "epoch 39\n",
            " batch Loss train: 0.06460196524858475\n",
            "i 370\n",
            "epoch 39\n",
            " batch Loss train: 0.05995646491646767\n",
            "i 371\n",
            "epoch 39\n",
            " batch Loss train: 0.0725896954536438\n",
            "i 372\n",
            "epoch 39\n",
            " batch Loss train: 0.07321902364492416\n",
            "i 373\n",
            "epoch 39\n",
            " batch Loss train: 0.06250444799661636\n",
            "i 374\n",
            "epoch 39\n",
            " batch Loss train: 0.10420146584510803\n",
            "i 375\n",
            "epoch 39\n",
            " batch Loss train: 0.08892621099948883\n",
            "i 376\n",
            "epoch 39\n",
            " batch Loss train: 0.056491777300834656\n",
            "i 377\n",
            "epoch 39\n",
            " batch Loss train: 0.07338260114192963\n",
            "i 378\n",
            "epoch 39\n",
            " batch Loss train: 0.08093860000371933\n",
            "i 379\n",
            "epoch 39\n",
            " batch Loss train: 0.08446738123893738\n",
            "i 380\n",
            "epoch 39\n",
            " batch Loss train: 0.07611583918333054\n",
            "i 381\n",
            "epoch 39\n",
            " batch Loss train: 0.08257642388343811\n",
            "i 382\n",
            "epoch 39\n",
            " batch Loss train: 0.06163579970598221\n",
            "i 383\n",
            "epoch 39\n",
            " batch Loss train: 0.10096917301416397\n",
            "i 384\n",
            "epoch 39\n",
            " batch Loss train: 0.07900171726942062\n",
            "i 385\n",
            "epoch 39\n",
            " batch Loss train: 0.0783873200416565\n",
            "i 386\n",
            "epoch 39\n",
            " batch Loss train: 0.07535495609045029\n",
            "i 387\n",
            "epoch 39\n",
            " batch Loss train: 0.09555286169052124\n",
            "i 388\n",
            "epoch 39\n",
            " batch Loss train: 0.06547711044549942\n",
            "i 389\n",
            "epoch 39\n",
            " batch Loss train: 0.05973348766565323\n",
            "i 390\n",
            "epoch 39\n",
            " batch Loss train: 0.06872528046369553\n",
            "i 391\n",
            "epoch 39\n",
            " batch Loss train: 0.05062459036707878\n",
            "i 392\n",
            "epoch 39\n",
            " batch Loss train: 0.04508424922823906\n",
            "i 393\n",
            "epoch 39\n",
            " batch Loss train: 0.06518068164587021\n",
            "i 394\n",
            "epoch 39\n",
            " batch Loss train: 0.10061199218034744\n",
            "i 395\n",
            "epoch 39\n",
            " batch Loss train: 0.08298124372959137\n",
            "i 396\n",
            "epoch 39\n",
            " batch Loss train: 0.09901174157857895\n",
            "i 397\n",
            "epoch 39\n",
            " batch Loss train: 0.058468375355005264\n",
            "i 398\n",
            "epoch 39\n",
            " batch Loss train: 0.06822755187749863\n",
            "i 399\n",
            "epoch 39\n",
            " batch Loss train: 0.06940095126628876\n",
            "i 400\n",
            "epoch 39\n",
            " batch Loss train: 0.08618905395269394\n",
            "i 401\n",
            "epoch 39\n",
            " batch Loss train: 0.07812170684337616\n",
            "i 402\n",
            "epoch 39\n",
            " batch Loss train: 0.07098855823278427\n",
            "i 403\n",
            "epoch 39\n",
            " batch Loss train: 0.05576138570904732\n",
            "i 404\n",
            "epoch 39\n",
            " batch Loss train: 0.07929868996143341\n",
            "i 405\n",
            "epoch 39\n",
            " batch Loss train: 0.07928501814603806\n",
            "i 406\n",
            "epoch 39\n",
            " batch Loss train: 0.08270944654941559\n",
            "i 407\n",
            "epoch 39\n",
            " batch Loss train: 0.09767437726259232\n",
            "i 408\n",
            "epoch 39\n",
            " batch Loss train: 0.08840331435203552\n",
            "i 409\n",
            "epoch 39\n",
            " batch Loss train: 0.06271469593048096\n",
            "i 410\n",
            "epoch 39\n",
            " batch Loss train: 0.05925548076629639\n",
            "i 411\n",
            "epoch 39\n",
            " batch Loss train: 0.07091283053159714\n",
            "i 412\n",
            "epoch 39\n",
            " batch Loss train: 0.07961290329694748\n",
            "i 413\n",
            "epoch 39\n",
            " batch Loss train: 0.07734860479831696\n",
            "i 414\n",
            "epoch 39\n",
            " batch Loss train: 0.0921848937869072\n",
            "i 415\n",
            "epoch 39\n",
            " batch Loss train: 0.08669076859951019\n",
            "i 416\n",
            "epoch 39\n",
            " batch Loss train: 0.08410000801086426\n",
            "i 417\n",
            "epoch 39\n",
            " batch Loss train: 0.07296188175678253\n",
            "i 418\n",
            "epoch 39\n",
            " batch Loss train: 0.08030477911233902\n",
            "i 419\n",
            "epoch 39\n",
            " batch Loss train: 0.07241938263177872\n",
            "i 420\n",
            "epoch 39\n",
            " batch Loss train: 0.11166181415319443\n",
            "i 421\n",
            "epoch 39\n",
            " batch Loss train: 0.09188681840896606\n",
            "i 422\n",
            "epoch 39\n",
            " batch Loss train: 0.09703870117664337\n",
            "i 423\n",
            "epoch 39\n",
            " batch Loss train: 0.0661732405424118\n",
            "i 424\n",
            "epoch 39\n",
            " batch Loss train: 0.11667522042989731\n",
            "i 425\n",
            "epoch 39\n",
            " batch Loss train: 0.08848688751459122\n",
            "i 426\n",
            "epoch 39\n",
            " batch Loss train: 0.11714938282966614\n",
            "i 427\n",
            "epoch 39\n",
            " batch Loss train: 0.09598124772310257\n",
            "i 428\n",
            "epoch 39\n",
            " batch Loss train: 0.08646351844072342\n",
            "i 429\n",
            "epoch 39\n",
            " batch Loss train: 0.11492624878883362\n",
            "i 430\n",
            "epoch 39\n",
            " batch Loss train: 0.05959096923470497\n",
            "i 431\n",
            "epoch 39\n",
            " batch Loss train: 0.059961993247270584\n",
            "i 432\n",
            "epoch 39\n",
            " batch Loss train: 0.08632459491491318\n",
            "i 433\n",
            "epoch 39\n",
            " batch Loss train: 0.08610829710960388\n",
            "i 434\n",
            "epoch 39\n",
            " batch Loss train: 0.09234338998794556\n",
            "i 435\n",
            "epoch 39\n",
            " batch Loss train: 0.0841219574213028\n",
            "i 436\n",
            "epoch 39\n",
            " batch Loss train: 0.086490198969841\n",
            "i 437\n",
            "epoch 39\n",
            " batch Loss train: 0.10788440704345703\n",
            "i 438\n",
            "epoch 39\n",
            " batch Loss train: 0.07777633517980576\n",
            "i 439\n",
            "epoch 39\n",
            " batch Loss train: 0.08181072771549225\n",
            "i 440\n",
            "epoch 39\n",
            " batch Loss train: 0.07591746002435684\n",
            "i 441\n",
            "epoch 39\n",
            " batch Loss train: 0.1019689291715622\n",
            "i 442\n",
            "epoch 39\n",
            " batch Loss train: 0.08937995135784149\n",
            "i 443\n",
            "epoch 39\n",
            " batch Loss train: 0.07301745563745499\n",
            "i 444\n",
            "epoch 39\n",
            " batch Loss train: 0.0876597985625267\n",
            "i 445\n",
            "epoch 39\n",
            " batch Loss train: 0.09879489243030548\n",
            "total epoch Loss train: tensor(0.0988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 40\n",
            " batch Loss train: 0.05446495860815048\n",
            "i 1\n",
            "epoch 40\n",
            " batch Loss train: 0.061114028096199036\n",
            "i 2\n",
            "epoch 40\n",
            " batch Loss train: 0.10124155879020691\n",
            "i 3\n",
            "epoch 40\n",
            " batch Loss train: 0.05114191398024559\n",
            "i 4\n",
            "epoch 40\n",
            " batch Loss train: 0.04861051216721535\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 40\n",
            " batch Loss train: 0.07136183232069016\n",
            "i 6\n",
            "epoch 40\n",
            " batch Loss train: 0.06673822551965714\n",
            "i 7\n",
            "epoch 40\n",
            " batch Loss train: 0.05402594059705734\n",
            "i 8\n",
            "epoch 40\n",
            " batch Loss train: 0.05087535083293915\n",
            "i 9\n",
            "epoch 40\n",
            " batch Loss train: 0.06748586148023605\n",
            "i 10\n",
            "epoch 40\n",
            " batch Loss train: 0.10614852607250214\n",
            "i 11\n",
            "epoch 40\n",
            " batch Loss train: 0.09151864051818848\n",
            "i 12\n",
            "epoch 40\n",
            " batch Loss train: 0.07207073271274567\n",
            "i 13\n",
            "epoch 40\n",
            " batch Loss train: 0.08137670159339905\n",
            "i 14\n",
            "epoch 40\n",
            " batch Loss train: 0.06605258584022522\n",
            "i 15\n",
            "epoch 40\n",
            " batch Loss train: 0.07579843699932098\n",
            "i 16\n",
            "epoch 40\n",
            " batch Loss train: 0.0726976990699768\n",
            "i 17\n",
            "epoch 40\n",
            " batch Loss train: 0.07679171860218048\n",
            "i 18\n",
            "epoch 40\n",
            " batch Loss train: 0.07046355307102203\n",
            "i 19\n",
            "epoch 40\n",
            " batch Loss train: 0.09585867822170258\n",
            "i 20\n",
            "epoch 40\n",
            " batch Loss train: 0.06549093872308731\n",
            "i 21\n",
            "epoch 40\n",
            " batch Loss train: 0.08940489590167999\n",
            "i 22\n",
            "epoch 40\n",
            " batch Loss train: 0.09122858941555023\n",
            "i 23\n",
            "epoch 40\n",
            " batch Loss train: 0.07538019120693207\n",
            "i 24\n",
            "epoch 40\n",
            " batch Loss train: 0.0692358985543251\n",
            "i 25\n",
            "epoch 40\n",
            " batch Loss train: 0.08765728771686554\n",
            "i 26\n",
            "epoch 40\n",
            " batch Loss train: 0.05696991831064224\n",
            "i 27\n",
            "epoch 40\n",
            " batch Loss train: 0.06589764356613159\n",
            "i 28\n",
            "epoch 40\n",
            " batch Loss train: 0.05594406649470329\n",
            "i 29\n",
            "epoch 40\n",
            " batch Loss train: 0.06160338595509529\n",
            "i 30\n",
            "epoch 40\n",
            " batch Loss train: 0.09228979051113129\n",
            "i 31\n",
            "epoch 40\n",
            " batch Loss train: 0.0846858024597168\n",
            "i 32\n",
            "epoch 40\n",
            " batch Loss train: 0.0884956642985344\n",
            "i 33\n",
            "epoch 40\n",
            " batch Loss train: 0.07430525869131088\n",
            "i 34\n",
            "epoch 40\n",
            " batch Loss train: 0.07274162769317627\n",
            "i 35\n",
            "epoch 40\n",
            " batch Loss train: 0.08884193003177643\n",
            "i 36\n",
            "epoch 40\n",
            " batch Loss train: 0.0708005353808403\n",
            "i 37\n",
            "epoch 40\n",
            " batch Loss train: 0.07732903212308884\n",
            "i 38\n",
            "epoch 40\n",
            " batch Loss train: 0.0802343487739563\n",
            "i 39\n",
            "epoch 40\n",
            " batch Loss train: 0.07083677500486374\n",
            "i 40\n",
            "epoch 40\n",
            " batch Loss train: 0.06320402026176453\n",
            "i 41\n",
            "epoch 40\n",
            " batch Loss train: 0.10070589184761047\n",
            "i 42\n",
            "epoch 40\n",
            " batch Loss train: 0.06787038594484329\n",
            "i 43\n",
            "epoch 40\n",
            " batch Loss train: 0.05970820412039757\n",
            "i 44\n",
            "epoch 40\n",
            " batch Loss train: 0.07920680940151215\n",
            "i 45\n",
            "epoch 40\n",
            " batch Loss train: 0.08231674879789352\n",
            "i 46\n",
            "epoch 40\n",
            " batch Loss train: 0.0750204548239708\n",
            "i 47\n",
            "epoch 40\n",
            " batch Loss train: 0.06647887825965881\n",
            "i 48\n",
            "epoch 40\n",
            " batch Loss train: 0.0791659951210022\n",
            "i 49\n",
            "epoch 40\n",
            " batch Loss train: 0.09492041915655136\n",
            "i 50\n",
            "epoch 40\n",
            " batch Loss train: 0.07459983974695206\n",
            "i 51\n",
            "epoch 40\n",
            " batch Loss train: 0.057819969952106476\n",
            "i 52\n",
            "epoch 40\n",
            " batch Loss train: 0.05484728887677193\n",
            "i 53\n",
            "epoch 40\n",
            " batch Loss train: 0.06727185100317001\n",
            "i 54\n",
            "epoch 40\n",
            " batch Loss train: 0.07427788525819778\n",
            "i 55\n",
            "epoch 40\n",
            " batch Loss train: 0.06296367943286896\n",
            "i 56\n",
            "epoch 40\n",
            " batch Loss train: 0.06350171566009521\n",
            "i 57\n",
            "epoch 40\n",
            " batch Loss train: 0.060043271631002426\n",
            "i 58\n",
            "epoch 40\n",
            " batch Loss train: 0.07041934132575989\n",
            "i 59\n",
            "epoch 40\n",
            " batch Loss train: 0.05040103569626808\n",
            "i 60\n",
            "epoch 40\n",
            " batch Loss train: 0.07914819568395615\n",
            "i 61\n",
            "epoch 40\n",
            " batch Loss train: 0.08278857171535492\n",
            "i 62\n",
            "epoch 40\n",
            " batch Loss train: 0.07396253198385239\n",
            "i 63\n",
            "epoch 40\n",
            " batch Loss train: 0.05719568207859993\n",
            "i 64\n",
            "epoch 40\n",
            " batch Loss train: 0.08736274391412735\n",
            "i 65\n",
            "epoch 40\n",
            " batch Loss train: 0.07149675488471985\n",
            "i 66\n",
            "epoch 40\n",
            " batch Loss train: 0.09664202481508255\n",
            "i 67\n",
            "epoch 40\n",
            " batch Loss train: 0.06351890414953232\n",
            "i 68\n",
            "epoch 40\n",
            " batch Loss train: 0.07630079239606857\n",
            "i 69\n",
            "epoch 40\n",
            " batch Loss train: 0.05363082513213158\n",
            "i 70\n",
            "epoch 40\n",
            " batch Loss train: 0.07228748500347137\n",
            "i 71\n",
            "epoch 40\n",
            " batch Loss train: 0.0683741346001625\n",
            "i 72\n",
            "epoch 40\n",
            " batch Loss train: 0.0874217227101326\n",
            "i 73\n",
            "epoch 40\n",
            " batch Loss train: 0.08612443506717682\n",
            "i 74\n",
            "epoch 40\n",
            " batch Loss train: 0.07261201739311218\n",
            "i 75\n",
            "epoch 40\n",
            " batch Loss train: 0.05480492115020752\n",
            "i 76\n",
            "epoch 40\n",
            " batch Loss train: 0.08596153557300568\n",
            "i 77\n",
            "epoch 40\n",
            " batch Loss train: 0.06297402083873749\n",
            "i 78\n",
            "epoch 40\n",
            " batch Loss train: 0.058068662881851196\n",
            "i 79\n",
            "epoch 40\n",
            " batch Loss train: 0.07418642938137054\n",
            "i 80\n",
            "epoch 40\n",
            " batch Loss train: 0.06574147194623947\n",
            "i 81\n",
            "epoch 40\n",
            " batch Loss train: 0.0646495372056961\n",
            "i 82\n",
            "epoch 40\n",
            " batch Loss train: 0.0630776584148407\n",
            "i 83\n",
            "epoch 40\n",
            " batch Loss train: 0.081036277115345\n",
            "i 84\n",
            "epoch 40\n",
            " batch Loss train: 0.07671469449996948\n",
            "i 85\n",
            "epoch 40\n",
            " batch Loss train: 0.10815981775522232\n",
            "i 86\n",
            "epoch 40\n",
            " batch Loss train: 0.05966092646121979\n",
            "i 87\n",
            "epoch 40\n",
            " batch Loss train: 0.06162983179092407\n",
            "i 88\n",
            "epoch 40\n",
            " batch Loss train: 0.0578492097556591\n",
            "i 89\n",
            "epoch 40\n",
            " batch Loss train: 0.05801764875650406\n",
            "i 90\n",
            "epoch 40\n",
            " batch Loss train: 0.08565220981836319\n",
            "i 91\n",
            "epoch 40\n",
            " batch Loss train: 0.08163782954216003\n",
            "i 92\n",
            "epoch 40\n",
            " batch Loss train: 0.0745995044708252\n",
            "i 93\n",
            "epoch 40\n",
            " batch Loss train: 0.07093625515699387\n",
            "i 94\n",
            "epoch 40\n",
            " batch Loss train: 0.06716040521860123\n",
            "i 95\n",
            "epoch 40\n",
            " batch Loss train: 0.05852785333991051\n",
            "i 96\n",
            "epoch 40\n",
            " batch Loss train: 0.06375785917043686\n",
            "i 97\n",
            "epoch 40\n",
            " batch Loss train: 0.07083277404308319\n",
            "i 98\n",
            "epoch 40\n",
            " batch Loss train: 0.08838210999965668\n",
            "i 99\n",
            "epoch 40\n",
            " batch Loss train: 0.0671849176287651\n",
            "i 100\n",
            "epoch 40\n",
            " batch Loss train: 0.05083222687244415\n",
            "i 101\n",
            "epoch 40\n",
            " batch Loss train: 0.06453356891870499\n",
            "i 102\n",
            "epoch 40\n",
            " batch Loss train: 0.06969728320837021\n",
            "i 103\n",
            "epoch 40\n",
            " batch Loss train: 0.06960683315992355\n",
            "i 104\n",
            "epoch 40\n",
            " batch Loss train: 0.08473443239927292\n",
            "i 105\n",
            "epoch 40\n",
            " batch Loss train: 0.08808795362710953\n",
            "i 106\n",
            "epoch 40\n",
            " batch Loss train: 0.08264850080013275\n",
            "i 107\n",
            "epoch 40\n",
            " batch Loss train: 0.062137357890605927\n",
            "i 108\n",
            "epoch 40\n",
            " batch Loss train: 0.05553620308637619\n",
            "i 109\n",
            "epoch 40\n",
            " batch Loss train: 0.07427041977643967\n",
            "i 110\n",
            "epoch 40\n",
            " batch Loss train: 0.06963615864515305\n",
            "i 111\n",
            "epoch 40\n",
            " batch Loss train: 0.061523761600255966\n",
            "i 112\n",
            "epoch 40\n",
            " batch Loss train: 0.06793853640556335\n",
            "i 113\n",
            "epoch 40\n",
            " batch Loss train: 0.09722495079040527\n",
            "i 114\n",
            "epoch 40\n",
            " batch Loss train: 0.09163164347410202\n",
            "i 115\n",
            "epoch 40\n",
            " batch Loss train: 0.09930584579706192\n",
            "i 116\n",
            "epoch 40\n",
            " batch Loss train: 0.0674799233675003\n",
            "i 117\n",
            "epoch 40\n",
            " batch Loss train: 0.07056553661823273\n",
            "i 118\n",
            "epoch 40\n",
            " batch Loss train: 0.09717874228954315\n",
            "i 119\n",
            "epoch 40\n",
            " batch Loss train: 0.08280333131551743\n",
            "i 120\n",
            "epoch 40\n",
            " batch Loss train: 0.07255256921052933\n",
            "i 121\n",
            "epoch 40\n",
            " batch Loss train: 0.07234826683998108\n",
            "i 122\n",
            "epoch 40\n",
            " batch Loss train: 0.0574071891605854\n",
            "i 123\n",
            "epoch 40\n",
            " batch Loss train: 0.0665888711810112\n",
            "i 124\n",
            "epoch 40\n",
            " batch Loss train: 0.08108822256326675\n",
            "i 125\n",
            "epoch 40\n",
            " batch Loss train: 0.09347919374704361\n",
            "i 126\n",
            "epoch 40\n",
            " batch Loss train: 0.10435806959867477\n",
            "i 127\n",
            "epoch 40\n",
            " batch Loss train: 0.05985229089856148\n",
            "i 128\n",
            "epoch 40\n",
            " batch Loss train: 0.08160798996686935\n",
            "i 129\n",
            "epoch 40\n",
            " batch Loss train: 0.07196804881095886\n",
            "i 130\n",
            "epoch 40\n",
            " batch Loss train: 0.07837045192718506\n",
            "i 131\n",
            "epoch 40\n",
            " batch Loss train: 0.07427360117435455\n",
            "i 132\n",
            "epoch 40\n",
            " batch Loss train: 0.07582930475473404\n",
            "i 133\n",
            "epoch 40\n",
            " batch Loss train: 0.05990748479962349\n",
            "i 134\n",
            "epoch 40\n",
            " batch Loss train: 0.0829005315899849\n",
            "i 135\n",
            "epoch 40\n",
            " batch Loss train: 0.08208555728197098\n",
            "i 136\n",
            "epoch 40\n",
            " batch Loss train: 0.07869959622621536\n",
            "i 137\n",
            "epoch 40\n",
            " batch Loss train: 0.05765526741743088\n",
            "i 138\n",
            "epoch 40\n",
            " batch Loss train: 0.07795751839876175\n",
            "i 139\n",
            "epoch 40\n",
            " batch Loss train: 0.06648028641939163\n",
            "i 140\n",
            "epoch 40\n",
            " batch Loss train: 0.17151831090450287\n",
            "i 141\n",
            "epoch 40\n",
            " batch Loss train: 0.15257203578948975\n",
            "i 142\n",
            "epoch 40\n",
            " batch Loss train: 0.09317011386156082\n",
            "i 143\n",
            "epoch 40\n",
            " batch Loss train: 0.1081458106637001\n",
            "i 144\n",
            "epoch 40\n",
            " batch Loss train: 0.12869685888290405\n",
            "i 145\n",
            "epoch 40\n",
            " batch Loss train: 0.10471037030220032\n",
            "i 146\n",
            "epoch 40\n",
            " batch Loss train: 0.09315928816795349\n",
            "i 147\n",
            "epoch 40\n",
            " batch Loss train: 0.10160387307405472\n",
            "i 148\n",
            "epoch 40\n",
            " batch Loss train: 0.08892185986042023\n",
            "i 149\n",
            "epoch 40\n",
            " batch Loss train: 0.07566696405410767\n",
            "i 150\n",
            "epoch 40\n",
            " batch Loss train: 0.09061066806316376\n",
            "i 151\n",
            "epoch 40\n",
            " batch Loss train: 0.08857428282499313\n",
            "i 152\n",
            "epoch 40\n",
            " batch Loss train: 0.09674639999866486\n",
            "i 153\n",
            "epoch 40\n",
            " batch Loss train: 0.09594554454088211\n",
            "i 154\n",
            "epoch 40\n",
            " batch Loss train: 0.11653733998537064\n",
            "i 155\n",
            "epoch 40\n",
            " batch Loss train: 0.09583685547113419\n",
            "i 156\n",
            "epoch 40\n",
            " batch Loss train: 0.07814185321331024\n",
            "i 157\n",
            "epoch 40\n",
            " batch Loss train: 0.09541017562150955\n",
            "i 158\n",
            "epoch 40\n",
            " batch Loss train: 0.07488398998975754\n",
            "i 159\n",
            "epoch 40\n",
            " batch Loss train: 0.10965809971094131\n",
            "i 160\n",
            "epoch 40\n",
            " batch Loss train: 0.08239177614450455\n",
            "i 161\n",
            "epoch 40\n",
            " batch Loss train: 0.11380136758089066\n",
            "i 162\n",
            "epoch 40\n",
            " batch Loss train: 0.06847740709781647\n",
            "i 163\n",
            "epoch 40\n",
            " batch Loss train: 0.11679387092590332\n",
            "i 164\n",
            "epoch 40\n",
            " batch Loss train: 0.07819569855928421\n",
            "i 165\n",
            "epoch 40\n",
            " batch Loss train: 0.09713613986968994\n",
            "i 166\n",
            "epoch 40\n",
            " batch Loss train: 0.08266058564186096\n",
            "i 167\n",
            "epoch 40\n",
            " batch Loss train: 0.11291259527206421\n",
            "i 168\n",
            "epoch 40\n",
            " batch Loss train: 0.10570879280567169\n",
            "i 169\n",
            "epoch 40\n",
            " batch Loss train: 0.0885317325592041\n",
            "i 170\n",
            "epoch 40\n",
            " batch Loss train: 0.08788039535284042\n",
            "i 171\n",
            "epoch 40\n",
            " batch Loss train: 0.08880177140235901\n",
            "i 172\n",
            "epoch 40\n",
            " batch Loss train: 0.09192530810832977\n",
            "i 173\n",
            "epoch 40\n",
            " batch Loss train: 0.08454699069261551\n",
            "i 174\n",
            "epoch 40\n",
            " batch Loss train: 0.0910542830824852\n",
            "i 175\n",
            "epoch 40\n",
            " batch Loss train: 0.09081222116947174\n",
            "i 176\n",
            "epoch 40\n",
            " batch Loss train: 0.0812968909740448\n",
            "i 177\n",
            "epoch 40\n",
            " batch Loss train: 0.09589856117963791\n",
            "i 178\n",
            "epoch 40\n",
            " batch Loss train: 0.08991440385580063\n",
            "i 179\n",
            "epoch 40\n",
            " batch Loss train: 0.06404531747102737\n",
            "i 180\n",
            "epoch 40\n",
            " batch Loss train: 0.0974377691745758\n",
            "i 181\n",
            "epoch 40\n",
            " batch Loss train: 0.10081484168767929\n",
            "i 182\n",
            "epoch 40\n",
            " batch Loss train: 0.15067319571971893\n",
            "i 183\n",
            "epoch 40\n",
            " batch Loss train: 0.08210170269012451\n",
            "i 184\n",
            "epoch 40\n",
            " batch Loss train: 0.08284204453229904\n",
            "i 185\n",
            "epoch 40\n",
            " batch Loss train: 0.10173974931240082\n",
            "i 186\n",
            "epoch 40\n",
            " batch Loss train: 0.10736555606126785\n",
            "i 187\n",
            "epoch 40\n",
            " batch Loss train: 0.07550571858882904\n",
            "i 188\n",
            "epoch 40\n",
            " batch Loss train: 0.08839384466409683\n",
            "i 189\n",
            "epoch 40\n",
            " batch Loss train: 0.11146816611289978\n",
            "i 190\n",
            "epoch 40\n",
            " batch Loss train: 0.08497020602226257\n",
            "i 191\n",
            "epoch 40\n",
            " batch Loss train: 0.08064964413642883\n",
            "i 192\n",
            "epoch 40\n",
            " batch Loss train: 0.05223429575562477\n",
            "i 193\n",
            "epoch 40\n",
            " batch Loss train: 0.06909196078777313\n",
            "i 194\n",
            "epoch 40\n",
            " batch Loss train: 0.0780724361538887\n",
            "i 195\n",
            "epoch 40\n",
            " batch Loss train: 0.08546023070812225\n",
            "i 196\n",
            "epoch 40\n",
            " batch Loss train: 0.08009118586778641\n",
            "i 197\n",
            "epoch 40\n",
            " batch Loss train: 0.09034936875104904\n",
            "i 198\n",
            "epoch 40\n",
            " batch Loss train: 0.08803153038024902\n",
            "i 199\n",
            "epoch 40\n",
            " batch Loss train: 0.08721345663070679\n",
            "i 200\n",
            "epoch 40\n",
            " batch Loss train: 0.1172734722495079\n",
            "i 201\n",
            "epoch 40\n",
            " batch Loss train: 0.06066476181149483\n",
            "i 202\n",
            "epoch 40\n",
            " batch Loss train: 0.07324882596731186\n",
            "i 203\n",
            "epoch 40\n",
            " batch Loss train: 0.101377934217453\n",
            "i 204\n",
            "epoch 40\n",
            " batch Loss train: 0.08278384804725647\n",
            "i 205\n",
            "epoch 40\n",
            " batch Loss train: 0.07739126682281494\n",
            "i 206\n",
            "epoch 40\n",
            " batch Loss train: 0.0831177681684494\n",
            "i 207\n",
            "epoch 40\n",
            " batch Loss train: 0.06239347159862518\n",
            "i 208\n",
            "epoch 40\n",
            " batch Loss train: 0.07357236742973328\n",
            "i 209\n",
            "epoch 40\n",
            " batch Loss train: 0.1033918634057045\n",
            "i 210\n",
            "epoch 40\n",
            " batch Loss train: 0.0799776166677475\n",
            "i 211\n",
            "epoch 40\n",
            " batch Loss train: 0.06931662559509277\n",
            "i 212\n",
            "epoch 40\n",
            " batch Loss train: 0.07120915502309799\n",
            "i 213\n",
            "epoch 40\n",
            " batch Loss train: 0.06157182902097702\n",
            "i 214\n",
            "epoch 40\n",
            " batch Loss train: 0.08942095190286636\n",
            "i 215\n",
            "epoch 40\n",
            " batch Loss train: 0.10815706104040146\n",
            "i 216\n",
            "epoch 40\n",
            " batch Loss train: 0.06672654300928116\n",
            "i 217\n",
            "epoch 40\n",
            " batch Loss train: 0.07080500572919846\n",
            "i 218\n",
            "epoch 40\n",
            " batch Loss train: 0.09104247391223907\n",
            "i 219\n",
            "epoch 40\n",
            " batch Loss train: 0.07510855048894882\n",
            "i 220\n",
            "epoch 40\n",
            " batch Loss train: 0.09594801813364029\n",
            "i 221\n",
            "epoch 40\n",
            " batch Loss train: 0.07959578186273575\n",
            "i 222\n",
            "epoch 40\n",
            " batch Loss train: 0.07227537781000137\n",
            "i 223\n",
            "epoch 40\n",
            " batch Loss train: 0.06249149516224861\n",
            "i 224\n",
            "epoch 40\n",
            " batch Loss train: 0.07788000255823135\n",
            "i 225\n",
            "epoch 40\n",
            " batch Loss train: 0.09301003813743591\n",
            "i 226\n",
            "epoch 40\n",
            " batch Loss train: 0.0904630646109581\n",
            "i 227\n",
            "epoch 40\n",
            " batch Loss train: 0.07755862921476364\n",
            "i 228\n",
            "epoch 40\n",
            " batch Loss train: 0.11414894461631775\n",
            "i 229\n",
            "epoch 40\n",
            " batch Loss train: 0.07089021801948547\n",
            "i 230\n",
            "epoch 40\n",
            " batch Loss train: 0.07341548800468445\n",
            "i 231\n",
            "epoch 40\n",
            " batch Loss train: 0.06224514916539192\n",
            "i 232\n",
            "epoch 40\n",
            " batch Loss train: 0.10057691484689713\n",
            "i 233\n",
            "epoch 40\n",
            " batch Loss train: 0.07543209195137024\n",
            "i 234\n",
            "epoch 40\n",
            " batch Loss train: 0.06018122285604477\n",
            "i 235\n",
            "epoch 40\n",
            " batch Loss train: 0.06314770132303238\n",
            "i 236\n",
            "epoch 40\n",
            " batch Loss train: 0.07307851314544678\n",
            "i 237\n",
            "epoch 40\n",
            " batch Loss train: 0.07511477917432785\n",
            "i 238\n",
            "epoch 40\n",
            " batch Loss train: 0.0681171789765358\n",
            "i 239\n",
            "epoch 40\n",
            " batch Loss train: 0.07542875409126282\n",
            "i 240\n",
            "epoch 40\n",
            " batch Loss train: 0.07678748667240143\n",
            "i 241\n",
            "epoch 40\n",
            " batch Loss train: 0.08421866595745087\n",
            "i 242\n",
            "epoch 40\n",
            " batch Loss train: 0.04948236793279648\n",
            "i 243\n",
            "epoch 40\n",
            " batch Loss train: 0.0752134621143341\n",
            "i 244\n",
            "epoch 40\n",
            " batch Loss train: 0.08223053812980652\n",
            "i 245\n",
            "epoch 40\n",
            " batch Loss train: 0.07591807842254639\n",
            "i 246\n",
            "epoch 40\n",
            " batch Loss train: 0.07044175267219543\n",
            "i 247\n",
            "epoch 40\n",
            " batch Loss train: 0.08323857188224792\n",
            "i 248\n",
            "epoch 40\n",
            " batch Loss train: 0.06853076070547104\n",
            "i 249\n",
            "epoch 40\n",
            " batch Loss train: 0.08162587136030197\n",
            "i 250\n",
            "epoch 40\n",
            " batch Loss train: 0.08666224032640457\n",
            "i 251\n",
            "epoch 40\n",
            " batch Loss train: 0.0842486023902893\n",
            "i 252\n",
            "epoch 40\n",
            " batch Loss train: 0.07307861000299454\n",
            "i 253\n",
            "epoch 40\n",
            " batch Loss train: 0.07576443254947662\n",
            "i 254\n",
            "epoch 40\n",
            " batch Loss train: 0.08043857663869858\n",
            "i 255\n",
            "epoch 40\n",
            " batch Loss train: 0.08789127320051193\n",
            "i 256\n",
            "epoch 40\n",
            " batch Loss train: 0.07613413035869598\n",
            "i 257\n",
            "epoch 40\n",
            " batch Loss train: 0.08313020318746567\n",
            "i 258\n",
            "epoch 40\n",
            " batch Loss train: 0.08311503380537033\n",
            "i 259\n",
            "epoch 40\n",
            " batch Loss train: 0.06904307752847672\n",
            "i 260\n",
            "epoch 40\n",
            " batch Loss train: 0.07490281760692596\n",
            "i 261\n",
            "epoch 40\n",
            " batch Loss train: 0.07815499603748322\n",
            "i 262\n",
            "epoch 40\n",
            " batch Loss train: 0.07646875083446503\n",
            "i 263\n",
            "epoch 40\n",
            " batch Loss train: 0.06770063191652298\n",
            "i 264\n",
            "epoch 40\n",
            " batch Loss train: 0.07313641905784607\n",
            "i 265\n",
            "epoch 40\n",
            " batch Loss train: 0.06866733729839325\n",
            "i 266\n",
            "epoch 40\n",
            " batch Loss train: 0.06832369416952133\n",
            "i 267\n",
            "epoch 40\n",
            " batch Loss train: 0.08427232503890991\n",
            "i 268\n",
            "epoch 40\n",
            " batch Loss train: 0.07074765115976334\n",
            "i 269\n",
            "epoch 40\n",
            " batch Loss train: 0.0737033411860466\n",
            "i 270\n",
            "epoch 40\n",
            " batch Loss train: 0.07333855330944061\n",
            "i 271\n",
            "epoch 40\n",
            " batch Loss train: 0.06958772242069244\n",
            "i 272\n",
            "epoch 40\n",
            " batch Loss train: 0.10065996646881104\n",
            "i 273\n",
            "epoch 40\n",
            " batch Loss train: 0.07799983769655228\n",
            "i 274\n",
            "epoch 40\n",
            " batch Loss train: 0.06609012931585312\n",
            "i 275\n",
            "epoch 40\n",
            " batch Loss train: 0.09550900757312775\n",
            "i 276\n",
            "epoch 40\n",
            " batch Loss train: 0.08040328323841095\n",
            "i 277\n",
            "epoch 40\n",
            " batch Loss train: 0.06054196506738663\n",
            "i 278\n",
            "epoch 40\n",
            " batch Loss train: 0.0696488469839096\n",
            "i 279\n",
            "epoch 40\n",
            " batch Loss train: 0.0861739069223404\n",
            "i 280\n",
            "epoch 40\n",
            " batch Loss train: 0.08263270556926727\n",
            "i 281\n",
            "epoch 40\n",
            " batch Loss train: 0.07380624860525131\n",
            "i 282\n",
            "epoch 40\n",
            " batch Loss train: 0.06651736050844193\n",
            "i 283\n",
            "epoch 40\n",
            " batch Loss train: 0.06898538023233414\n",
            "i 284\n",
            "epoch 40\n",
            " batch Loss train: 0.07976916432380676\n",
            "i 285\n",
            "epoch 40\n",
            " batch Loss train: 0.08774124830961227\n",
            "i 286\n",
            "epoch 40\n",
            " batch Loss train: 0.08972088247537613\n",
            "i 287\n",
            "epoch 40\n",
            " batch Loss train: 0.08051601052284241\n",
            "i 288\n",
            "epoch 40\n",
            " batch Loss train: 0.08015662431716919\n",
            "i 289\n",
            "epoch 40\n",
            " batch Loss train: 0.07443995028734207\n",
            "i 290\n",
            "epoch 40\n",
            " batch Loss train: 0.09590020775794983\n",
            "i 291\n",
            "epoch 40\n",
            " batch Loss train: 0.05412457883358002\n",
            "i 292\n",
            "epoch 40\n",
            " batch Loss train: 0.058622948825359344\n",
            "i 293\n",
            "epoch 40\n",
            " batch Loss train: 0.07420839369297028\n",
            "i 294\n",
            "epoch 40\n",
            " batch Loss train: 0.08841149508953094\n",
            "i 295\n",
            "epoch 40\n",
            " batch Loss train: 0.07899127155542374\n",
            "i 296\n",
            "epoch 40\n",
            " batch Loss train: 0.07256674021482468\n",
            "i 297\n",
            "epoch 40\n",
            " batch Loss train: 0.07568731904029846\n",
            "i 298\n",
            "epoch 40\n",
            " batch Loss train: 0.07920849323272705\n",
            "i 299\n",
            "epoch 40\n",
            " batch Loss train: 0.08028300851583481\n",
            "i 300\n",
            "epoch 40\n",
            " batch Loss train: 0.04990939050912857\n",
            "i 301\n",
            "epoch 40\n",
            " batch Loss train: 0.08642193675041199\n",
            "i 302\n",
            "epoch 40\n",
            " batch Loss train: 0.06611994653940201\n",
            "i 303\n",
            "epoch 40\n",
            " batch Loss train: 0.09612740576267242\n",
            "i 304\n",
            "epoch 40\n",
            " batch Loss train: 0.0606786273419857\n",
            "i 305\n",
            "epoch 40\n",
            " batch Loss train: 0.07201884686946869\n",
            "i 306\n",
            "epoch 40\n",
            " batch Loss train: 0.07426988333463669\n",
            "i 307\n",
            "epoch 40\n",
            " batch Loss train: 0.06040864810347557\n",
            "i 308\n",
            "epoch 40\n",
            " batch Loss train: 0.07294578105211258\n",
            "i 309\n",
            "epoch 40\n",
            " batch Loss train: 0.0665493980050087\n",
            "i 310\n",
            "epoch 40\n",
            " batch Loss train: 0.07899461686611176\n",
            "i 311\n",
            "epoch 40\n",
            " batch Loss train: 0.0632074624300003\n",
            "i 312\n",
            "epoch 40\n",
            " batch Loss train: 0.09556343406438828\n",
            "i 313\n",
            "epoch 40\n",
            " batch Loss train: 0.06545234471559525\n",
            "i 314\n",
            "epoch 40\n",
            " batch Loss train: 0.06387192755937576\n",
            "i 315\n",
            "epoch 40\n",
            " batch Loss train: 0.08230506628751755\n",
            "i 316\n",
            "epoch 40\n",
            " batch Loss train: 0.07058447599411011\n",
            "i 317\n",
            "epoch 40\n",
            " batch Loss train: 0.08880244195461273\n",
            "i 318\n",
            "epoch 40\n",
            " batch Loss train: 0.07182591408491135\n",
            "i 319\n",
            "epoch 40\n",
            " batch Loss train: 0.07007955014705658\n",
            "i 320\n",
            "epoch 40\n",
            " batch Loss train: 0.07082818448543549\n",
            "i 321\n",
            "epoch 40\n",
            " batch Loss train: 0.07818464189767838\n",
            "i 322\n",
            "epoch 40\n",
            " batch Loss train: 0.07291266322135925\n",
            "i 323\n",
            "epoch 40\n",
            " batch Loss train: 0.05931777134537697\n",
            "i 324\n",
            "epoch 40\n",
            " batch Loss train: 0.07981050759553909\n",
            "i 325\n",
            "epoch 40\n",
            " batch Loss train: 0.06189686805009842\n",
            "i 326\n",
            "epoch 40\n",
            " batch Loss train: 0.08056372404098511\n",
            "i 327\n",
            "epoch 40\n",
            " batch Loss train: 0.07280048727989197\n",
            "i 328\n",
            "epoch 40\n",
            " batch Loss train: 0.07058249413967133\n",
            "i 329\n",
            "epoch 40\n",
            " batch Loss train: 0.08763280510902405\n",
            "i 330\n",
            "epoch 40\n",
            " batch Loss train: 0.07230541110038757\n",
            "i 331\n",
            "epoch 40\n",
            " batch Loss train: 0.04932649806141853\n",
            "i 332\n",
            "epoch 40\n",
            " batch Loss train: 0.08120813965797424\n",
            "i 333\n",
            "epoch 40\n",
            " batch Loss train: 0.08781272172927856\n",
            "i 334\n",
            "epoch 40\n",
            " batch Loss train: 0.11061909794807434\n",
            "i 335\n",
            "epoch 40\n",
            " batch Loss train: 0.07155400514602661\n",
            "i 336\n",
            "epoch 40\n",
            " batch Loss train: 0.09050174057483673\n",
            "i 337\n",
            "epoch 40\n",
            " batch Loss train: 0.07661419361829758\n",
            "i 338\n",
            "epoch 40\n",
            " batch Loss train: 0.07828714698553085\n",
            "i 339\n",
            "epoch 40\n",
            " batch Loss train: 0.06894698739051819\n",
            "i 340\n",
            "epoch 40\n",
            " batch Loss train: 0.07357686012983322\n",
            "i 341\n",
            "epoch 40\n",
            " batch Loss train: 0.05972396954894066\n",
            "i 342\n",
            "epoch 40\n",
            " batch Loss train: 0.08499849587678909\n",
            "i 343\n",
            "epoch 40\n",
            " batch Loss train: 0.06780779361724854\n",
            "i 344\n",
            "epoch 40\n",
            " batch Loss train: 0.07771408557891846\n",
            "i 345\n",
            "epoch 40\n",
            " batch Loss train: 0.08262623101472855\n",
            "i 346\n",
            "epoch 40\n",
            " batch Loss train: 0.08168873935937881\n",
            "i 347\n",
            "epoch 40\n",
            " batch Loss train: 0.06756050139665604\n",
            "i 348\n",
            "epoch 40\n",
            " batch Loss train: 0.11309601366519928\n",
            "i 349\n",
            "epoch 40\n",
            " batch Loss train: 0.07284596562385559\n",
            "i 350\n",
            "epoch 40\n",
            " batch Loss train: 0.08331765234470367\n",
            "i 351\n",
            "epoch 40\n",
            " batch Loss train: 0.07724674046039581\n",
            "i 352\n",
            "epoch 40\n",
            " batch Loss train: 0.08023528009653091\n",
            "i 353\n",
            "epoch 40\n",
            " batch Loss train: 0.06659097224473953\n",
            "i 354\n",
            "epoch 40\n",
            " batch Loss train: 0.09851229935884476\n",
            "i 355\n",
            "epoch 40\n",
            " batch Loss train: 0.07901295274496078\n",
            "i 356\n",
            "epoch 40\n",
            " batch Loss train: 0.07936118543148041\n",
            "i 357\n",
            "epoch 40\n",
            " batch Loss train: 0.05799859017133713\n",
            "i 358\n",
            "epoch 40\n",
            " batch Loss train: 0.0712953433394432\n",
            "i 359\n",
            "epoch 40\n",
            " batch Loss train: 0.0823129341006279\n",
            "i 360\n",
            "epoch 40\n",
            " batch Loss train: 0.09284497052431107\n",
            "i 361\n",
            "epoch 40\n",
            " batch Loss train: 0.06960942596197128\n",
            "i 362\n",
            "epoch 40\n",
            " batch Loss train: 0.0711911991238594\n",
            "i 363\n",
            "epoch 40\n",
            " batch Loss train: 0.0685771256685257\n",
            "i 364\n",
            "epoch 40\n",
            " batch Loss train: 0.06831961125135422\n",
            "i 365\n",
            "epoch 40\n",
            " batch Loss train: 0.09196928888559341\n",
            "i 366\n",
            "epoch 40\n",
            " batch Loss train: 0.06263750046491623\n",
            "i 367\n",
            "epoch 40\n",
            " batch Loss train: 0.059964340180158615\n",
            "i 368\n",
            "epoch 40\n",
            " batch Loss train: 0.07950589060783386\n",
            "i 369\n",
            "epoch 40\n",
            " batch Loss train: 0.0832417905330658\n",
            "i 370\n",
            "epoch 40\n",
            " batch Loss train: 0.07286399602890015\n",
            "i 371\n",
            "epoch 40\n",
            " batch Loss train: 0.07815161347389221\n",
            "i 372\n",
            "epoch 40\n",
            " batch Loss train: 0.05164830759167671\n",
            "i 373\n",
            "epoch 40\n",
            " batch Loss train: 0.07600793242454529\n",
            "i 374\n",
            "epoch 40\n",
            " batch Loss train: 0.07461681962013245\n",
            "i 375\n",
            "epoch 40\n",
            " batch Loss train: 0.08218425512313843\n",
            "i 376\n",
            "epoch 40\n",
            " batch Loss train: 0.07830677181482315\n",
            "i 377\n",
            "epoch 40\n",
            " batch Loss train: 0.0759587436914444\n",
            "i 378\n",
            "epoch 40\n",
            " batch Loss train: 0.06525816023349762\n",
            "i 379\n",
            "epoch 40\n",
            " batch Loss train: 0.07181426882743835\n",
            "i 380\n",
            "epoch 40\n",
            " batch Loss train: 0.09521133452653885\n",
            "i 381\n",
            "epoch 40\n",
            " batch Loss train: 0.06937375664710999\n",
            "i 382\n",
            "epoch 40\n",
            " batch Loss train: 0.062022339552640915\n",
            "i 383\n",
            "epoch 40\n",
            " batch Loss train: 0.08694794028997421\n",
            "i 384\n",
            "epoch 40\n",
            " batch Loss train: 0.0640626773238182\n",
            "i 385\n",
            "epoch 40\n",
            " batch Loss train: 0.06856139749288559\n",
            "i 386\n",
            "epoch 40\n",
            " batch Loss train: 0.07584474235773087\n",
            "i 387\n",
            "epoch 40\n",
            " batch Loss train: 0.09029402583837509\n",
            "i 388\n",
            "epoch 40\n",
            " batch Loss train: 0.064963199198246\n",
            "i 389\n",
            "epoch 40\n",
            " batch Loss train: 0.10169367492198944\n",
            "i 390\n",
            "epoch 40\n",
            " batch Loss train: 0.0780203640460968\n",
            "i 391\n",
            "epoch 40\n",
            " batch Loss train: 0.06320128589868546\n",
            "i 392\n",
            "epoch 40\n",
            " batch Loss train: 0.07378913462162018\n",
            "i 393\n",
            "epoch 40\n",
            " batch Loss train: 0.07836507260799408\n",
            "i 394\n",
            "epoch 40\n",
            " batch Loss train: 0.05883045122027397\n",
            "i 395\n",
            "epoch 40\n",
            " batch Loss train: 0.09036392718553543\n",
            "i 396\n",
            "epoch 40\n",
            " batch Loss train: 0.07469247281551361\n",
            "i 397\n",
            "epoch 40\n",
            " batch Loss train: 0.06752806901931763\n",
            "i 398\n",
            "epoch 40\n",
            " batch Loss train: 0.07247383147478104\n",
            "i 399\n",
            "epoch 40\n",
            " batch Loss train: 0.09174510091543198\n",
            "i 400\n",
            "epoch 40\n",
            " batch Loss train: 0.06018679216504097\n",
            "i 401\n",
            "epoch 40\n",
            " batch Loss train: 0.07120651006698608\n",
            "i 402\n",
            "epoch 40\n",
            " batch Loss train: 0.06883198022842407\n",
            "i 403\n",
            "epoch 40\n",
            " batch Loss train: 0.0701155811548233\n",
            "i 404\n",
            "epoch 40\n",
            " batch Loss train: 0.08315544575452805\n",
            "i 405\n",
            "epoch 40\n",
            " batch Loss train: 0.07592780888080597\n",
            "i 406\n",
            "epoch 40\n",
            " batch Loss train: 0.06865333020687103\n",
            "i 407\n",
            "epoch 40\n",
            " batch Loss train: 0.07992596179246902\n",
            "i 408\n",
            "epoch 40\n",
            " batch Loss train: 0.07154275476932526\n",
            "i 409\n",
            "epoch 40\n",
            " batch Loss train: 0.059828273952007294\n",
            "i 410\n",
            "epoch 40\n",
            " batch Loss train: 0.10218524932861328\n",
            "i 411\n",
            "epoch 40\n",
            " batch Loss train: 0.058424949645996094\n",
            "i 412\n",
            "epoch 40\n",
            " batch Loss train: 0.0778207778930664\n",
            "i 413\n",
            "epoch 40\n",
            " batch Loss train: 0.08346381038427353\n",
            "i 414\n",
            "epoch 40\n",
            " batch Loss train: 0.05858834832906723\n",
            "i 415\n",
            "epoch 40\n",
            " batch Loss train: 0.06925376504659653\n",
            "i 416\n",
            "epoch 40\n",
            " batch Loss train: 0.045910049229860306\n",
            "i 417\n",
            "epoch 40\n",
            " batch Loss train: 0.058448757976293564\n",
            "i 418\n",
            "epoch 40\n",
            " batch Loss train: 0.07412255555391312\n",
            "i 419\n",
            "epoch 40\n",
            " batch Loss train: 0.0840189978480339\n",
            "i 420\n",
            "epoch 40\n",
            " batch Loss train: 0.09033400565385818\n",
            "i 421\n",
            "epoch 40\n",
            " batch Loss train: 0.059730883687734604\n",
            "i 422\n",
            "epoch 40\n",
            " batch Loss train: 0.09702244400978088\n",
            "i 423\n",
            "epoch 40\n",
            " batch Loss train: 0.06899163872003555\n",
            "i 424\n",
            "epoch 40\n",
            " batch Loss train: 0.0689682886004448\n",
            "i 425\n",
            "epoch 40\n",
            " batch Loss train: 0.052344974130392075\n",
            "i 426\n",
            "epoch 40\n",
            " batch Loss train: 0.07380613684654236\n",
            "i 427\n",
            "epoch 40\n",
            " batch Loss train: 0.07883743941783905\n",
            "i 428\n",
            "epoch 40\n",
            " batch Loss train: 0.08217407017946243\n",
            "i 429\n",
            "epoch 40\n",
            " batch Loss train: 0.09924547374248505\n",
            "i 430\n",
            "epoch 40\n",
            " batch Loss train: 0.06007188931107521\n",
            "i 431\n",
            "epoch 40\n",
            " batch Loss train: 0.06465937197208405\n",
            "i 432\n",
            "epoch 40\n",
            " batch Loss train: 0.06963052600622177\n",
            "i 433\n",
            "epoch 40\n",
            " batch Loss train: 0.08454769849777222\n",
            "i 434\n",
            "epoch 40\n",
            " batch Loss train: 0.0569179430603981\n",
            "i 435\n",
            "epoch 40\n",
            " batch Loss train: 0.09023164212703705\n",
            "i 436\n",
            "epoch 40\n",
            " batch Loss train: 0.07050970196723938\n",
            "i 437\n",
            "epoch 40\n",
            " batch Loss train: 0.08250310271978378\n",
            "i 438\n",
            "epoch 40\n",
            " batch Loss train: 0.0725657194852829\n",
            "i 439\n",
            "epoch 40\n",
            " batch Loss train: 0.0984107255935669\n",
            "i 440\n",
            "epoch 40\n",
            " batch Loss train: 0.07341023534536362\n",
            "i 441\n",
            "epoch 40\n",
            " batch Loss train: 0.053449083119630814\n",
            "i 442\n",
            "epoch 40\n",
            " batch Loss train: 0.07360170036554337\n",
            "i 443\n",
            "epoch 40\n",
            " batch Loss train: 0.08110226690769196\n",
            "i 444\n",
            "epoch 40\n",
            " batch Loss train: 0.08221576362848282\n",
            "i 445\n",
            "epoch 40\n",
            " batch Loss train: 0.06282561272382736\n",
            "total epoch Loss train: tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 41\n",
            " batch Loss train: 0.0417264848947525\n",
            "i 1\n",
            "epoch 41\n",
            " batch Loss train: 0.0564693883061409\n",
            "i 2\n",
            "epoch 41\n",
            " batch Loss train: 0.03887041658163071\n",
            "i 3\n",
            "epoch 41\n",
            " batch Loss train: 0.05707288905978203\n",
            "i 4\n",
            "epoch 41\n",
            " batch Loss train: 0.060931507498025894\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 41\n",
            " batch Loss train: 0.049768224358558655\n",
            "i 6\n",
            "epoch 41\n",
            " batch Loss train: 0.08137614279985428\n",
            "i 7\n",
            "epoch 41\n",
            " batch Loss train: 0.05738838389515877\n",
            "i 8\n",
            "epoch 41\n",
            " batch Loss train: 0.060743529349565506\n",
            "i 9\n",
            "epoch 41\n",
            " batch Loss train: 0.06140298768877983\n",
            "i 10\n",
            "epoch 41\n",
            " batch Loss train: 0.044899944216012955\n",
            "i 11\n",
            "epoch 41\n",
            " batch Loss train: 0.05518065020442009\n",
            "i 12\n",
            "epoch 41\n",
            " batch Loss train: 0.06907325983047485\n",
            "i 13\n",
            "epoch 41\n",
            " batch Loss train: 0.055030666291713715\n",
            "i 14\n",
            "epoch 41\n",
            " batch Loss train: 0.050271861255168915\n",
            "i 15\n",
            "epoch 41\n",
            " batch Loss train: 0.06319793313741684\n",
            "i 16\n",
            "epoch 41\n",
            " batch Loss train: 0.048401765525341034\n",
            "i 17\n",
            "epoch 41\n",
            " batch Loss train: 0.05914914235472679\n",
            "i 18\n",
            "epoch 41\n",
            " batch Loss train: 0.0560772530734539\n",
            "i 19\n",
            "epoch 41\n",
            " batch Loss train: 0.04862174764275551\n",
            "i 20\n",
            "epoch 41\n",
            " batch Loss train: 0.052377764135599136\n",
            "i 21\n",
            "epoch 41\n",
            " batch Loss train: 0.05217632278800011\n",
            "i 22\n",
            "epoch 41\n",
            " batch Loss train: 0.07360716909170151\n",
            "i 23\n",
            "epoch 41\n",
            " batch Loss train: 0.05085061118006706\n",
            "i 24\n",
            "epoch 41\n",
            " batch Loss train: 0.05019591003656387\n",
            "i 25\n",
            "epoch 41\n",
            " batch Loss train: 0.05276383087038994\n",
            "i 26\n",
            "epoch 41\n",
            " batch Loss train: 0.06720026582479477\n",
            "i 27\n",
            "epoch 41\n",
            " batch Loss train: 0.04707955941557884\n",
            "i 28\n",
            "epoch 41\n",
            " batch Loss train: 0.05015548691153526\n",
            "i 29\n",
            "epoch 41\n",
            " batch Loss train: 0.048852287232875824\n",
            "i 30\n",
            "epoch 41\n",
            " batch Loss train: 0.07386334985494614\n",
            "i 31\n",
            "epoch 41\n",
            " batch Loss train: 0.06448359787464142\n",
            "i 32\n",
            "epoch 41\n",
            " batch Loss train: 0.045695602893829346\n",
            "i 33\n",
            "epoch 41\n",
            " batch Loss train: 0.055334869772195816\n",
            "i 34\n",
            "epoch 41\n",
            " batch Loss train: 0.049624551087617874\n",
            "i 35\n",
            "epoch 41\n",
            " batch Loss train: 0.07179676741361618\n",
            "i 36\n",
            "epoch 41\n",
            " batch Loss train: 0.07656064629554749\n",
            "i 37\n",
            "epoch 41\n",
            " batch Loss train: 0.05624353885650635\n",
            "i 38\n",
            "epoch 41\n",
            " batch Loss train: 0.05407086759805679\n",
            "i 39\n",
            "epoch 41\n",
            " batch Loss train: 0.08939822018146515\n",
            "i 40\n",
            "epoch 41\n",
            " batch Loss train: 0.0612129308283329\n",
            "i 41\n",
            "epoch 41\n",
            " batch Loss train: 0.06047401204705238\n",
            "i 42\n",
            "epoch 41\n",
            " batch Loss train: 0.06039506942033768\n",
            "i 43\n",
            "epoch 41\n",
            " batch Loss train: 0.06270287185907364\n",
            "i 44\n",
            "epoch 41\n",
            " batch Loss train: 0.04552260786294937\n",
            "i 45\n",
            "epoch 41\n",
            " batch Loss train: 0.0549013614654541\n",
            "i 46\n",
            "epoch 41\n",
            " batch Loss train: 0.059359122067689896\n",
            "i 47\n",
            "epoch 41\n",
            " batch Loss train: 0.06800933182239532\n",
            "i 48\n",
            "epoch 41\n",
            " batch Loss train: 0.05123891308903694\n",
            "i 49\n",
            "epoch 41\n",
            " batch Loss train: 0.07034220546483994\n",
            "i 50\n",
            "epoch 41\n",
            " batch Loss train: 0.05078313499689102\n",
            "i 51\n",
            "epoch 41\n",
            " batch Loss train: 0.07241468876600266\n",
            "i 52\n",
            "epoch 41\n",
            " batch Loss train: 0.06666374206542969\n",
            "i 53\n",
            "epoch 41\n",
            " batch Loss train: 0.06692452728748322\n",
            "i 54\n",
            "epoch 41\n",
            " batch Loss train: 0.051932353526353836\n",
            "i 55\n",
            "epoch 41\n",
            " batch Loss train: 0.0511612705886364\n",
            "i 56\n",
            "epoch 41\n",
            " batch Loss train: 0.05664614588022232\n",
            "i 57\n",
            "epoch 41\n",
            " batch Loss train: 0.03665816783905029\n",
            "i 58\n",
            "epoch 41\n",
            " batch Loss train: 0.04933469370007515\n",
            "i 59\n",
            "epoch 41\n",
            " batch Loss train: 0.04063398391008377\n",
            "i 60\n",
            "epoch 41\n",
            " batch Loss train: 0.05903599038720131\n",
            "i 61\n",
            "epoch 41\n",
            " batch Loss train: 0.055529527366161346\n",
            "i 62\n",
            "epoch 41\n",
            " batch Loss train: 0.058924879878759384\n",
            "i 63\n",
            "epoch 41\n",
            " batch Loss train: 0.07329010218381882\n",
            "i 64\n",
            "epoch 41\n",
            " batch Loss train: 0.05198809504508972\n",
            "i 65\n",
            "epoch 41\n",
            " batch Loss train: 0.05081414431333542\n",
            "i 66\n",
            "epoch 41\n",
            " batch Loss train: 0.07036010175943375\n",
            "i 67\n",
            "epoch 41\n",
            " batch Loss train: 0.04973568022251129\n",
            "i 68\n",
            "epoch 41\n",
            " batch Loss train: 0.06468840688467026\n",
            "i 69\n",
            "epoch 41\n",
            " batch Loss train: 0.06812713295221329\n",
            "i 70\n",
            "epoch 41\n",
            " batch Loss train: 0.052333369851112366\n",
            "i 71\n",
            "epoch 41\n",
            " batch Loss train: 0.047458089888095856\n",
            "i 72\n",
            "epoch 41\n",
            " batch Loss train: 0.05886097252368927\n",
            "i 73\n",
            "epoch 41\n",
            " batch Loss train: 0.06624771654605865\n",
            "i 74\n",
            "epoch 41\n",
            " batch Loss train: 0.06822358071804047\n",
            "i 75\n",
            "epoch 41\n",
            " batch Loss train: 0.058761585503816605\n",
            "i 76\n",
            "epoch 41\n",
            " batch Loss train: 0.04947736859321594\n",
            "i 77\n",
            "epoch 41\n",
            " batch Loss train: 0.06176646798849106\n",
            "i 78\n",
            "epoch 41\n",
            " batch Loss train: 0.05928933247923851\n",
            "i 79\n",
            "epoch 41\n",
            " batch Loss train: 0.0598178394138813\n",
            "i 80\n",
            "epoch 41\n",
            " batch Loss train: 0.07161175459623337\n",
            "i 81\n",
            "epoch 41\n",
            " batch Loss train: 0.05598951876163483\n",
            "i 82\n",
            "epoch 41\n",
            " batch Loss train: 0.053149182349443436\n",
            "i 83\n",
            "epoch 41\n",
            " batch Loss train: 0.04299095273017883\n",
            "i 84\n",
            "epoch 41\n",
            " batch Loss train: 0.056952089071273804\n",
            "i 85\n",
            "epoch 41\n",
            " batch Loss train: 0.07436203211545944\n",
            "i 86\n",
            "epoch 41\n",
            " batch Loss train: 0.05051466077566147\n",
            "i 87\n",
            "epoch 41\n",
            " batch Loss train: 0.06719355285167694\n",
            "i 88\n",
            "epoch 41\n",
            " batch Loss train: 0.06440265476703644\n",
            "i 89\n",
            "epoch 41\n",
            " batch Loss train: 0.07523099333047867\n",
            "i 90\n",
            "epoch 41\n",
            " batch Loss train: 0.058484919369220734\n",
            "i 91\n",
            "epoch 41\n",
            " batch Loss train: 0.05162873491644859\n",
            "i 92\n",
            "epoch 41\n",
            " batch Loss train: 0.051301147788763046\n",
            "i 93\n",
            "epoch 41\n",
            " batch Loss train: 0.05948376655578613\n",
            "i 94\n",
            "epoch 41\n",
            " batch Loss train: 0.056922513991594315\n",
            "i 95\n",
            "epoch 41\n",
            " batch Loss train: 0.06988927721977234\n",
            "i 96\n",
            "epoch 41\n",
            " batch Loss train: 0.05946189537644386\n",
            "i 97\n",
            "epoch 41\n",
            " batch Loss train: 0.05441262200474739\n",
            "i 98\n",
            "epoch 41\n",
            " batch Loss train: 0.059089645743370056\n",
            "i 99\n",
            "epoch 41\n",
            " batch Loss train: 0.0635213553905487\n",
            "i 100\n",
            "epoch 41\n",
            " batch Loss train: 0.07671663910150528\n",
            "i 101\n",
            "epoch 41\n",
            " batch Loss train: 0.0589829757809639\n",
            "i 102\n",
            "epoch 41\n",
            " batch Loss train: 0.052943598479032516\n",
            "i 103\n",
            "epoch 41\n",
            " batch Loss train: 0.06265372782945633\n",
            "i 104\n",
            "epoch 41\n",
            " batch Loss train: 0.05741674825549126\n",
            "i 105\n",
            "epoch 41\n",
            " batch Loss train: 0.06010747328400612\n",
            "i 106\n",
            "epoch 41\n",
            " batch Loss train: 0.06365908682346344\n",
            "i 107\n",
            "epoch 41\n",
            " batch Loss train: 0.0810575857758522\n",
            "i 108\n",
            "epoch 41\n",
            " batch Loss train: 0.05690484493970871\n",
            "i 109\n",
            "epoch 41\n",
            " batch Loss train: 0.0600418783724308\n",
            "i 110\n",
            "epoch 41\n",
            " batch Loss train: 0.07065638899803162\n",
            "i 111\n",
            "epoch 41\n",
            " batch Loss train: 0.07568196952342987\n",
            "i 112\n",
            "epoch 41\n",
            " batch Loss train: 0.087412029504776\n",
            "i 113\n",
            "epoch 41\n",
            " batch Loss train: 0.059539906680583954\n",
            "i 114\n",
            "epoch 41\n",
            " batch Loss train: 0.06347857415676117\n",
            "i 115\n",
            "epoch 41\n",
            " batch Loss train: 0.05006293207406998\n",
            "i 116\n",
            "epoch 41\n",
            " batch Loss train: 0.0764196366071701\n",
            "i 117\n",
            "epoch 41\n",
            " batch Loss train: 0.060212429612874985\n",
            "i 118\n",
            "epoch 41\n",
            " batch Loss train: 0.08698846399784088\n",
            "i 119\n",
            "epoch 41\n",
            " batch Loss train: 0.07670585811138153\n",
            "i 120\n",
            "epoch 41\n",
            " batch Loss train: 0.06411197781562805\n",
            "i 121\n",
            "epoch 41\n",
            " batch Loss train: 0.059676993638277054\n",
            "i 122\n",
            "epoch 41\n",
            " batch Loss train: 0.05518551543354988\n",
            "i 123\n",
            "epoch 41\n",
            " batch Loss train: 0.06484703719615936\n",
            "i 124\n",
            "epoch 41\n",
            " batch Loss train: 0.06326579302549362\n",
            "i 125\n",
            "epoch 41\n",
            " batch Loss train: 0.05409872531890869\n",
            "i 126\n",
            "epoch 41\n",
            " batch Loss train: 0.06507188081741333\n",
            "i 127\n",
            "epoch 41\n",
            " batch Loss train: 0.056520555168390274\n",
            "i 128\n",
            "epoch 41\n",
            " batch Loss train: 0.07091471552848816\n",
            "i 129\n",
            "epoch 41\n",
            " batch Loss train: 0.07373046875\n",
            "i 130\n",
            "epoch 41\n",
            " batch Loss train: 0.06345263868570328\n",
            "i 131\n",
            "epoch 41\n",
            " batch Loss train: 0.04479168355464935\n",
            "i 132\n",
            "epoch 41\n",
            " batch Loss train: 0.07029298692941666\n",
            "i 133\n",
            "epoch 41\n",
            " batch Loss train: 0.06715265661478043\n",
            "i 134\n",
            "epoch 41\n",
            " batch Loss train: 0.06515525281429291\n",
            "i 135\n",
            "epoch 41\n",
            " batch Loss train: 0.06882022321224213\n",
            "i 136\n",
            "epoch 41\n",
            " batch Loss train: 0.06851233541965485\n",
            "i 137\n",
            "epoch 41\n",
            " batch Loss train: 0.048871781677007675\n",
            "i 138\n",
            "epoch 41\n",
            " batch Loss train: 0.0592234767973423\n",
            "i 139\n",
            "epoch 41\n",
            " batch Loss train: 0.0854094997048378\n",
            "i 140\n",
            "epoch 41\n",
            " batch Loss train: 0.04842274636030197\n",
            "i 141\n",
            "epoch 41\n",
            " batch Loss train: 0.06715892255306244\n",
            "i 142\n",
            "epoch 41\n",
            " batch Loss train: 0.0577392615377903\n",
            "i 143\n",
            "epoch 41\n",
            " batch Loss train: 0.05521420016884804\n",
            "i 144\n",
            "epoch 41\n",
            " batch Loss train: 0.05571432411670685\n",
            "i 145\n",
            "epoch 41\n",
            " batch Loss train: 0.08592405170202255\n",
            "i 146\n",
            "epoch 41\n",
            " batch Loss train: 0.06749092787504196\n",
            "i 147\n",
            "epoch 41\n",
            " batch Loss train: 0.06232917308807373\n",
            "i 148\n",
            "epoch 41\n",
            " batch Loss train: 0.048363108187913895\n",
            "i 149\n",
            "epoch 41\n",
            " batch Loss train: 0.08509400486946106\n",
            "i 150\n",
            "epoch 41\n",
            " batch Loss train: 0.04874870926141739\n",
            "i 151\n",
            "epoch 41\n",
            " batch Loss train: 0.0831572562456131\n",
            "i 152\n",
            "epoch 41\n",
            " batch Loss train: 0.06006516143679619\n",
            "i 153\n",
            "epoch 41\n",
            " batch Loss train: 0.05972161889076233\n",
            "i 154\n",
            "epoch 41\n",
            " batch Loss train: 0.06677112728357315\n",
            "i 155\n",
            "epoch 41\n",
            " batch Loss train: 0.05659544840455055\n",
            "i 156\n",
            "epoch 41\n",
            " batch Loss train: 0.05071224644780159\n",
            "i 157\n",
            "epoch 41\n",
            " batch Loss train: 0.07532379031181335\n",
            "i 158\n",
            "epoch 41\n",
            " batch Loss train: 0.06572180241346359\n",
            "i 159\n",
            "epoch 41\n",
            " batch Loss train: 0.06290490180253983\n",
            "i 160\n",
            "epoch 41\n",
            " batch Loss train: 0.0589190348982811\n",
            "i 161\n",
            "epoch 41\n",
            " batch Loss train: 0.04799891263246536\n",
            "i 162\n",
            "epoch 41\n",
            " batch Loss train: 0.09256765991449356\n",
            "i 163\n",
            "epoch 41\n",
            " batch Loss train: 0.045330941677093506\n",
            "i 164\n",
            "epoch 41\n",
            " batch Loss train: 0.048138201236724854\n",
            "i 165\n",
            "epoch 41\n",
            " batch Loss train: 0.06112921983003616\n",
            "i 166\n",
            "epoch 41\n",
            " batch Loss train: 0.0737147405743599\n",
            "i 167\n",
            "epoch 41\n",
            " batch Loss train: 0.05516143888235092\n",
            "i 168\n",
            "epoch 41\n",
            " batch Loss train: 0.058695122599601746\n",
            "i 169\n",
            "epoch 41\n",
            " batch Loss train: 0.06774988025426865\n",
            "i 170\n",
            "epoch 41\n",
            " batch Loss train: 0.0668569877743721\n",
            "i 171\n",
            "epoch 41\n",
            " batch Loss train: 0.05979720875620842\n",
            "i 172\n",
            "epoch 41\n",
            " batch Loss train: 0.05213093012571335\n",
            "i 173\n",
            "epoch 41\n",
            " batch Loss train: 0.06050208583474159\n",
            "i 174\n",
            "epoch 41\n",
            " batch Loss train: 0.05514274165034294\n",
            "i 175\n",
            "epoch 41\n",
            " batch Loss train: 0.046818703413009644\n",
            "i 176\n",
            "epoch 41\n",
            " batch Loss train: 0.07258673757314682\n",
            "i 177\n",
            "epoch 41\n",
            " batch Loss train: 0.07074030488729477\n",
            "i 178\n",
            "epoch 41\n",
            " batch Loss train: 0.06227720528841019\n",
            "i 179\n",
            "epoch 41\n",
            " batch Loss train: 0.06282579153776169\n",
            "i 180\n",
            "epoch 41\n",
            " batch Loss train: 0.07269444316625595\n",
            "i 181\n",
            "epoch 41\n",
            " batch Loss train: 0.05918186157941818\n",
            "i 182\n",
            "epoch 41\n",
            " batch Loss train: 0.054655637592077255\n",
            "i 183\n",
            "epoch 41\n",
            " batch Loss train: 0.07558496296405792\n",
            "i 184\n",
            "epoch 41\n",
            " batch Loss train: 0.07098052650690079\n",
            "i 185\n",
            "epoch 41\n",
            " batch Loss train: 0.0613129585981369\n",
            "i 186\n",
            "epoch 41\n",
            " batch Loss train: 0.07011448591947556\n",
            "i 187\n",
            "epoch 41\n",
            " batch Loss train: 0.06646792590618134\n",
            "i 188\n",
            "epoch 41\n",
            " batch Loss train: 0.06640946120023727\n",
            "i 189\n",
            "epoch 41\n",
            " batch Loss train: 0.054336607456207275\n",
            "i 190\n",
            "epoch 41\n",
            " batch Loss train: 0.056560683995485306\n",
            "i 191\n",
            "epoch 41\n",
            " batch Loss train: 0.06253858655691147\n",
            "i 192\n",
            "epoch 41\n",
            " batch Loss train: 0.09282448142766953\n",
            "i 193\n",
            "epoch 41\n",
            " batch Loss train: 0.05990414693951607\n",
            "i 194\n",
            "epoch 41\n",
            " batch Loss train: 0.07019750028848648\n",
            "i 195\n",
            "epoch 41\n",
            " batch Loss train: 0.062396373599767685\n",
            "i 196\n",
            "epoch 41\n",
            " batch Loss train: 0.051225412636995316\n",
            "i 197\n",
            "epoch 41\n",
            " batch Loss train: 0.054093580693006516\n",
            "i 198\n",
            "epoch 41\n",
            " batch Loss train: 0.044536788016557693\n",
            "i 199\n",
            "epoch 41\n",
            " batch Loss train: 0.08183976262807846\n",
            "i 200\n",
            "epoch 41\n",
            " batch Loss train: 0.060863181948661804\n",
            "i 201\n",
            "epoch 41\n",
            " batch Loss train: 0.060786955058574677\n",
            "i 202\n",
            "epoch 41\n",
            " batch Loss train: 0.08682619035243988\n",
            "i 203\n",
            "epoch 41\n",
            " batch Loss train: 0.087396539747715\n",
            "i 204\n",
            "epoch 41\n",
            " batch Loss train: 0.05989502742886543\n",
            "i 205\n",
            "epoch 41\n",
            " batch Loss train: 0.08025406301021576\n",
            "i 206\n",
            "epoch 41\n",
            " batch Loss train: 0.05913702771067619\n",
            "i 207\n",
            "epoch 41\n",
            " batch Loss train: 0.0770077258348465\n",
            "i 208\n",
            "epoch 41\n",
            " batch Loss train: 0.05245431140065193\n",
            "i 209\n",
            "epoch 41\n",
            " batch Loss train: 0.05383771285414696\n",
            "i 210\n",
            "epoch 41\n",
            " batch Loss train: 0.07139846682548523\n",
            "i 211\n",
            "epoch 41\n",
            " batch Loss train: 0.053168173879384995\n",
            "i 212\n",
            "epoch 41\n",
            " batch Loss train: 0.055544767528772354\n",
            "i 213\n",
            "epoch 41\n",
            " batch Loss train: 0.059943921864032745\n",
            "i 214\n",
            "epoch 41\n",
            " batch Loss train: 0.06897349655628204\n",
            "i 215\n",
            "epoch 41\n",
            " batch Loss train: 0.05512167140841484\n",
            "i 216\n",
            "epoch 41\n",
            " batch Loss train: 0.05736660584807396\n",
            "i 217\n",
            "epoch 41\n",
            " batch Loss train: 0.07840020209550858\n",
            "i 218\n",
            "epoch 41\n",
            " batch Loss train: 0.06465428322553635\n",
            "i 219\n",
            "epoch 41\n",
            " batch Loss train: 0.050454575568437576\n",
            "i 220\n",
            "epoch 41\n",
            " batch Loss train: 0.05698549002408981\n",
            "i 221\n",
            "epoch 41\n",
            " batch Loss train: 0.06229438632726669\n",
            "i 222\n",
            "epoch 41\n",
            " batch Loss train: 0.054818976670503616\n",
            "i 223\n",
            "epoch 41\n",
            " batch Loss train: 0.0590377114713192\n",
            "i 224\n",
            "epoch 41\n",
            " batch Loss train: 0.05590015649795532\n",
            "i 225\n",
            "epoch 41\n",
            " batch Loss train: 0.07250463962554932\n",
            "i 226\n",
            "epoch 41\n",
            " batch Loss train: 0.06749248504638672\n",
            "i 227\n",
            "epoch 41\n",
            " batch Loss train: 0.05925653874874115\n",
            "i 228\n",
            "epoch 41\n",
            " batch Loss train: 0.04964201897382736\n",
            "i 229\n",
            "epoch 41\n",
            " batch Loss train: 0.05566736310720444\n",
            "i 230\n",
            "epoch 41\n",
            " batch Loss train: 0.0709279254078865\n",
            "i 231\n",
            "epoch 41\n",
            " batch Loss train: 0.05758547782897949\n",
            "i 232\n",
            "epoch 41\n",
            " batch Loss train: 0.05232766270637512\n",
            "i 233\n",
            "epoch 41\n",
            " batch Loss train: 0.04977896809577942\n",
            "i 234\n",
            "epoch 41\n",
            " batch Loss train: 0.07722492516040802\n",
            "i 235\n",
            "epoch 41\n",
            " batch Loss train: 0.043143369257450104\n",
            "i 236\n",
            "epoch 41\n",
            " batch Loss train: 0.06731760501861572\n",
            "i 237\n",
            "epoch 41\n",
            " batch Loss train: 0.07590152323246002\n",
            "i 238\n",
            "epoch 41\n",
            " batch Loss train: 0.06265973299741745\n",
            "i 239\n",
            "epoch 41\n",
            " batch Loss train: 0.043860435485839844\n",
            "i 240\n",
            "epoch 41\n",
            " batch Loss train: 0.06221325322985649\n",
            "i 241\n",
            "epoch 41\n",
            " batch Loss train: 0.05350946635007858\n",
            "i 242\n",
            "epoch 41\n",
            " batch Loss train: 0.0617787167429924\n",
            "i 243\n",
            "epoch 41\n",
            " batch Loss train: 0.08707559108734131\n",
            "i 244\n",
            "epoch 41\n",
            " batch Loss train: 0.06248002499341965\n",
            "i 245\n",
            "epoch 41\n",
            " batch Loss train: 0.06367885321378708\n",
            "i 246\n",
            "epoch 41\n",
            " batch Loss train: 0.046855419874191284\n",
            "i 247\n",
            "epoch 41\n",
            " batch Loss train: 0.0629567950963974\n",
            "i 248\n",
            "epoch 41\n",
            " batch Loss train: 0.04981329292058945\n",
            "i 249\n",
            "epoch 41\n",
            " batch Loss train: 0.07526156306266785\n",
            "i 250\n",
            "epoch 41\n",
            " batch Loss train: 0.05834001675248146\n",
            "i 251\n",
            "epoch 41\n",
            " batch Loss train: 0.051253512501716614\n",
            "i 252\n",
            "epoch 41\n",
            " batch Loss train: 0.06245534494519234\n",
            "i 253\n",
            "epoch 41\n",
            " batch Loss train: 0.07738341391086578\n",
            "i 254\n",
            "epoch 41\n",
            " batch Loss train: 0.06056961789727211\n",
            "i 255\n",
            "epoch 41\n",
            " batch Loss train: 0.054643455892801285\n",
            "i 256\n",
            "epoch 41\n",
            " batch Loss train: 0.06986498832702637\n",
            "i 257\n",
            "epoch 41\n",
            " batch Loss train: 0.0644713044166565\n",
            "i 258\n",
            "epoch 41\n",
            " batch Loss train: 0.06613516062498093\n",
            "i 259\n",
            "epoch 41\n",
            " batch Loss train: 0.049300678074359894\n",
            "i 260\n",
            "epoch 41\n",
            " batch Loss train: 0.04703177139163017\n",
            "i 261\n",
            "epoch 41\n",
            " batch Loss train: 0.068487748503685\n",
            "i 262\n",
            "epoch 41\n",
            " batch Loss train: 0.0618368424475193\n",
            "i 263\n",
            "epoch 41\n",
            " batch Loss train: 0.07067347317934036\n",
            "i 264\n",
            "epoch 41\n",
            " batch Loss train: 0.05817374587059021\n",
            "i 265\n",
            "epoch 41\n",
            " batch Loss train: 0.06688510626554489\n",
            "i 266\n",
            "epoch 41\n",
            " batch Loss train: 0.04549408331513405\n",
            "i 267\n",
            "epoch 41\n",
            " batch Loss train: 0.06428489089012146\n",
            "i 268\n",
            "epoch 41\n",
            " batch Loss train: 0.06736942380666733\n",
            "i 269\n",
            "epoch 41\n",
            " batch Loss train: 0.06348473578691483\n",
            "i 270\n",
            "epoch 41\n",
            " batch Loss train: 0.05635671317577362\n",
            "i 271\n",
            "epoch 41\n",
            " batch Loss train: 0.08772208541631699\n",
            "i 272\n",
            "epoch 41\n",
            " batch Loss train: 0.05456443130970001\n",
            "i 273\n",
            "epoch 41\n",
            " batch Loss train: 0.08178798109292984\n",
            "i 274\n",
            "epoch 41\n",
            " batch Loss train: 0.057252008467912674\n",
            "i 275\n",
            "epoch 41\n",
            " batch Loss train: 0.05128021910786629\n",
            "i 276\n",
            "epoch 41\n",
            " batch Loss train: 0.06596226245164871\n",
            "i 277\n",
            "epoch 41\n",
            " batch Loss train: 0.05762989819049835\n",
            "i 278\n",
            "epoch 41\n",
            " batch Loss train: 0.06320983916521072\n",
            "i 279\n",
            "epoch 41\n",
            " batch Loss train: 0.09499043971300125\n",
            "i 280\n",
            "epoch 41\n",
            " batch Loss train: 0.06702039390802383\n",
            "i 281\n",
            "epoch 41\n",
            " batch Loss train: 0.05011064186692238\n",
            "i 282\n",
            "epoch 41\n",
            " batch Loss train: 0.07358191907405853\n",
            "i 283\n",
            "epoch 41\n",
            " batch Loss train: 0.0790223628282547\n",
            "i 284\n",
            "epoch 41\n",
            " batch Loss train: 0.059742819517850876\n",
            "i 285\n",
            "epoch 41\n",
            " batch Loss train: 0.052043113857507706\n",
            "i 286\n",
            "epoch 41\n",
            " batch Loss train: 0.07671206444501877\n",
            "i 287\n",
            "epoch 41\n",
            " batch Loss train: 0.06940070539712906\n",
            "i 288\n",
            "epoch 41\n",
            " batch Loss train: 0.06528092920780182\n",
            "i 289\n",
            "epoch 41\n",
            " batch Loss train: 0.07323744148015976\n",
            "i 290\n",
            "epoch 41\n",
            " batch Loss train: 0.05943625047802925\n",
            "i 291\n",
            "epoch 41\n",
            " batch Loss train: 0.06623715162277222\n",
            "i 292\n",
            "epoch 41\n",
            " batch Loss train: 0.06408106535673141\n",
            "i 293\n",
            "epoch 41\n",
            " batch Loss train: 0.06014424189925194\n",
            "i 294\n",
            "epoch 41\n",
            " batch Loss train: 0.06104159355163574\n",
            "i 295\n",
            "epoch 41\n",
            " batch Loss train: 0.07140377163887024\n",
            "i 296\n",
            "epoch 41\n",
            " batch Loss train: 0.06783023476600647\n",
            "i 297\n",
            "epoch 41\n",
            " batch Loss train: 0.06764569133520126\n",
            "i 298\n",
            "epoch 41\n",
            " batch Loss train: 0.056182458996772766\n",
            "i 299\n",
            "epoch 41\n",
            " batch Loss train: 0.08275817334651947\n",
            "i 300\n",
            "epoch 41\n",
            " batch Loss train: 0.058952659368515015\n",
            "i 301\n",
            "epoch 41\n",
            " batch Loss train: 0.05795726552605629\n",
            "i 302\n",
            "epoch 41\n",
            " batch Loss train: 0.0536176972091198\n",
            "i 303\n",
            "epoch 41\n",
            " batch Loss train: 0.06768045574426651\n",
            "i 304\n",
            "epoch 41\n",
            " batch Loss train: 0.05454500392079353\n",
            "i 305\n",
            "epoch 41\n",
            " batch Loss train: 0.05756814032793045\n",
            "i 306\n",
            "epoch 41\n",
            " batch Loss train: 0.06123651564121246\n",
            "i 307\n",
            "epoch 41\n",
            " batch Loss train: 0.07652498781681061\n",
            "i 308\n",
            "epoch 41\n",
            " batch Loss train: 0.0522737056016922\n",
            "i 309\n",
            "epoch 41\n",
            " batch Loss train: 0.07535544037818909\n",
            "i 310\n",
            "epoch 41\n",
            " batch Loss train: 0.061848364770412445\n",
            "i 311\n",
            "epoch 41\n",
            " batch Loss train: 0.05980042368173599\n",
            "i 312\n",
            "epoch 41\n",
            " batch Loss train: 0.057039931416511536\n",
            "i 313\n",
            "epoch 41\n",
            " batch Loss train: 0.06596770882606506\n",
            "i 314\n",
            "epoch 41\n",
            " batch Loss train: 0.08901190012693405\n",
            "i 315\n",
            "epoch 41\n",
            " batch Loss train: 0.0551484115421772\n",
            "i 316\n",
            "epoch 41\n",
            " batch Loss train: 0.08327944576740265\n",
            "i 317\n",
            "epoch 41\n",
            " batch Loss train: 0.05305641517043114\n",
            "i 318\n",
            "epoch 41\n",
            " batch Loss train: 0.05998633801937103\n",
            "i 319\n",
            "epoch 41\n",
            " batch Loss train: 0.053826190531253815\n",
            "i 320\n",
            "epoch 41\n",
            " batch Loss train: 0.04920260235667229\n",
            "i 321\n",
            "epoch 41\n",
            " batch Loss train: 0.07138372957706451\n",
            "i 322\n",
            "epoch 41\n",
            " batch Loss train: 0.07073584198951721\n",
            "i 323\n",
            "epoch 41\n",
            " batch Loss train: 0.05542954429984093\n",
            "i 324\n",
            "epoch 41\n",
            " batch Loss train: 0.07304634153842926\n",
            "i 325\n",
            "epoch 41\n",
            " batch Loss train: 0.057054657489061356\n",
            "i 326\n",
            "epoch 41\n",
            " batch Loss train: 0.0640287771821022\n",
            "i 327\n",
            "epoch 41\n",
            " batch Loss train: 0.059344708919525146\n",
            "i 328\n",
            "epoch 41\n",
            " batch Loss train: 0.07585246115922928\n",
            "i 329\n",
            "epoch 41\n",
            " batch Loss train: 0.08626756072044373\n",
            "i 330\n",
            "epoch 41\n",
            " batch Loss train: 0.0466068759560585\n",
            "i 331\n",
            "epoch 41\n",
            " batch Loss train: 0.05029357969760895\n",
            "i 332\n",
            "epoch 41\n",
            " batch Loss train: 0.0607929565012455\n",
            "i 333\n",
            "epoch 41\n",
            " batch Loss train: 0.0433310866355896\n",
            "i 334\n",
            "epoch 41\n",
            " batch Loss train: 0.06282537430524826\n",
            "i 335\n",
            "epoch 41\n",
            " batch Loss train: 0.0833846852183342\n",
            "i 336\n",
            "epoch 41\n",
            " batch Loss train: 0.052463632076978683\n",
            "i 337\n",
            "epoch 41\n",
            " batch Loss train: 0.06756826490163803\n",
            "i 338\n",
            "epoch 41\n",
            " batch Loss train: 0.05184300243854523\n",
            "i 339\n",
            "epoch 41\n",
            " batch Loss train: 0.0578913614153862\n",
            "i 340\n",
            "epoch 41\n",
            " batch Loss train: 0.057467542588710785\n",
            "i 341\n",
            "epoch 41\n",
            " batch Loss train: 0.06969038397073746\n",
            "i 342\n",
            "epoch 41\n",
            " batch Loss train: 0.06269236654043198\n",
            "i 343\n",
            "epoch 41\n",
            " batch Loss train: 0.08248942345380783\n",
            "i 344\n",
            "epoch 41\n",
            " batch Loss train: 0.059479206800460815\n",
            "i 345\n",
            "epoch 41\n",
            " batch Loss train: 0.09492769837379456\n",
            "i 346\n",
            "epoch 41\n",
            " batch Loss train: 0.07692794501781464\n",
            "i 347\n",
            "epoch 41\n",
            " batch Loss train: 0.09274889528751373\n",
            "i 348\n",
            "epoch 41\n",
            " batch Loss train: 0.06040048226714134\n",
            "i 349\n",
            "epoch 41\n",
            " batch Loss train: 0.04978228732943535\n",
            "i 350\n",
            "epoch 41\n",
            " batch Loss train: 0.059593625366687775\n",
            "i 351\n",
            "epoch 41\n",
            " batch Loss train: 0.05230726674199104\n",
            "i 352\n",
            "epoch 41\n",
            " batch Loss train: 0.05510123819112778\n",
            "i 353\n",
            "epoch 41\n",
            " batch Loss train: 0.05918886885046959\n",
            "i 354\n",
            "epoch 41\n",
            " batch Loss train: 0.07548365741968155\n",
            "i 355\n",
            "epoch 41\n",
            " batch Loss train: 0.07623271644115448\n",
            "i 356\n",
            "epoch 41\n",
            " batch Loss train: 0.08411741256713867\n",
            "i 357\n",
            "epoch 41\n",
            " batch Loss train: 0.0583399161696434\n",
            "i 358\n",
            "epoch 41\n",
            " batch Loss train: 0.0875135138630867\n",
            "i 359\n",
            "epoch 41\n",
            " batch Loss train: 0.06304120272397995\n",
            "i 360\n",
            "epoch 41\n",
            " batch Loss train: 0.060209643095731735\n",
            "i 361\n",
            "epoch 41\n",
            " batch Loss train: 0.05499190092086792\n",
            "i 362\n",
            "epoch 41\n",
            " batch Loss train: 0.060876261442899704\n",
            "i 363\n",
            "epoch 41\n",
            " batch Loss train: 0.05428532138466835\n",
            "i 364\n",
            "epoch 41\n",
            " batch Loss train: 0.08561732620000839\n",
            "i 365\n",
            "epoch 41\n",
            " batch Loss train: 0.06499551981687546\n",
            "i 366\n",
            "epoch 41\n",
            " batch Loss train: 0.08206808567047119\n",
            "i 367\n",
            "epoch 41\n",
            " batch Loss train: 0.06629897654056549\n",
            "i 368\n",
            "epoch 41\n",
            " batch Loss train: 0.06547620892524719\n",
            "i 369\n",
            "epoch 41\n",
            " batch Loss train: 0.07686424255371094\n",
            "i 370\n",
            "epoch 41\n",
            " batch Loss train: 0.04940243065357208\n",
            "i 371\n",
            "epoch 41\n",
            " batch Loss train: 0.07561144977807999\n",
            "i 372\n",
            "epoch 41\n",
            " batch Loss train: 0.062099143862724304\n",
            "i 373\n",
            "epoch 41\n",
            " batch Loss train: 0.06540834903717041\n",
            "i 374\n",
            "epoch 41\n",
            " batch Loss train: 0.06387322396039963\n",
            "i 375\n",
            "epoch 41\n",
            " batch Loss train: 0.07003481686115265\n",
            "i 376\n",
            "epoch 41\n",
            " batch Loss train: 0.06438978761434555\n",
            "i 377\n",
            "epoch 41\n",
            " batch Loss train: 0.06589695811271667\n",
            "i 378\n",
            "epoch 41\n",
            " batch Loss train: 0.06058942526578903\n",
            "i 379\n",
            "epoch 41\n",
            " batch Loss train: 0.05460734665393829\n",
            "i 380\n",
            "epoch 41\n",
            " batch Loss train: 0.06731545180082321\n",
            "i 381\n",
            "epoch 41\n",
            " batch Loss train: 0.07554197311401367\n",
            "i 382\n",
            "epoch 41\n",
            " batch Loss train: 0.04956962168216705\n",
            "i 383\n",
            "epoch 41\n",
            " batch Loss train: 0.06725961714982986\n",
            "i 384\n",
            "epoch 41\n",
            " batch Loss train: 0.06345953047275543\n",
            "i 385\n",
            "epoch 41\n",
            " batch Loss train: 0.06940925121307373\n",
            "i 386\n",
            "epoch 41\n",
            " batch Loss train: 0.06665382534265518\n",
            "i 387\n",
            "epoch 41\n",
            " batch Loss train: 0.05382281914353371\n",
            "i 388\n",
            "epoch 41\n",
            " batch Loss train: 0.06295129656791687\n",
            "i 389\n",
            "epoch 41\n",
            " batch Loss train: 0.04373116046190262\n",
            "i 390\n",
            "epoch 41\n",
            " batch Loss train: 0.09451010823249817\n",
            "i 391\n",
            "epoch 41\n",
            " batch Loss train: 0.06934668123722076\n",
            "i 392\n",
            "epoch 41\n",
            " batch Loss train: 0.06183105707168579\n",
            "i 393\n",
            "epoch 41\n",
            " batch Loss train: 0.04920601472258568\n",
            "i 394\n",
            "epoch 41\n",
            " batch Loss train: 0.04398774728178978\n",
            "i 395\n",
            "epoch 41\n",
            " batch Loss train: 0.06822651624679565\n",
            "i 396\n",
            "epoch 41\n",
            " batch Loss train: 0.04659951478242874\n",
            "i 397\n",
            "epoch 41\n",
            " batch Loss train: 0.05014171823859215\n",
            "i 398\n",
            "epoch 41\n",
            " batch Loss train: 0.06829378753900528\n",
            "i 399\n",
            "epoch 41\n",
            " batch Loss train: 0.07788043469190598\n",
            "i 400\n",
            "epoch 41\n",
            " batch Loss train: 0.056102205067873\n",
            "i 401\n",
            "epoch 41\n",
            " batch Loss train: 0.06863842159509659\n",
            "i 402\n",
            "epoch 41\n",
            " batch Loss train: 0.07548557966947556\n",
            "i 403\n",
            "epoch 41\n",
            " batch Loss train: 0.06851029396057129\n",
            "i 404\n",
            "epoch 41\n",
            " batch Loss train: 0.06738577038049698\n",
            "i 405\n",
            "epoch 41\n",
            " batch Loss train: 0.048729028552770615\n",
            "i 406\n",
            "epoch 41\n",
            " batch Loss train: 0.07297907024621964\n",
            "i 407\n",
            "epoch 41\n",
            " batch Loss train: 0.08790013194084167\n",
            "i 408\n",
            "epoch 41\n",
            " batch Loss train: 0.09124846011400223\n",
            "i 409\n",
            "epoch 41\n",
            " batch Loss train: 0.06161051616072655\n",
            "i 410\n",
            "epoch 41\n",
            " batch Loss train: 0.07305680960416794\n",
            "i 411\n",
            "epoch 41\n",
            " batch Loss train: 0.07906102389097214\n",
            "i 412\n",
            "epoch 41\n",
            " batch Loss train: 0.0884288102388382\n",
            "i 413\n",
            "epoch 41\n",
            " batch Loss train: 0.05426909402012825\n",
            "i 414\n",
            "epoch 41\n",
            " batch Loss train: 0.07404276728630066\n",
            "i 415\n",
            "epoch 41\n",
            " batch Loss train: 0.05825544148683548\n",
            "i 416\n",
            "epoch 41\n",
            " batch Loss train: 0.08045005798339844\n",
            "i 417\n",
            "epoch 41\n",
            " batch Loss train: 0.07994601130485535\n",
            "i 418\n",
            "epoch 41\n",
            " batch Loss train: 0.07112868875265121\n",
            "i 419\n",
            "epoch 41\n",
            " batch Loss train: 0.06099722534418106\n",
            "i 420\n",
            "epoch 41\n",
            " batch Loss train: 0.06193146854639053\n",
            "i 421\n",
            "epoch 41\n",
            " batch Loss train: 0.06491969525814056\n",
            "i 422\n",
            "epoch 41\n",
            " batch Loss train: 0.08326464891433716\n",
            "i 423\n",
            "epoch 41\n",
            " batch Loss train: 0.0756208673119545\n",
            "i 424\n",
            "epoch 41\n",
            " batch Loss train: 0.05327726528048515\n",
            "i 425\n",
            "epoch 41\n",
            " batch Loss train: 0.07357371598482132\n",
            "i 426\n",
            "epoch 41\n",
            " batch Loss train: 0.06366898864507675\n",
            "i 427\n",
            "epoch 41\n",
            " batch Loss train: 0.05576179921627045\n",
            "i 428\n",
            "epoch 41\n",
            " batch Loss train: 0.07029221206903458\n",
            "i 429\n",
            "epoch 41\n",
            " batch Loss train: 0.05362837016582489\n",
            "i 430\n",
            "epoch 41\n",
            " batch Loss train: 0.062315940856933594\n",
            "i 431\n",
            "epoch 41\n",
            " batch Loss train: 0.05156766623258591\n",
            "i 432\n",
            "epoch 41\n",
            " batch Loss train: 0.05808266997337341\n",
            "i 433\n",
            "epoch 41\n",
            " batch Loss train: 0.08024071902036667\n",
            "i 434\n",
            "epoch 41\n",
            " batch Loss train: 0.06828884035348892\n",
            "i 435\n",
            "epoch 41\n",
            " batch Loss train: 0.06972800195217133\n",
            "i 436\n",
            "epoch 41\n",
            " batch Loss train: 0.08473705500364304\n",
            "i 437\n",
            "epoch 41\n",
            " batch Loss train: 0.07087776064872742\n",
            "i 438\n",
            "epoch 41\n",
            " batch Loss train: 0.05038522556424141\n",
            "i 439\n",
            "epoch 41\n",
            " batch Loss train: 0.061662256717681885\n",
            "i 440\n",
            "epoch 41\n",
            " batch Loss train: 0.0885152593255043\n",
            "i 441\n",
            "epoch 41\n",
            " batch Loss train: 0.08567648380994797\n",
            "i 442\n",
            "epoch 41\n",
            " batch Loss train: 0.06845412403345108\n",
            "i 443\n",
            "epoch 41\n",
            " batch Loss train: 0.07484352588653564\n",
            "i 444\n",
            "epoch 41\n",
            " batch Loss train: 0.07648596912622452\n",
            "i 445\n",
            "epoch 41\n",
            " batch Loss train: 0.07797262817621231\n",
            "total epoch Loss train: tensor(0.0780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 42\n",
            " batch Loss train: 0.03861388564109802\n",
            "i 1\n",
            "epoch 42\n",
            " batch Loss train: 0.04951011389493942\n",
            "i 2\n",
            "epoch 42\n",
            " batch Loss train: 0.06695866584777832\n",
            "i 3\n",
            "epoch 42\n",
            " batch Loss train: 0.07458949089050293\n",
            "i 4\n",
            "epoch 42\n",
            " batch Loss train: 0.06655966490507126\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 42\n",
            " batch Loss train: 0.04603531211614609\n",
            "i 6\n",
            "epoch 42\n",
            " batch Loss train: 0.05281311646103859\n",
            "i 7\n",
            "epoch 42\n",
            " batch Loss train: 0.07667049020528793\n",
            "i 8\n",
            "epoch 42\n",
            " batch Loss train: 0.06026419252157211\n",
            "i 9\n",
            "epoch 42\n",
            " batch Loss train: 0.06574621796607971\n",
            "i 10\n",
            "epoch 42\n",
            " batch Loss train: 0.05883261188864708\n",
            "i 11\n",
            "epoch 42\n",
            " batch Loss train: 0.05662843957543373\n",
            "i 12\n",
            "epoch 42\n",
            " batch Loss train: 0.060583338141441345\n",
            "i 13\n",
            "epoch 42\n",
            " batch Loss train: 0.06383325159549713\n",
            "i 14\n",
            "epoch 42\n",
            " batch Loss train: 0.06704653054475784\n",
            "i 15\n",
            "epoch 42\n",
            " batch Loss train: 0.052437130361795425\n",
            "i 16\n",
            "epoch 42\n",
            " batch Loss train: 0.06565558910369873\n",
            "i 17\n",
            "epoch 42\n",
            " batch Loss train: 0.044334590435028076\n",
            "i 18\n",
            "epoch 42\n",
            " batch Loss train: 0.054770976305007935\n",
            "i 19\n",
            "epoch 42\n",
            " batch Loss train: 0.04076994210481644\n",
            "i 20\n",
            "epoch 42\n",
            " batch Loss train: 0.048730626702308655\n",
            "i 21\n",
            "epoch 42\n",
            " batch Loss train: 0.04993118718266487\n",
            "i 22\n",
            "epoch 42\n",
            " batch Loss train: 0.052503615617752075\n",
            "i 23\n",
            "epoch 42\n",
            " batch Loss train: 0.05225592106580734\n",
            "i 24\n",
            "epoch 42\n",
            " batch Loss train: 0.053027089685201645\n",
            "i 25\n",
            "epoch 42\n",
            " batch Loss train: 0.07218768447637558\n",
            "i 26\n",
            "epoch 42\n",
            " batch Loss train: 0.05983800068497658\n",
            "i 27\n",
            "epoch 42\n",
            " batch Loss train: 0.07518511265516281\n",
            "i 28\n",
            "epoch 42\n",
            " batch Loss train: 0.08426116406917572\n",
            "i 29\n",
            "epoch 42\n",
            " batch Loss train: 0.049452994018793106\n",
            "i 30\n",
            "epoch 42\n",
            " batch Loss train: 0.05359716713428497\n",
            "i 31\n",
            "epoch 42\n",
            " batch Loss train: 0.05514835938811302\n",
            "i 32\n",
            "epoch 42\n",
            " batch Loss train: 0.06010310724377632\n",
            "i 33\n",
            "epoch 42\n",
            " batch Loss train: 0.05776681378483772\n",
            "i 34\n",
            "epoch 42\n",
            " batch Loss train: 0.05651392415165901\n",
            "i 35\n",
            "epoch 42\n",
            " batch Loss train: 0.06481748074293137\n",
            "i 36\n",
            "epoch 42\n",
            " batch Loss train: 0.052696019411087036\n",
            "i 37\n",
            "epoch 42\n",
            " batch Loss train: 0.062090445309877396\n",
            "i 38\n",
            "epoch 42\n",
            " batch Loss train: 0.0789148136973381\n",
            "i 39\n",
            "epoch 42\n",
            " batch Loss train: 0.05548349767923355\n",
            "i 40\n",
            "epoch 42\n",
            " batch Loss train: 0.049573078751564026\n",
            "i 41\n",
            "epoch 42\n",
            " batch Loss train: 0.05430274456739426\n",
            "i 42\n",
            "epoch 42\n",
            " batch Loss train: 0.0578533299267292\n",
            "i 43\n",
            "epoch 42\n",
            " batch Loss train: 0.041931044310331345\n",
            "i 44\n",
            "epoch 42\n",
            " batch Loss train: 0.05336757004261017\n",
            "i 45\n",
            "epoch 42\n",
            " batch Loss train: 0.05853583291172981\n",
            "i 46\n",
            "epoch 42\n",
            " batch Loss train: 0.07726528495550156\n",
            "i 47\n",
            "epoch 42\n",
            " batch Loss train: 0.0848722755908966\n",
            "i 48\n",
            "epoch 42\n",
            " batch Loss train: 0.05569492653012276\n",
            "i 49\n",
            "epoch 42\n",
            " batch Loss train: 0.04644007980823517\n",
            "i 50\n",
            "epoch 42\n",
            " batch Loss train: 0.061258766800165176\n",
            "i 51\n",
            "epoch 42\n",
            " batch Loss train: 0.047961026430130005\n",
            "i 52\n",
            "epoch 42\n",
            " batch Loss train: 0.05371280759572983\n",
            "i 53\n",
            "epoch 42\n",
            " batch Loss train: 0.04542439803481102\n",
            "i 54\n",
            "epoch 42\n",
            " batch Loss train: 0.046836987137794495\n",
            "i 55\n",
            "epoch 42\n",
            " batch Loss train: 0.07106470316648483\n",
            "i 56\n",
            "epoch 42\n",
            " batch Loss train: 0.05869324505329132\n",
            "i 57\n",
            "epoch 42\n",
            " batch Loss train: 0.07997146248817444\n",
            "i 58\n",
            "epoch 42\n",
            " batch Loss train: 0.0700109452009201\n",
            "i 59\n",
            "epoch 42\n",
            " batch Loss train: 0.05434136465191841\n",
            "i 60\n",
            "epoch 42\n",
            " batch Loss train: 0.05238713324069977\n",
            "i 61\n",
            "epoch 42\n",
            " batch Loss train: 0.04669336974620819\n",
            "i 62\n",
            "epoch 42\n",
            " batch Loss train: 0.05500904843211174\n",
            "i 63\n",
            "epoch 42\n",
            " batch Loss train: 0.05190547555685043\n",
            "i 64\n",
            "epoch 42\n",
            " batch Loss train: 0.04657871276140213\n",
            "i 65\n",
            "epoch 42\n",
            " batch Loss train: 0.05023941397666931\n",
            "i 66\n",
            "epoch 42\n",
            " batch Loss train: 0.05654145032167435\n",
            "i 67\n",
            "epoch 42\n",
            " batch Loss train: 0.07588659971952438\n",
            "i 68\n",
            "epoch 42\n",
            " batch Loss train: 0.05256376042962074\n",
            "i 69\n",
            "epoch 42\n",
            " batch Loss train: 0.046131134033203125\n",
            "i 70\n",
            "epoch 42\n",
            " batch Loss train: 0.06288262456655502\n",
            "i 71\n",
            "epoch 42\n",
            " batch Loss train: 0.06035145744681358\n",
            "i 72\n",
            "epoch 42\n",
            " batch Loss train: 0.046582601964473724\n",
            "i 73\n",
            "epoch 42\n",
            " batch Loss train: 0.058047037571668625\n",
            "i 74\n",
            "epoch 42\n",
            " batch Loss train: 0.061837613582611084\n",
            "i 75\n",
            "epoch 42\n",
            " batch Loss train: 0.05896712467074394\n",
            "i 76\n",
            "epoch 42\n",
            " batch Loss train: 0.05932903289794922\n",
            "i 77\n",
            "epoch 42\n",
            " batch Loss train: 0.07854202389717102\n",
            "i 78\n",
            "epoch 42\n",
            " batch Loss train: 0.0631888210773468\n",
            "i 79\n",
            "epoch 42\n",
            " batch Loss train: 0.06882942467927933\n",
            "i 80\n",
            "epoch 42\n",
            " batch Loss train: 0.048808928579092026\n",
            "i 81\n",
            "epoch 42\n",
            " batch Loss train: 0.06585171818733215\n",
            "i 82\n",
            "epoch 42\n",
            " batch Loss train: 0.06741225719451904\n",
            "i 83\n",
            "epoch 42\n",
            " batch Loss train: 0.05041233450174332\n",
            "i 84\n",
            "epoch 42\n",
            " batch Loss train: 0.0558321587741375\n",
            "i 85\n",
            "epoch 42\n",
            " batch Loss train: 0.056499868631362915\n",
            "i 86\n",
            "epoch 42\n",
            " batch Loss train: 0.08450852334499359\n",
            "i 87\n",
            "epoch 42\n",
            " batch Loss train: 0.06878065317869186\n",
            "i 88\n",
            "epoch 42\n",
            " batch Loss train: 0.03381631150841713\n",
            "i 89\n",
            "epoch 42\n",
            " batch Loss train: 0.05290555581450462\n",
            "i 90\n",
            "epoch 42\n",
            " batch Loss train: 0.04663222283124924\n",
            "i 91\n",
            "epoch 42\n",
            " batch Loss train: 0.05653197690844536\n",
            "i 92\n",
            "epoch 42\n",
            " batch Loss train: 0.049504995346069336\n",
            "i 93\n",
            "epoch 42\n",
            " batch Loss train: 0.06853661686182022\n",
            "i 94\n",
            "epoch 42\n",
            " batch Loss train: 0.06165124848484993\n",
            "i 95\n",
            "epoch 42\n",
            " batch Loss train: 0.04440200328826904\n",
            "i 96\n",
            "epoch 42\n",
            " batch Loss train: 0.06808597594499588\n",
            "i 97\n",
            "epoch 42\n",
            " batch Loss train: 0.04632612317800522\n",
            "i 98\n",
            "epoch 42\n",
            " batch Loss train: 0.056953076273202896\n",
            "i 99\n",
            "epoch 42\n",
            " batch Loss train: 0.0657077357172966\n",
            "i 100\n",
            "epoch 42\n",
            " batch Loss train: 0.042836010456085205\n",
            "i 101\n",
            "epoch 42\n",
            " batch Loss train: 0.051015499979257584\n",
            "i 102\n",
            "epoch 42\n",
            " batch Loss train: 0.06487655639648438\n",
            "i 103\n",
            "epoch 42\n",
            " batch Loss train: 0.04951771721243858\n",
            "i 104\n",
            "epoch 42\n",
            " batch Loss train: 0.06779536604881287\n",
            "i 105\n",
            "epoch 42\n",
            " batch Loss train: 0.05075370892882347\n",
            "i 106\n",
            "epoch 42\n",
            " batch Loss train: 0.06077602505683899\n",
            "i 107\n",
            "epoch 42\n",
            " batch Loss train: 0.06334969401359558\n",
            "i 108\n",
            "epoch 42\n",
            " batch Loss train: 0.04686955362558365\n",
            "i 109\n",
            "epoch 42\n",
            " batch Loss train: 0.058684054762125015\n",
            "i 110\n",
            "epoch 42\n",
            " batch Loss train: 0.05066125467419624\n",
            "i 111\n",
            "epoch 42\n",
            " batch Loss train: 0.057428620755672455\n",
            "i 112\n",
            "epoch 42\n",
            " batch Loss train: 0.05857822671532631\n",
            "i 113\n",
            "epoch 42\n",
            " batch Loss train: 0.06787589937448502\n",
            "i 114\n",
            "epoch 42\n",
            " batch Loss train: 0.04622047394514084\n",
            "i 115\n",
            "epoch 42\n",
            " batch Loss train: 0.07826674729585648\n",
            "i 116\n",
            "epoch 42\n",
            " batch Loss train: 0.0845293179154396\n",
            "i 117\n",
            "epoch 42\n",
            " batch Loss train: 0.06554405391216278\n",
            "i 118\n",
            "epoch 42\n",
            " batch Loss train: 0.08648084849119186\n",
            "i 119\n",
            "epoch 42\n",
            " batch Loss train: 0.051686596125364304\n",
            "i 120\n",
            "epoch 42\n",
            " batch Loss train: 0.06847282499074936\n",
            "i 121\n",
            "epoch 42\n",
            " batch Loss train: 0.05392840877175331\n",
            "i 122\n",
            "epoch 42\n",
            " batch Loss train: 0.040945615619421005\n",
            "i 123\n",
            "epoch 42\n",
            " batch Loss train: 0.061849940568208694\n",
            "i 124\n",
            "epoch 42\n",
            " batch Loss train: 0.07987147569656372\n",
            "i 125\n",
            "epoch 42\n",
            " batch Loss train: 0.04547091946005821\n",
            "i 126\n",
            "epoch 42\n",
            " batch Loss train: 0.06098172068595886\n",
            "i 127\n",
            "epoch 42\n",
            " batch Loss train: 0.07351481169462204\n",
            "i 128\n",
            "epoch 42\n",
            " batch Loss train: 0.05645978823304176\n",
            "i 129\n",
            "epoch 42\n",
            " batch Loss train: 0.051707640290260315\n",
            "i 130\n",
            "epoch 42\n",
            " batch Loss train: 0.0520232655107975\n",
            "i 131\n",
            "epoch 42\n",
            " batch Loss train: 0.059582196176052094\n",
            "i 132\n",
            "epoch 42\n",
            " batch Loss train: 0.07315747439861298\n",
            "i 133\n",
            "epoch 42\n",
            " batch Loss train: 0.0798986479640007\n",
            "i 134\n",
            "epoch 42\n",
            " batch Loss train: 0.06946218758821487\n",
            "i 135\n",
            "epoch 42\n",
            " batch Loss train: 0.06852124631404877\n",
            "i 136\n",
            "epoch 42\n",
            " batch Loss train: 0.054465293884277344\n",
            "i 137\n",
            "epoch 42\n",
            " batch Loss train: 0.06600448489189148\n",
            "i 138\n",
            "epoch 42\n",
            " batch Loss train: 0.050138406455516815\n",
            "i 139\n",
            "epoch 42\n",
            " batch Loss train: 0.05933031067252159\n",
            "i 140\n",
            "epoch 42\n",
            " batch Loss train: 0.057960569858551025\n",
            "i 141\n",
            "epoch 42\n",
            " batch Loss train: 0.07039272040128708\n",
            "i 142\n",
            "epoch 42\n",
            " batch Loss train: 0.05396931618452072\n",
            "i 143\n",
            "epoch 42\n",
            " batch Loss train: 0.054426372051239014\n",
            "i 144\n",
            "epoch 42\n",
            " batch Loss train: 0.06771481782197952\n",
            "i 145\n",
            "epoch 42\n",
            " batch Loss train: 0.06307683140039444\n",
            "i 146\n",
            "epoch 42\n",
            " batch Loss train: 0.05661996081471443\n",
            "i 147\n",
            "epoch 42\n",
            " batch Loss train: 0.04929167404770851\n",
            "i 148\n",
            "epoch 42\n",
            " batch Loss train: 0.05941141024231911\n",
            "i 149\n",
            "epoch 42\n",
            " batch Loss train: 0.07470927387475967\n",
            "i 150\n",
            "epoch 42\n",
            " batch Loss train: 0.050793495029211044\n",
            "i 151\n",
            "epoch 42\n",
            " batch Loss train: 0.053851377218961716\n",
            "i 152\n",
            "epoch 42\n",
            " batch Loss train: 0.05371358245611191\n",
            "i 153\n",
            "epoch 42\n",
            " batch Loss train: 0.06033916026353836\n",
            "i 154\n",
            "epoch 42\n",
            " batch Loss train: 0.05410854145884514\n",
            "i 155\n",
            "epoch 42\n",
            " batch Loss train: 0.05945825204253197\n",
            "i 156\n",
            "epoch 42\n",
            " batch Loss train: 0.04433345049619675\n",
            "i 157\n",
            "epoch 42\n",
            " batch Loss train: 0.051736634224653244\n",
            "i 158\n",
            "epoch 42\n",
            " batch Loss train: 0.06483764201402664\n",
            "i 159\n",
            "epoch 42\n",
            " batch Loss train: 0.043136823922395706\n",
            "i 160\n",
            "epoch 42\n",
            " batch Loss train: 0.07451193779706955\n",
            "i 161\n",
            "epoch 42\n",
            " batch Loss train: 0.053236525505781174\n",
            "i 162\n",
            "epoch 42\n",
            " batch Loss train: 0.04717317968606949\n",
            "i 163\n",
            "epoch 42\n",
            " batch Loss train: 0.07756688445806503\n",
            "i 164\n",
            "epoch 42\n",
            " batch Loss train: 0.06567313522100449\n",
            "i 165\n",
            "epoch 42\n",
            " batch Loss train: 0.073837511241436\n",
            "i 166\n",
            "epoch 42\n",
            " batch Loss train: 0.063609778881073\n",
            "i 167\n",
            "epoch 42\n",
            " batch Loss train: 0.05970277264714241\n",
            "i 168\n",
            "epoch 42\n",
            " batch Loss train: 0.05003445968031883\n",
            "i 169\n",
            "epoch 42\n",
            " batch Loss train: 0.07197710871696472\n",
            "i 170\n",
            "epoch 42\n",
            " batch Loss train: 0.0591476745903492\n",
            "i 171\n",
            "epoch 42\n",
            " batch Loss train: 0.06605958193540573\n",
            "i 172\n",
            "epoch 42\n",
            " batch Loss train: 0.04977772757411003\n",
            "i 173\n",
            "epoch 42\n",
            " batch Loss train: 0.06177884340286255\n",
            "i 174\n",
            "epoch 42\n",
            " batch Loss train: 0.05611523985862732\n",
            "i 175\n",
            "epoch 42\n",
            " batch Loss train: 0.05618715658783913\n",
            "i 176\n",
            "epoch 42\n",
            " batch Loss train: 0.05788725987076759\n",
            "i 177\n",
            "epoch 42\n",
            " batch Loss train: 0.0916486456990242\n",
            "i 178\n",
            "epoch 42\n",
            " batch Loss train: 0.07212654501199722\n",
            "i 179\n",
            "epoch 42\n",
            " batch Loss train: 0.05135183408856392\n",
            "i 180\n",
            "epoch 42\n",
            " batch Loss train: 0.08233246952295303\n",
            "i 181\n",
            "epoch 42\n",
            " batch Loss train: 0.07755982130765915\n",
            "i 182\n",
            "epoch 42\n",
            " batch Loss train: 0.06105925515294075\n",
            "i 183\n",
            "epoch 42\n",
            " batch Loss train: 0.04953613877296448\n",
            "i 184\n",
            "epoch 42\n",
            " batch Loss train: 0.059655528515577316\n",
            "i 185\n",
            "epoch 42\n",
            " batch Loss train: 0.06137130782008171\n",
            "i 186\n",
            "epoch 42\n",
            " batch Loss train: 0.07825221866369247\n",
            "i 187\n",
            "epoch 42\n",
            " batch Loss train: 0.07399336993694305\n",
            "i 188\n",
            "epoch 42\n",
            " batch Loss train: 0.04046880081295967\n",
            "i 189\n",
            "epoch 42\n",
            " batch Loss train: 0.06496495008468628\n",
            "i 190\n",
            "epoch 42\n",
            " batch Loss train: 0.07503384351730347\n",
            "i 191\n",
            "epoch 42\n",
            " batch Loss train: 0.06832220405340195\n",
            "i 192\n",
            "epoch 42\n",
            " batch Loss train: 0.06143050268292427\n",
            "i 193\n",
            "epoch 42\n",
            " batch Loss train: 0.05908763036131859\n",
            "i 194\n",
            "epoch 42\n",
            " batch Loss train: 0.059014443308115005\n",
            "i 195\n",
            "epoch 42\n",
            " batch Loss train: 0.08172225952148438\n",
            "i 196\n",
            "epoch 42\n",
            " batch Loss train: 0.055616285651922226\n",
            "i 197\n",
            "epoch 42\n",
            " batch Loss train: 0.07661806792020798\n",
            "i 198\n",
            "epoch 42\n",
            " batch Loss train: 0.06207502633333206\n",
            "i 199\n",
            "epoch 42\n",
            " batch Loss train: 0.07309134304523468\n",
            "i 200\n",
            "epoch 42\n",
            " batch Loss train: 0.04783592373132706\n",
            "i 201\n",
            "epoch 42\n",
            " batch Loss train: 0.0577116496860981\n",
            "i 202\n",
            "epoch 42\n",
            " batch Loss train: 0.07764449715614319\n",
            "i 203\n",
            "epoch 42\n",
            " batch Loss train: 0.04895968362689018\n",
            "i 204\n",
            "epoch 42\n",
            " batch Loss train: 0.05600113794207573\n",
            "i 205\n",
            "epoch 42\n",
            " batch Loss train: 0.045750655233860016\n",
            "i 206\n",
            "epoch 42\n",
            " batch Loss train: 0.056839536875486374\n",
            "i 207\n",
            "epoch 42\n",
            " batch Loss train: 0.05691816285252571\n",
            "i 208\n",
            "epoch 42\n",
            " batch Loss train: 0.07651170343160629\n",
            "i 209\n",
            "epoch 42\n",
            " batch Loss train: 0.062487244606018066\n",
            "i 210\n",
            "epoch 42\n",
            " batch Loss train: 0.06678220629692078\n",
            "i 211\n",
            "epoch 42\n",
            " batch Loss train: 0.07733431458473206\n",
            "i 212\n",
            "epoch 42\n",
            " batch Loss train: 0.056480202823877335\n",
            "i 213\n",
            "epoch 42\n",
            " batch Loss train: 0.0685681402683258\n",
            "i 214\n",
            "epoch 42\n",
            " batch Loss train: 0.05712367966771126\n",
            "i 215\n",
            "epoch 42\n",
            " batch Loss train: 0.06988421827554703\n",
            "i 216\n",
            "epoch 42\n",
            " batch Loss train: 0.06439159065485\n",
            "i 217\n",
            "epoch 42\n",
            " batch Loss train: 0.06072161719202995\n",
            "i 218\n",
            "epoch 42\n",
            " batch Loss train: 0.05049409344792366\n",
            "i 219\n",
            "epoch 42\n",
            " batch Loss train: 0.06241333857178688\n",
            "i 220\n",
            "epoch 42\n",
            " batch Loss train: 0.07418309897184372\n",
            "i 221\n",
            "epoch 42\n",
            " batch Loss train: 0.05500714108347893\n",
            "i 222\n",
            "epoch 42\n",
            " batch Loss train: 0.054280348122119904\n",
            "i 223\n",
            "epoch 42\n",
            " batch Loss train: 0.0711800679564476\n",
            "i 224\n",
            "epoch 42\n",
            " batch Loss train: 0.0763087198138237\n",
            "i 225\n",
            "epoch 42\n",
            " batch Loss train: 0.06560006737709045\n",
            "i 226\n",
            "epoch 42\n",
            " batch Loss train: 0.06358848512172699\n",
            "i 227\n",
            "epoch 42\n",
            " batch Loss train: 0.05173947289586067\n",
            "i 228\n",
            "epoch 42\n",
            " batch Loss train: 0.05842932313680649\n",
            "i 229\n",
            "epoch 42\n",
            " batch Loss train: 0.0651644840836525\n",
            "i 230\n",
            "epoch 42\n",
            " batch Loss train: 0.08568470180034637\n",
            "i 231\n",
            "epoch 42\n",
            " batch Loss train: 0.07100056111812592\n",
            "i 232\n",
            "epoch 42\n",
            " batch Loss train: 0.06074362248182297\n",
            "i 233\n",
            "epoch 42\n",
            " batch Loss train: 0.052550774067640305\n",
            "i 234\n",
            "epoch 42\n",
            " batch Loss train: 0.07324127107858658\n",
            "i 235\n",
            "epoch 42\n",
            " batch Loss train: 0.05217839404940605\n",
            "i 236\n",
            "epoch 42\n",
            " batch Loss train: 0.05229487642645836\n",
            "i 237\n",
            "epoch 42\n",
            " batch Loss train: 0.06483171135187149\n",
            "i 238\n",
            "epoch 42\n",
            " batch Loss train: 0.061955615878105164\n",
            "i 239\n",
            "epoch 42\n",
            " batch Loss train: 0.0536799356341362\n",
            "i 240\n",
            "epoch 42\n",
            " batch Loss train: 0.0740833729505539\n",
            "i 241\n",
            "epoch 42\n",
            " batch Loss train: 0.06991200894117355\n",
            "i 242\n",
            "epoch 42\n",
            " batch Loss train: 0.06854982674121857\n",
            "i 243\n",
            "epoch 42\n",
            " batch Loss train: 0.0663648471236229\n",
            "i 244\n",
            "epoch 42\n",
            " batch Loss train: 0.0715780183672905\n",
            "i 245\n",
            "epoch 42\n",
            " batch Loss train: 0.04608083888888359\n",
            "i 246\n",
            "epoch 42\n",
            " batch Loss train: 0.06475432962179184\n",
            "i 247\n",
            "epoch 42\n",
            " batch Loss train: 0.06103430688381195\n",
            "i 248\n",
            "epoch 42\n",
            " batch Loss train: 0.05811818689107895\n",
            "i 249\n",
            "epoch 42\n",
            " batch Loss train: 0.055962953716516495\n",
            "i 250\n",
            "epoch 42\n",
            " batch Loss train: 0.066879041492939\n",
            "i 251\n",
            "epoch 42\n",
            " batch Loss train: 0.04989184811711311\n",
            "i 252\n",
            "epoch 42\n",
            " batch Loss train: 0.06275280565023422\n",
            "i 253\n",
            "epoch 42\n",
            " batch Loss train: 0.06569288671016693\n",
            "i 254\n",
            "epoch 42\n",
            " batch Loss train: 0.06836024671792984\n",
            "i 255\n",
            "epoch 42\n",
            " batch Loss train: 0.08516226708889008\n",
            "i 256\n",
            "epoch 42\n",
            " batch Loss train: 0.0746743381023407\n",
            "i 257\n",
            "epoch 42\n",
            " batch Loss train: 0.06238248944282532\n",
            "i 258\n",
            "epoch 42\n",
            " batch Loss train: 0.06450187414884567\n",
            "i 259\n",
            "epoch 42\n",
            " batch Loss train: 0.07796464115381241\n",
            "i 260\n",
            "epoch 42\n",
            " batch Loss train: 0.05577801540493965\n",
            "i 261\n",
            "epoch 42\n",
            " batch Loss train: 0.0711505115032196\n",
            "i 262\n",
            "epoch 42\n",
            " batch Loss train: 0.045917727053165436\n",
            "i 263\n",
            "epoch 42\n",
            " batch Loss train: 0.06043518707156181\n",
            "i 264\n",
            "epoch 42\n",
            " batch Loss train: 0.05554323270916939\n",
            "i 265\n",
            "epoch 42\n",
            " batch Loss train: 0.06200093775987625\n",
            "i 266\n",
            "epoch 42\n",
            " batch Loss train: 0.05185162276029587\n",
            "i 267\n",
            "epoch 42\n",
            " batch Loss train: 0.054226104170084\n",
            "i 268\n",
            "epoch 42\n",
            " batch Loss train: 0.05161888152360916\n",
            "i 269\n",
            "epoch 42\n",
            " batch Loss train: 0.05158541351556778\n",
            "i 270\n",
            "epoch 42\n",
            " batch Loss train: 0.08834819495677948\n",
            "i 271\n",
            "epoch 42\n",
            " batch Loss train: 0.06202045828104019\n",
            "i 272\n",
            "epoch 42\n",
            " batch Loss train: 0.05156116932630539\n",
            "i 273\n",
            "epoch 42\n",
            " batch Loss train: 0.09525788575410843\n",
            "i 274\n",
            "epoch 42\n",
            " batch Loss train: 0.05825207754969597\n",
            "i 275\n",
            "epoch 42\n",
            " batch Loss train: 0.053360506892204285\n",
            "i 276\n",
            "epoch 42\n",
            " batch Loss train: 0.08289244771003723\n",
            "i 277\n",
            "epoch 42\n",
            " batch Loss train: 0.07229141891002655\n",
            "i 278\n",
            "epoch 42\n",
            " batch Loss train: 0.07012514770030975\n",
            "i 279\n",
            "epoch 42\n",
            " batch Loss train: 0.057802751660346985\n",
            "i 280\n",
            "epoch 42\n",
            " batch Loss train: 0.04975951835513115\n",
            "i 281\n",
            "epoch 42\n",
            " batch Loss train: 0.05227576941251755\n",
            "i 282\n",
            "epoch 42\n",
            " batch Loss train: 0.05701233446598053\n",
            "i 283\n",
            "epoch 42\n",
            " batch Loss train: 0.08253858983516693\n",
            "i 284\n",
            "epoch 42\n",
            " batch Loss train: 0.052057359367609024\n",
            "i 285\n",
            "epoch 42\n",
            " batch Loss train: 0.06409937143325806\n",
            "i 286\n",
            "epoch 42\n",
            " batch Loss train: 0.06481759995222092\n",
            "i 287\n",
            "epoch 42\n",
            " batch Loss train: 0.07421966642141342\n",
            "i 288\n",
            "epoch 42\n",
            " batch Loss train: 0.05100809410214424\n",
            "i 289\n",
            "epoch 42\n",
            " batch Loss train: 0.05894112214446068\n",
            "i 290\n",
            "epoch 42\n",
            " batch Loss train: 0.0607825368642807\n",
            "i 291\n",
            "epoch 42\n",
            " batch Loss train: 0.07353471964597702\n",
            "i 292\n",
            "epoch 42\n",
            " batch Loss train: 0.05970500782132149\n",
            "i 293\n",
            "epoch 42\n",
            " batch Loss train: 0.07051981985569\n",
            "i 294\n",
            "epoch 42\n",
            " batch Loss train: 0.0462040938436985\n",
            "i 295\n",
            "epoch 42\n",
            " batch Loss train: 0.07023432850837708\n",
            "i 296\n",
            "epoch 42\n",
            " batch Loss train: 0.07306168228387833\n",
            "i 297\n",
            "epoch 42\n",
            " batch Loss train: 0.08470824360847473\n",
            "i 298\n",
            "epoch 42\n",
            " batch Loss train: 0.07006938755512238\n",
            "i 299\n",
            "epoch 42\n",
            " batch Loss train: 0.061429738998413086\n",
            "i 300\n",
            "epoch 42\n",
            " batch Loss train: 0.07116711139678955\n",
            "i 301\n",
            "epoch 42\n",
            " batch Loss train: 0.059414755553007126\n",
            "i 302\n",
            "epoch 42\n",
            " batch Loss train: 0.06902382522821426\n",
            "i 303\n",
            "epoch 42\n",
            " batch Loss train: 0.06803511083126068\n",
            "i 304\n",
            "epoch 42\n",
            " batch Loss train: 0.05714239925146103\n",
            "i 305\n",
            "epoch 42\n",
            " batch Loss train: 0.0584300197660923\n",
            "i 306\n",
            "epoch 42\n",
            " batch Loss train: 0.051950592547655106\n",
            "i 307\n",
            "epoch 42\n",
            " batch Loss train: 0.0617968812584877\n",
            "i 308\n",
            "epoch 42\n",
            " batch Loss train: 0.06049391254782677\n",
            "i 309\n",
            "epoch 42\n",
            " batch Loss train: 0.06593991816043854\n",
            "i 310\n",
            "epoch 42\n",
            " batch Loss train: 0.06123660132288933\n",
            "i 311\n",
            "epoch 42\n",
            " batch Loss train: 0.050085749477148056\n",
            "i 312\n",
            "epoch 42\n",
            " batch Loss train: 0.06076737120747566\n",
            "i 313\n",
            "epoch 42\n",
            " batch Loss train: 0.05629180371761322\n",
            "i 314\n",
            "epoch 42\n",
            " batch Loss train: 0.058388978242874146\n",
            "i 315\n",
            "epoch 42\n",
            " batch Loss train: 0.06134409457445145\n",
            "i 316\n",
            "epoch 42\n",
            " batch Loss train: 0.058668531477451324\n",
            "i 317\n",
            "epoch 42\n",
            " batch Loss train: 0.04285256192088127\n",
            "i 318\n",
            "epoch 42\n",
            " batch Loss train: 0.06903005391359329\n",
            "i 319\n",
            "epoch 42\n",
            " batch Loss train: 0.07220692932605743\n",
            "i 320\n",
            "epoch 42\n",
            " batch Loss train: 0.07053473591804504\n",
            "i 321\n",
            "epoch 42\n",
            " batch Loss train: 0.08543133735656738\n",
            "i 322\n",
            "epoch 42\n",
            " batch Loss train: 0.06327083706855774\n",
            "i 323\n",
            "epoch 42\n",
            " batch Loss train: 0.06522836536169052\n",
            "i 324\n",
            "epoch 42\n",
            " batch Loss train: 0.05368829891085625\n",
            "i 325\n",
            "epoch 42\n",
            " batch Loss train: 0.06294512748718262\n",
            "i 326\n",
            "epoch 42\n",
            " batch Loss train: 0.07501982897520065\n",
            "i 327\n",
            "epoch 42\n",
            " batch Loss train: 0.07802744954824448\n",
            "i 328\n",
            "epoch 42\n",
            " batch Loss train: 0.05219664052128792\n",
            "i 329\n",
            "epoch 42\n",
            " batch Loss train: 0.058102186769247055\n",
            "i 330\n",
            "epoch 42\n",
            " batch Loss train: 0.05652967095375061\n",
            "i 331\n",
            "epoch 42\n",
            " batch Loss train: 0.050387706607580185\n",
            "i 332\n",
            "epoch 42\n",
            " batch Loss train: 0.06254131346940994\n",
            "i 333\n",
            "epoch 42\n",
            " batch Loss train: 0.07108838111162186\n",
            "i 334\n",
            "epoch 42\n",
            " batch Loss train: 0.08990634232759476\n",
            "i 335\n",
            "epoch 42\n",
            " batch Loss train: 0.05547298491001129\n",
            "i 336\n",
            "epoch 42\n",
            " batch Loss train: 0.06536031514406204\n",
            "i 337\n",
            "epoch 42\n",
            " batch Loss train: 0.051366835832595825\n",
            "i 338\n",
            "epoch 42\n",
            " batch Loss train: 0.10290729254484177\n",
            "i 339\n",
            "epoch 42\n",
            " batch Loss train: 0.05038494989275932\n",
            "i 340\n",
            "epoch 42\n",
            " batch Loss train: 0.05149349570274353\n",
            "i 341\n",
            "epoch 42\n",
            " batch Loss train: 0.05683568865060806\n",
            "i 342\n",
            "epoch 42\n",
            " batch Loss train: 0.05553584545850754\n",
            "i 343\n",
            "epoch 42\n",
            " batch Loss train: 0.06339799612760544\n",
            "i 344\n",
            "epoch 42\n",
            " batch Loss train: 0.06978629529476166\n",
            "i 345\n",
            "epoch 42\n",
            " batch Loss train: 0.1294863373041153\n",
            "i 346\n",
            "epoch 42\n",
            " batch Loss train: 0.06420863419771194\n",
            "i 347\n",
            "epoch 42\n",
            " batch Loss train: 0.05832117795944214\n",
            "i 348\n",
            "epoch 42\n",
            " batch Loss train: 0.060548439621925354\n",
            "i 349\n",
            "epoch 42\n",
            " batch Loss train: 0.0679432824254036\n",
            "i 350\n",
            "epoch 42\n",
            " batch Loss train: 0.06979023665189743\n",
            "i 351\n",
            "epoch 42\n",
            " batch Loss train: 0.0772876963019371\n",
            "i 352\n",
            "epoch 42\n",
            " batch Loss train: 0.07427246123552322\n",
            "i 353\n",
            "epoch 42\n",
            " batch Loss train: 0.06293778121471405\n",
            "i 354\n",
            "epoch 42\n",
            " batch Loss train: 0.07309962809085846\n",
            "i 355\n",
            "epoch 42\n",
            " batch Loss train: 0.06580290198326111\n",
            "i 356\n",
            "epoch 42\n",
            " batch Loss train: 0.06885311752557755\n",
            "i 357\n",
            "epoch 42\n",
            " batch Loss train: 0.06294554471969604\n",
            "i 358\n",
            "epoch 42\n",
            " batch Loss train: 0.060672543942928314\n",
            "i 359\n",
            "epoch 42\n",
            " batch Loss train: 0.05323680862784386\n",
            "i 360\n",
            "epoch 42\n",
            " batch Loss train: 0.0624811053276062\n",
            "i 361\n",
            "epoch 42\n",
            " batch Loss train: 0.08204599469900131\n",
            "i 362\n",
            "epoch 42\n",
            " batch Loss train: 0.06768057495355606\n",
            "i 363\n",
            "epoch 42\n",
            " batch Loss train: 0.06174302101135254\n",
            "i 364\n",
            "epoch 42\n",
            " batch Loss train: 0.07951338589191437\n",
            "i 365\n",
            "epoch 42\n",
            " batch Loss train: 0.05411233380436897\n",
            "i 366\n",
            "epoch 42\n",
            " batch Loss train: 0.07319958508014679\n",
            "i 367\n",
            "epoch 42\n",
            " batch Loss train: 0.06310161203145981\n",
            "i 368\n",
            "epoch 42\n",
            " batch Loss train: 0.04758211970329285\n",
            "i 369\n",
            "epoch 42\n",
            " batch Loss train: 0.07970558106899261\n",
            "i 370\n",
            "epoch 42\n",
            " batch Loss train: 0.06497065722942352\n",
            "i 371\n",
            "epoch 42\n",
            " batch Loss train: 0.053679458796978\n",
            "i 372\n",
            "epoch 42\n",
            " batch Loss train: 0.06278663873672485\n",
            "i 373\n",
            "epoch 42\n",
            " batch Loss train: 0.06908509135246277\n",
            "i 374\n",
            "epoch 42\n",
            " batch Loss train: 0.06972451508045197\n",
            "i 375\n",
            "epoch 42\n",
            " batch Loss train: 0.07247122377157211\n",
            "i 376\n",
            "epoch 42\n",
            " batch Loss train: 0.05798978731036186\n",
            "i 377\n",
            "epoch 42\n",
            " batch Loss train: 0.08181877434253693\n",
            "i 378\n",
            "epoch 42\n",
            " batch Loss train: 0.054057713598012924\n",
            "i 379\n",
            "epoch 42\n",
            " batch Loss train: 0.0719548687338829\n",
            "i 380\n",
            "epoch 42\n",
            " batch Loss train: 0.06133582815527916\n",
            "i 381\n",
            "epoch 42\n",
            " batch Loss train: 0.05191319063305855\n",
            "i 382\n",
            "epoch 42\n",
            " batch Loss train: 0.05530950427055359\n",
            "i 383\n",
            "epoch 42\n",
            " batch Loss train: 0.06552693992853165\n",
            "i 384\n",
            "epoch 42\n",
            " batch Loss train: 0.05781354010105133\n",
            "i 385\n",
            "epoch 42\n",
            " batch Loss train: 0.07149901241064072\n",
            "i 386\n",
            "epoch 42\n",
            " batch Loss train: 0.06190747022628784\n",
            "i 387\n",
            "epoch 42\n",
            " batch Loss train: 0.07474315166473389\n",
            "i 388\n",
            "epoch 42\n",
            " batch Loss train: 0.09143465757369995\n",
            "i 389\n",
            "epoch 42\n",
            " batch Loss train: 0.05301126465201378\n",
            "i 390\n",
            "epoch 42\n",
            " batch Loss train: 0.05183346942067146\n",
            "i 391\n",
            "epoch 42\n",
            " batch Loss train: 0.08928197622299194\n",
            "i 392\n",
            "epoch 42\n",
            " batch Loss train: 0.08274509012699127\n",
            "i 393\n",
            "epoch 42\n",
            " batch Loss train: 0.06965237855911255\n",
            "i 394\n",
            "epoch 42\n",
            " batch Loss train: 0.07583264261484146\n",
            "i 395\n",
            "epoch 42\n",
            " batch Loss train: 0.04846014827489853\n",
            "i 396\n",
            "epoch 42\n",
            " batch Loss train: 0.05865718051791191\n",
            "i 397\n",
            "epoch 42\n",
            " batch Loss train: 0.04795201122760773\n",
            "i 398\n",
            "epoch 42\n",
            " batch Loss train: 0.06567558646202087\n",
            "i 399\n",
            "epoch 42\n",
            " batch Loss train: 0.08425228297710419\n",
            "i 400\n",
            "epoch 42\n",
            " batch Loss train: 0.05063507333397865\n",
            "i 401\n",
            "epoch 42\n",
            " batch Loss train: 0.07236067950725555\n",
            "i 402\n",
            "epoch 42\n",
            " batch Loss train: 0.0939585492014885\n",
            "i 403\n",
            "epoch 42\n",
            " batch Loss train: 0.07797449082136154\n",
            "i 404\n",
            "epoch 42\n",
            " batch Loss train: 0.06565019488334656\n",
            "i 405\n",
            "epoch 42\n",
            " batch Loss train: 0.0627288967370987\n",
            "i 406\n",
            "epoch 42\n",
            " batch Loss train: 0.06127244234085083\n",
            "i 407\n",
            "epoch 42\n",
            " batch Loss train: 0.08958505839109421\n",
            "i 408\n",
            "epoch 42\n",
            " batch Loss train: 0.061414532363414764\n",
            "i 409\n",
            "epoch 42\n",
            " batch Loss train: 0.061889488250017166\n",
            "i 410\n",
            "epoch 42\n",
            " batch Loss train: 0.0575270801782608\n",
            "i 411\n",
            "epoch 42\n",
            " batch Loss train: 0.07357913255691528\n",
            "i 412\n",
            "epoch 42\n",
            " batch Loss train: 0.0604400560259819\n",
            "i 413\n",
            "epoch 42\n",
            " batch Loss train: 0.06307531893253326\n",
            "i 414\n",
            "epoch 42\n",
            " batch Loss train: 0.07869204878807068\n",
            "i 415\n",
            "epoch 42\n",
            " batch Loss train: 0.05935066565871239\n",
            "i 416\n",
            "epoch 42\n",
            " batch Loss train: 0.0802096277475357\n",
            "i 417\n",
            "epoch 42\n",
            " batch Loss train: 0.07255760580301285\n",
            "i 418\n",
            "epoch 42\n",
            " batch Loss train: 0.07223398983478546\n",
            "i 419\n",
            "epoch 42\n",
            " batch Loss train: 0.06322207301855087\n",
            "i 420\n",
            "epoch 42\n",
            " batch Loss train: 0.07664312422275543\n",
            "i 421\n",
            "epoch 42\n",
            " batch Loss train: 0.05631593242287636\n",
            "i 422\n",
            "epoch 42\n",
            " batch Loss train: 0.05543477460741997\n",
            "i 423\n",
            "epoch 42\n",
            " batch Loss train: 0.09247633814811707\n",
            "i 424\n",
            "epoch 42\n",
            " batch Loss train: 0.05404457449913025\n",
            "i 425\n",
            "epoch 42\n",
            " batch Loss train: 0.07297943532466888\n",
            "i 426\n",
            "epoch 42\n",
            " batch Loss train: 0.05531935766339302\n",
            "i 427\n",
            "epoch 42\n",
            " batch Loss train: 0.061706095933914185\n",
            "i 428\n",
            "epoch 42\n",
            " batch Loss train: 0.08628655970096588\n",
            "i 429\n",
            "epoch 42\n",
            " batch Loss train: 0.07108592242002487\n",
            "i 430\n",
            "epoch 42\n",
            " batch Loss train: 0.0908430740237236\n",
            "i 431\n",
            "epoch 42\n",
            " batch Loss train: 0.06263404339551926\n",
            "i 432\n",
            "epoch 42\n",
            " batch Loss train: 0.06058286502957344\n",
            "i 433\n",
            "epoch 42\n",
            " batch Loss train: 0.05174235627055168\n",
            "i 434\n",
            "epoch 42\n",
            " batch Loss train: 0.056459080427885056\n",
            "i 435\n",
            "epoch 42\n",
            " batch Loss train: 0.054881494492292404\n",
            "i 436\n",
            "epoch 42\n",
            " batch Loss train: 0.05059422180056572\n",
            "i 437\n",
            "epoch 42\n",
            " batch Loss train: 0.1094139963388443\n",
            "i 438\n",
            "epoch 42\n",
            " batch Loss train: 0.08568202704191208\n",
            "i 439\n",
            "epoch 42\n",
            " batch Loss train: 0.08195757865905762\n",
            "i 440\n",
            "epoch 42\n",
            " batch Loss train: 0.06841196119785309\n",
            "i 441\n",
            "epoch 42\n",
            " batch Loss train: 0.06312922388315201\n",
            "i 442\n",
            "epoch 42\n",
            " batch Loss train: 0.08819879591464996\n",
            "i 443\n",
            "epoch 42\n",
            " batch Loss train: 0.0630679503083229\n",
            "i 444\n",
            "epoch 42\n",
            " batch Loss train: 0.08219058066606522\n",
            "i 445\n",
            "epoch 42\n",
            " batch Loss train: 0.07452671229839325\n",
            "total epoch Loss train: tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 43\n",
            " batch Loss train: 0.10036484152078629\n",
            "i 1\n",
            "epoch 43\n",
            " batch Loss train: 0.060367949306964874\n",
            "i 2\n",
            "epoch 43\n",
            " batch Loss train: 0.06389187276363373\n",
            "i 3\n",
            "epoch 43\n",
            " batch Loss train: 0.06943181902170181\n",
            "i 4\n",
            "epoch 43\n",
            " batch Loss train: 0.0666927918791771\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 43\n",
            " batch Loss train: 0.07725723087787628\n",
            "i 6\n",
            "epoch 43\n",
            " batch Loss train: 0.07235720753669739\n",
            "i 7\n",
            "epoch 43\n",
            " batch Loss train: 0.06475019454956055\n",
            "i 8\n",
            "epoch 43\n",
            " batch Loss train: 0.07390428334474564\n",
            "i 9\n",
            "epoch 43\n",
            " batch Loss train: 0.07202677428722382\n",
            "i 10\n",
            "epoch 43\n",
            " batch Loss train: 0.08834250271320343\n",
            "i 11\n",
            "epoch 43\n",
            " batch Loss train: 0.06288710981607437\n",
            "i 12\n",
            "epoch 43\n",
            " batch Loss train: 0.06451836228370667\n",
            "i 13\n",
            "epoch 43\n",
            " batch Loss train: 0.05298528075218201\n",
            "i 14\n",
            "epoch 43\n",
            " batch Loss train: 0.04905618354678154\n",
            "i 15\n",
            "epoch 43\n",
            " batch Loss train: 0.04690495505928993\n",
            "i 16\n",
            "epoch 43\n",
            " batch Loss train: 0.05981391295790672\n",
            "i 17\n",
            "epoch 43\n",
            " batch Loss train: 0.06069875881075859\n",
            "i 18\n",
            "epoch 43\n",
            " batch Loss train: 0.049261175096035004\n",
            "i 19\n",
            "epoch 43\n",
            " batch Loss train: 0.08039457350969315\n",
            "i 20\n",
            "epoch 43\n",
            " batch Loss train: 0.05447627976536751\n",
            "i 21\n",
            "epoch 43\n",
            " batch Loss train: 0.05738700181245804\n",
            "i 22\n",
            "epoch 43\n",
            " batch Loss train: 0.06570975482463837\n",
            "i 23\n",
            "epoch 43\n",
            " batch Loss train: 0.056404706090688705\n",
            "i 24\n",
            "epoch 43\n",
            " batch Loss train: 0.0751970112323761\n",
            "i 25\n",
            "epoch 43\n",
            " batch Loss train: 0.05845445394515991\n",
            "i 26\n",
            "epoch 43\n",
            " batch Loss train: 0.05979978293180466\n",
            "i 27\n",
            "epoch 43\n",
            " batch Loss train: 0.06459009647369385\n",
            "i 28\n",
            "epoch 43\n",
            " batch Loss train: 0.05633530393242836\n",
            "i 29\n",
            "epoch 43\n",
            " batch Loss train: 0.06179113686084747\n",
            "i 30\n",
            "epoch 43\n",
            " batch Loss train: 0.05693766102194786\n",
            "i 31\n",
            "epoch 43\n",
            " batch Loss train: 0.0810772031545639\n",
            "i 32\n",
            "epoch 43\n",
            " batch Loss train: 0.08752898126840591\n",
            "i 33\n",
            "epoch 43\n",
            " batch Loss train: 0.0659254640340805\n",
            "i 34\n",
            "epoch 43\n",
            " batch Loss train: 0.056973766535520554\n",
            "i 35\n",
            "epoch 43\n",
            " batch Loss train: 0.0591820552945137\n",
            "i 36\n",
            "epoch 43\n",
            " batch Loss train: 0.05110032856464386\n",
            "i 37\n",
            "epoch 43\n",
            " batch Loss train: 0.088112011551857\n",
            "i 38\n",
            "epoch 43\n",
            " batch Loss train: 0.04935522377490997\n",
            "i 39\n",
            "epoch 43\n",
            " batch Loss train: 0.04888709634542465\n",
            "i 40\n",
            "epoch 43\n",
            " batch Loss train: 0.07169406861066818\n",
            "i 41\n",
            "epoch 43\n",
            " batch Loss train: 0.09464699029922485\n",
            "i 42\n",
            "epoch 43\n",
            " batch Loss train: 0.04438835754990578\n",
            "i 43\n",
            "epoch 43\n",
            " batch Loss train: 0.07081867009401321\n",
            "i 44\n",
            "epoch 43\n",
            " batch Loss train: 0.07015268504619598\n",
            "i 45\n",
            "epoch 43\n",
            " batch Loss train: 0.06778331845998764\n",
            "i 46\n",
            "epoch 43\n",
            " batch Loss train: 0.04113391041755676\n",
            "i 47\n",
            "epoch 43\n",
            " batch Loss train: 0.032703887671232224\n",
            "i 48\n",
            "epoch 43\n",
            " batch Loss train: 0.06130827218294144\n",
            "i 49\n",
            "epoch 43\n",
            " batch Loss train: 0.06627491116523743\n",
            "i 50\n",
            "epoch 43\n",
            " batch Loss train: 0.053412020206451416\n",
            "i 51\n",
            "epoch 43\n",
            " batch Loss train: 0.053992051631212234\n",
            "i 52\n",
            "epoch 43\n",
            " batch Loss train: 0.08433367311954498\n",
            "i 53\n",
            "epoch 43\n",
            " batch Loss train: 0.052031658589839935\n",
            "i 54\n",
            "epoch 43\n",
            " batch Loss train: 0.0652400553226471\n",
            "i 55\n",
            "epoch 43\n",
            " batch Loss train: 0.0557752288877964\n",
            "i 56\n",
            "epoch 43\n",
            " batch Loss train: 0.07321763783693314\n",
            "i 57\n",
            "epoch 43\n",
            " batch Loss train: 0.05678335949778557\n",
            "i 58\n",
            "epoch 43\n",
            " batch Loss train: 0.06679792702198029\n",
            "i 59\n",
            "epoch 43\n",
            " batch Loss train: 0.05754644796252251\n",
            "i 60\n",
            "epoch 43\n",
            " batch Loss train: 0.06961088627576828\n",
            "i 61\n",
            "epoch 43\n",
            " batch Loss train: 0.06157280504703522\n",
            "i 62\n",
            "epoch 43\n",
            " batch Loss train: 0.07916200160980225\n",
            "i 63\n",
            "epoch 43\n",
            " batch Loss train: 0.060312800109386444\n",
            "i 64\n",
            "epoch 43\n",
            " batch Loss train: 0.0558960847556591\n",
            "i 65\n",
            "epoch 43\n",
            " batch Loss train: 0.0797104686498642\n",
            "i 66\n",
            "epoch 43\n",
            " batch Loss train: 0.04964066296815872\n",
            "i 67\n",
            "epoch 43\n",
            " batch Loss train: 0.06726238131523132\n",
            "i 68\n",
            "epoch 43\n",
            " batch Loss train: 0.04017804190516472\n",
            "i 69\n",
            "epoch 43\n",
            " batch Loss train: 0.0712074413895607\n",
            "i 70\n",
            "epoch 43\n",
            " batch Loss train: 0.07022412121295929\n",
            "i 71\n",
            "epoch 43\n",
            " batch Loss train: 0.0597485788166523\n",
            "i 72\n",
            "epoch 43\n",
            " batch Loss train: 0.07687531411647797\n",
            "i 73\n",
            "epoch 43\n",
            " batch Loss train: 0.08200772106647491\n",
            "i 74\n",
            "epoch 43\n",
            " batch Loss train: 0.06722136586904526\n",
            "i 75\n",
            "epoch 43\n",
            " batch Loss train: 0.04890801012516022\n",
            "i 76\n",
            "epoch 43\n",
            " batch Loss train: 0.05040004104375839\n",
            "i 77\n",
            "epoch 43\n",
            " batch Loss train: 0.055593520402908325\n",
            "i 78\n",
            "epoch 43\n",
            " batch Loss train: 0.06481049954891205\n",
            "i 79\n",
            "epoch 43\n",
            " batch Loss train: 0.05423416197299957\n",
            "i 80\n",
            "epoch 43\n",
            " batch Loss train: 0.07176428288221359\n",
            "i 81\n",
            "epoch 43\n",
            " batch Loss train: 0.06611396372318268\n",
            "i 82\n",
            "epoch 43\n",
            " batch Loss train: 0.07327638566493988\n",
            "i 83\n",
            "epoch 43\n",
            " batch Loss train: 0.04991887882351875\n",
            "i 84\n",
            "epoch 43\n",
            " batch Loss train: 0.05287040397524834\n",
            "i 85\n",
            "epoch 43\n",
            " batch Loss train: 0.05746982619166374\n",
            "i 86\n",
            "epoch 43\n",
            " batch Loss train: 0.06864333897829056\n",
            "i 87\n",
            "epoch 43\n",
            " batch Loss train: 0.08473680168390274\n",
            "i 88\n",
            "epoch 43\n",
            " batch Loss train: 0.07221439480781555\n",
            "i 89\n",
            "epoch 43\n",
            " batch Loss train: 0.04268735274672508\n",
            "i 90\n",
            "epoch 43\n",
            " batch Loss train: 0.05077994242310524\n",
            "i 91\n",
            "epoch 43\n",
            " batch Loss train: 0.050636451691389084\n",
            "i 92\n",
            "epoch 43\n",
            " batch Loss train: 0.06587182730436325\n",
            "i 93\n",
            "epoch 43\n",
            " batch Loss train: 0.06178516149520874\n",
            "i 94\n",
            "epoch 43\n",
            " batch Loss train: 0.06384233385324478\n",
            "i 95\n",
            "epoch 43\n",
            " batch Loss train: 0.05958087742328644\n",
            "i 96\n",
            "epoch 43\n",
            " batch Loss train: 0.06996870785951614\n",
            "i 97\n",
            "epoch 43\n",
            " batch Loss train: 0.0508548878133297\n",
            "i 98\n",
            "epoch 43\n",
            " batch Loss train: 0.06008896604180336\n",
            "i 99\n",
            "epoch 43\n",
            " batch Loss train: 0.04822558909654617\n",
            "i 100\n",
            "epoch 43\n",
            " batch Loss train: 0.05551779642701149\n",
            "i 101\n",
            "epoch 43\n",
            " batch Loss train: 0.0653214231133461\n",
            "i 102\n",
            "epoch 43\n",
            " batch Loss train: 0.04652920365333557\n",
            "i 103\n",
            "epoch 43\n",
            " batch Loss train: 0.0623382106423378\n",
            "i 104\n",
            "epoch 43\n",
            " batch Loss train: 0.05739984288811684\n",
            "i 105\n",
            "epoch 43\n",
            " batch Loss train: 0.05986816808581352\n",
            "i 106\n",
            "epoch 43\n",
            " batch Loss train: 0.051888179033994675\n",
            "i 107\n",
            "epoch 43\n",
            " batch Loss train: 0.05950427055358887\n",
            "i 108\n",
            "epoch 43\n",
            " batch Loss train: 0.05314157158136368\n",
            "i 109\n",
            "epoch 43\n",
            " batch Loss train: 0.0502752810716629\n",
            "i 110\n",
            "epoch 43\n",
            " batch Loss train: 0.06892167031764984\n",
            "i 111\n",
            "epoch 43\n",
            " batch Loss train: 0.06365127861499786\n",
            "i 112\n",
            "epoch 43\n",
            " batch Loss train: 0.0441708043217659\n",
            "i 113\n",
            "epoch 43\n",
            " batch Loss train: 0.06453541666269302\n",
            "i 114\n",
            "epoch 43\n",
            " batch Loss train: 0.07006775587797165\n",
            "i 115\n",
            "epoch 43\n",
            " batch Loss train: 0.07268976420164108\n",
            "i 116\n",
            "epoch 43\n",
            " batch Loss train: 0.07255466282367706\n",
            "i 117\n",
            "epoch 43\n",
            " batch Loss train: 0.052877191454172134\n",
            "i 118\n",
            "epoch 43\n",
            " batch Loss train: 0.07734881341457367\n",
            "i 119\n",
            "epoch 43\n",
            " batch Loss train: 0.07026374340057373\n",
            "i 120\n",
            "epoch 43\n",
            " batch Loss train: 0.06631471961736679\n",
            "i 121\n",
            "epoch 43\n",
            " batch Loss train: 0.0750468373298645\n",
            "i 122\n",
            "epoch 43\n",
            " batch Loss train: 0.06264859437942505\n",
            "i 123\n",
            "epoch 43\n",
            " batch Loss train: 0.045618217438459396\n",
            "i 124\n",
            "epoch 43\n",
            " batch Loss train: 0.1016048714518547\n",
            "i 125\n",
            "epoch 43\n",
            " batch Loss train: 0.04171500727534294\n",
            "i 126\n",
            "epoch 43\n",
            " batch Loss train: 0.06930544227361679\n",
            "i 127\n",
            "epoch 43\n",
            " batch Loss train: 0.060100387781858444\n",
            "i 128\n",
            "epoch 43\n",
            " batch Loss train: 0.06602338701486588\n",
            "i 129\n",
            "epoch 43\n",
            " batch Loss train: 0.06562919169664383\n",
            "i 130\n",
            "epoch 43\n",
            " batch Loss train: 0.059230539947748184\n",
            "i 131\n",
            "epoch 43\n",
            " batch Loss train: 0.04854205623269081\n",
            "i 132\n",
            "epoch 43\n",
            " batch Loss train: 0.06199738755822182\n",
            "i 133\n",
            "epoch 43\n",
            " batch Loss train: 0.05378488451242447\n",
            "i 134\n",
            "epoch 43\n",
            " batch Loss train: 0.05449850857257843\n",
            "i 135\n",
            "epoch 43\n",
            " batch Loss train: 0.07912436872720718\n",
            "i 136\n",
            "epoch 43\n",
            " batch Loss train: 0.06536970287561417\n",
            "i 137\n",
            "epoch 43\n",
            " batch Loss train: 0.0626123920083046\n",
            "i 138\n",
            "epoch 43\n",
            " batch Loss train: 0.06439004838466644\n",
            "i 139\n",
            "epoch 43\n",
            " batch Loss train: 0.07974948734045029\n",
            "i 140\n",
            "epoch 43\n",
            " batch Loss train: 0.06983883678913116\n",
            "i 141\n",
            "epoch 43\n",
            " batch Loss train: 0.08005333691835403\n",
            "i 142\n",
            "epoch 43\n",
            " batch Loss train: 0.04238046333193779\n",
            "i 143\n",
            "epoch 43\n",
            " batch Loss train: 0.05499054491519928\n",
            "i 144\n",
            "epoch 43\n",
            " batch Loss train: 0.0716150626540184\n",
            "i 145\n",
            "epoch 43\n",
            " batch Loss train: 0.07598522305488586\n",
            "i 146\n",
            "epoch 43\n",
            " batch Loss train: 0.048378534615039825\n",
            "i 147\n",
            "epoch 43\n",
            " batch Loss train: 0.07605493068695068\n",
            "i 148\n",
            "epoch 43\n",
            " batch Loss train: 0.059813499450683594\n",
            "i 149\n",
            "epoch 43\n",
            " batch Loss train: 0.07725996524095535\n",
            "i 150\n",
            "epoch 43\n",
            " batch Loss train: 0.05631396919488907\n",
            "i 151\n",
            "epoch 43\n",
            " batch Loss train: 0.08230556547641754\n",
            "i 152\n",
            "epoch 43\n",
            " batch Loss train: 0.06332454830408096\n",
            "i 153\n",
            "epoch 43\n",
            " batch Loss train: 0.059109583497047424\n",
            "i 154\n",
            "epoch 43\n",
            " batch Loss train: 0.056279852986335754\n",
            "i 155\n",
            "epoch 43\n",
            " batch Loss train: 0.05861697345972061\n",
            "i 156\n",
            "epoch 43\n",
            " batch Loss train: 0.06413016468286514\n",
            "i 157\n",
            "epoch 43\n",
            " batch Loss train: 0.06516366451978683\n",
            "i 158\n",
            "epoch 43\n",
            " batch Loss train: 0.07399921119213104\n",
            "i 159\n",
            "epoch 43\n",
            " batch Loss train: 0.06775039434432983\n",
            "i 160\n",
            "epoch 43\n",
            " batch Loss train: 0.0808628648519516\n",
            "i 161\n",
            "epoch 43\n",
            " batch Loss train: 0.10078410059213638\n",
            "i 162\n",
            "epoch 43\n",
            " batch Loss train: 0.058437906205654144\n",
            "i 163\n",
            "epoch 43\n",
            " batch Loss train: 0.07260723412036896\n",
            "i 164\n",
            "epoch 43\n",
            " batch Loss train: 0.0706433355808258\n",
            "i 165\n",
            "epoch 43\n",
            " batch Loss train: 0.07007091492414474\n",
            "i 166\n",
            "epoch 43\n",
            " batch Loss train: 0.07547163218259811\n",
            "i 167\n",
            "epoch 43\n",
            " batch Loss train: 0.08485563099384308\n",
            "i 168\n",
            "epoch 43\n",
            " batch Loss train: 0.07421688735485077\n",
            "i 169\n",
            "epoch 43\n",
            " batch Loss train: 0.08048845082521439\n",
            "i 170\n",
            "epoch 43\n",
            " batch Loss train: 0.07344182580709457\n",
            "i 171\n",
            "epoch 43\n",
            " batch Loss train: 0.05570216849446297\n",
            "i 172\n",
            "epoch 43\n",
            " batch Loss train: 0.059914376586675644\n",
            "i 173\n",
            "epoch 43\n",
            " batch Loss train: 0.06876152753829956\n",
            "i 174\n",
            "epoch 43\n",
            " batch Loss train: 0.0694296732544899\n",
            "i 175\n",
            "epoch 43\n",
            " batch Loss train: 0.08439972251653671\n",
            "i 176\n",
            "epoch 43\n",
            " batch Loss train: 0.06800419092178345\n",
            "i 177\n",
            "epoch 43\n",
            " batch Loss train: 0.05753422901034355\n",
            "i 178\n",
            "epoch 43\n",
            " batch Loss train: 0.04479092359542847\n",
            "i 179\n",
            "epoch 43\n",
            " batch Loss train: 0.08425623923540115\n",
            "i 180\n",
            "epoch 43\n",
            " batch Loss train: 0.05649062246084213\n",
            "i 181\n",
            "epoch 43\n",
            " batch Loss train: 0.0982254296541214\n",
            "i 182\n",
            "epoch 43\n",
            " batch Loss train: 0.0644981637597084\n",
            "i 183\n",
            "epoch 43\n",
            " batch Loss train: 0.0799240991473198\n",
            "i 184\n",
            "epoch 43\n",
            " batch Loss train: 0.06305892020463943\n",
            "i 185\n",
            "epoch 43\n",
            " batch Loss train: 0.0893956869840622\n",
            "i 186\n",
            "epoch 43\n",
            " batch Loss train: 0.10148649662733078\n",
            "i 187\n",
            "epoch 43\n",
            " batch Loss train: 0.055812276899814606\n",
            "i 188\n",
            "epoch 43\n",
            " batch Loss train: 0.06221991404891014\n",
            "i 189\n",
            "epoch 43\n",
            " batch Loss train: 0.05192384123802185\n",
            "i 190\n",
            "epoch 43\n",
            " batch Loss train: 0.06222566217184067\n",
            "i 191\n",
            "epoch 43\n",
            " batch Loss train: 0.059305284172296524\n",
            "i 192\n",
            "epoch 43\n",
            " batch Loss train: 0.0793248638510704\n",
            "i 193\n",
            "epoch 43\n",
            " batch Loss train: 0.056879669427871704\n",
            "i 194\n",
            "epoch 43\n",
            " batch Loss train: 0.06598342955112457\n",
            "i 195\n",
            "epoch 43\n",
            " batch Loss train: 0.06099816784262657\n",
            "i 196\n",
            "epoch 43\n",
            " batch Loss train: 0.07985201478004456\n",
            "i 197\n",
            "epoch 43\n",
            " batch Loss train: 0.05350589379668236\n",
            "i 198\n",
            "epoch 43\n",
            " batch Loss train: 0.05354023352265358\n",
            "i 199\n",
            "epoch 43\n",
            " batch Loss train: 0.05298052355647087\n",
            "i 200\n",
            "epoch 43\n",
            " batch Loss train: 0.07220590859651566\n",
            "i 201\n",
            "epoch 43\n",
            " batch Loss train: 0.07462336122989655\n",
            "i 202\n",
            "epoch 43\n",
            " batch Loss train: 0.0643661767244339\n",
            "i 203\n",
            "epoch 43\n",
            " batch Loss train: 0.09329954534769058\n",
            "i 204\n",
            "epoch 43\n",
            " batch Loss train: 0.09244148433208466\n",
            "i 205\n",
            "epoch 43\n",
            " batch Loss train: 0.06531757861375809\n",
            "i 206\n",
            "epoch 43\n",
            " batch Loss train: 0.06890401989221573\n",
            "i 207\n",
            "epoch 43\n",
            " batch Loss train: 0.05877571180462837\n",
            "i 208\n",
            "epoch 43\n",
            " batch Loss train: 0.08324899524450302\n",
            "i 209\n",
            "epoch 43\n",
            " batch Loss train: 0.05680948123335838\n",
            "i 210\n",
            "epoch 43\n",
            " batch Loss train: 0.05250927805900574\n",
            "i 211\n",
            "epoch 43\n",
            " batch Loss train: 0.08193457126617432\n",
            "i 212\n",
            "epoch 43\n",
            " batch Loss train: 0.07666445523500443\n",
            "i 213\n",
            "epoch 43\n",
            " batch Loss train: 0.08018724620342255\n",
            "i 214\n",
            "epoch 43\n",
            " batch Loss train: 0.07656104117631912\n",
            "i 215\n",
            "epoch 43\n",
            " batch Loss train: 0.07153218984603882\n",
            "i 216\n",
            "epoch 43\n",
            " batch Loss train: 0.065791055560112\n",
            "i 217\n",
            "epoch 43\n",
            " batch Loss train: 0.08925577253103256\n",
            "i 218\n",
            "epoch 43\n",
            " batch Loss train: 0.04985594376921654\n",
            "i 219\n",
            "epoch 43\n",
            " batch Loss train: 0.05323360860347748\n",
            "i 220\n",
            "epoch 43\n",
            " batch Loss train: 0.06432738155126572\n",
            "i 221\n",
            "epoch 43\n",
            " batch Loss train: 0.056302934885025024\n",
            "i 222\n",
            "epoch 43\n",
            " batch Loss train: 0.09479717165231705\n",
            "i 223\n",
            "epoch 43\n",
            " batch Loss train: 0.07415400445461273\n",
            "i 224\n",
            "epoch 43\n",
            " batch Loss train: 0.05281677097082138\n",
            "i 225\n",
            "epoch 43\n",
            " batch Loss train: 0.05617305636405945\n",
            "i 226\n",
            "epoch 43\n",
            " batch Loss train: 0.04823610559105873\n",
            "i 227\n",
            "epoch 43\n",
            " batch Loss train: 0.0597873218357563\n",
            "i 228\n",
            "epoch 43\n",
            " batch Loss train: 0.057692788541316986\n",
            "i 229\n",
            "epoch 43\n",
            " batch Loss train: 0.09010819345712662\n",
            "i 230\n",
            "epoch 43\n",
            " batch Loss train: 0.06028995290398598\n",
            "i 231\n",
            "epoch 43\n",
            " batch Loss train: 0.06966611742973328\n",
            "i 232\n",
            "epoch 43\n",
            " batch Loss train: 0.10152626782655716\n",
            "i 233\n",
            "epoch 43\n",
            " batch Loss train: 0.06651604920625687\n",
            "i 234\n",
            "epoch 43\n",
            " batch Loss train: 0.056601621210575104\n",
            "i 235\n",
            "epoch 43\n",
            " batch Loss train: 0.06016591191291809\n",
            "i 236\n",
            "epoch 43\n",
            " batch Loss train: 0.06857801228761673\n",
            "i 237\n",
            "epoch 43\n",
            " batch Loss train: 0.08091617375612259\n",
            "i 238\n",
            "epoch 43\n",
            " batch Loss train: 0.0452243834733963\n",
            "i 239\n",
            "epoch 43\n",
            " batch Loss train: 0.048177316784858704\n",
            "i 240\n",
            "epoch 43\n",
            " batch Loss train: 0.0690527930855751\n",
            "i 241\n",
            "epoch 43\n",
            " batch Loss train: 0.0748734399676323\n",
            "i 242\n",
            "epoch 43\n",
            " batch Loss train: 0.06904218345880508\n",
            "i 243\n",
            "epoch 43\n",
            " batch Loss train: 0.058201082050800323\n",
            "i 244\n",
            "epoch 43\n",
            " batch Loss train: 0.07417357712984085\n",
            "i 245\n",
            "epoch 43\n",
            " batch Loss train: 0.06933623552322388\n",
            "i 246\n",
            "epoch 43\n",
            " batch Loss train: 0.09293931722640991\n",
            "i 247\n",
            "epoch 43\n",
            " batch Loss train: 0.06259489059448242\n",
            "i 248\n",
            "epoch 43\n",
            " batch Loss train: 0.06120353192090988\n",
            "i 249\n",
            "epoch 43\n",
            " batch Loss train: 0.0787353590130806\n",
            "i 250\n",
            "epoch 43\n",
            " batch Loss train: 0.07186026871204376\n",
            "i 251\n",
            "epoch 43\n",
            " batch Loss train: 0.07580148428678513\n",
            "i 252\n",
            "epoch 43\n",
            " batch Loss train: 0.05194903910160065\n",
            "i 253\n",
            "epoch 43\n",
            " batch Loss train: 0.06359360367059708\n",
            "i 254\n",
            "epoch 43\n",
            " batch Loss train: 0.07651235908269882\n",
            "i 255\n",
            "epoch 43\n",
            " batch Loss train: 0.05258514732122421\n",
            "i 256\n",
            "epoch 43\n",
            " batch Loss train: 0.07662694901227951\n",
            "i 257\n",
            "epoch 43\n",
            " batch Loss train: 0.08183776587247849\n",
            "i 258\n",
            "epoch 43\n",
            " batch Loss train: 0.04814153164625168\n",
            "i 259\n",
            "epoch 43\n",
            " batch Loss train: 0.05673821270465851\n",
            "i 260\n",
            "epoch 43\n",
            " batch Loss train: 0.05876891314983368\n",
            "i 261\n",
            "epoch 43\n",
            " batch Loss train: 0.05455244332551956\n",
            "i 262\n",
            "epoch 43\n",
            " batch Loss train: 0.04553889483213425\n",
            "i 263\n",
            "epoch 43\n",
            " batch Loss train: 0.09173078089952469\n",
            "i 264\n",
            "epoch 43\n",
            " batch Loss train: 0.06146042421460152\n",
            "i 265\n",
            "epoch 43\n",
            " batch Loss train: 0.060064647346735\n",
            "i 266\n",
            "epoch 43\n",
            " batch Loss train: 0.09406641125679016\n",
            "i 267\n",
            "epoch 43\n",
            " batch Loss train: 0.05928318202495575\n",
            "i 268\n",
            "epoch 43\n",
            " batch Loss train: 0.04088533669710159\n",
            "i 269\n",
            "epoch 43\n",
            " batch Loss train: 0.054964423179626465\n",
            "i 270\n",
            "epoch 43\n",
            " batch Loss train: 0.0741196870803833\n",
            "i 271\n",
            "epoch 43\n",
            " batch Loss train: 0.06524216383695602\n",
            "i 272\n",
            "epoch 43\n",
            " batch Loss train: 0.07171687483787537\n",
            "i 273\n",
            "epoch 43\n",
            " batch Loss train: 0.06836796551942825\n",
            "i 274\n",
            "epoch 43\n",
            " batch Loss train: 0.08138487488031387\n",
            "i 275\n",
            "epoch 43\n",
            " batch Loss train: 0.06092439219355583\n",
            "i 276\n",
            "epoch 43\n",
            " batch Loss train: 0.08706537634134293\n",
            "i 277\n",
            "epoch 43\n",
            " batch Loss train: 0.07351990044116974\n",
            "i 278\n",
            "epoch 43\n",
            " batch Loss train: 0.07096294313669205\n",
            "i 279\n",
            "epoch 43\n",
            " batch Loss train: 0.07903844863176346\n",
            "i 280\n",
            "epoch 43\n",
            " batch Loss train: 0.058063820004463196\n",
            "i 281\n",
            "epoch 43\n",
            " batch Loss train: 0.05137089267373085\n",
            "i 282\n",
            "epoch 43\n",
            " batch Loss train: 0.07170384377241135\n",
            "i 283\n",
            "epoch 43\n",
            " batch Loss train: 0.06588027626276016\n",
            "i 284\n",
            "epoch 43\n",
            " batch Loss train: 0.09087253361940384\n",
            "i 285\n",
            "epoch 43\n",
            " batch Loss train: 0.06434575468301773\n",
            "i 286\n",
            "epoch 43\n",
            " batch Loss train: 0.0729827731847763\n",
            "i 287\n",
            "epoch 43\n",
            " batch Loss train: 0.06623118370771408\n",
            "i 288\n",
            "epoch 43\n",
            " batch Loss train: 0.06233597174286842\n",
            "i 289\n",
            "epoch 43\n",
            " batch Loss train: 0.06495737284421921\n",
            "i 290\n",
            "epoch 43\n",
            " batch Loss train: 0.05696152523159981\n",
            "i 291\n",
            "epoch 43\n",
            " batch Loss train: 0.05742764100432396\n",
            "i 292\n",
            "epoch 43\n",
            " batch Loss train: 0.0795094296336174\n",
            "i 293\n",
            "epoch 43\n",
            " batch Loss train: 0.045106012374162674\n",
            "i 294\n",
            "epoch 43\n",
            " batch Loss train: 0.044420525431632996\n",
            "i 295\n",
            "epoch 43\n",
            " batch Loss train: 0.056729838252067566\n",
            "i 296\n",
            "epoch 43\n",
            " batch Loss train: 0.06480236351490021\n",
            "i 297\n",
            "epoch 43\n",
            " batch Loss train: 0.049064308404922485\n",
            "i 298\n",
            "epoch 43\n",
            " batch Loss train: 0.06816145032644272\n",
            "i 299\n",
            "epoch 43\n",
            " batch Loss train: 0.05862438306212425\n",
            "i 300\n",
            "epoch 43\n",
            " batch Loss train: 0.07317105680704117\n",
            "i 301\n",
            "epoch 43\n",
            " batch Loss train: 0.08967187255620956\n",
            "i 302\n",
            "epoch 43\n",
            " batch Loss train: 0.056128181517124176\n",
            "i 303\n",
            "epoch 43\n",
            " batch Loss train: 0.07863255590200424\n",
            "i 304\n",
            "epoch 43\n",
            " batch Loss train: 0.05347904935479164\n",
            "i 305\n",
            "epoch 43\n",
            " batch Loss train: 0.0643099993467331\n",
            "i 306\n",
            "epoch 43\n",
            " batch Loss train: 0.06893546134233475\n",
            "i 307\n",
            "epoch 43\n",
            " batch Loss train: 0.07095392048358917\n",
            "i 308\n",
            "epoch 43\n",
            " batch Loss train: 0.06231677159667015\n",
            "i 309\n",
            "epoch 43\n",
            " batch Loss train: 0.06760004907846451\n",
            "i 310\n",
            "epoch 43\n",
            " batch Loss train: 0.07005632668733597\n",
            "i 311\n",
            "epoch 43\n",
            " batch Loss train: 0.0702115148305893\n",
            "i 312\n",
            "epoch 43\n",
            " batch Loss train: 0.06249259039759636\n",
            "i 313\n",
            "epoch 43\n",
            " batch Loss train: 0.057353194802999496\n",
            "i 314\n",
            "epoch 43\n",
            " batch Loss train: 0.05043988302350044\n",
            "i 315\n",
            "epoch 43\n",
            " batch Loss train: 0.06309420615434647\n",
            "i 316\n",
            "epoch 43\n",
            " batch Loss train: 0.06867730617523193\n",
            "i 317\n",
            "epoch 43\n",
            " batch Loss train: 0.04285963252186775\n",
            "i 318\n",
            "epoch 43\n",
            " batch Loss train: 0.057340361177921295\n",
            "i 319\n",
            "epoch 43\n",
            " batch Loss train: 0.053202852606773376\n",
            "i 320\n",
            "epoch 43\n",
            " batch Loss train: 0.05952519178390503\n",
            "i 321\n",
            "epoch 43\n",
            " batch Loss train: 0.06804896146059036\n",
            "i 322\n",
            "epoch 43\n",
            " batch Loss train: 0.06375549733638763\n",
            "i 323\n",
            "epoch 43\n",
            " batch Loss train: 0.05530626326799393\n",
            "i 324\n",
            "epoch 43\n",
            " batch Loss train: 0.08071616291999817\n",
            "i 325\n",
            "epoch 43\n",
            " batch Loss train: 0.0450788177549839\n",
            "i 326\n",
            "epoch 43\n",
            " batch Loss train: 0.056344978511333466\n",
            "i 327\n",
            "epoch 43\n",
            " batch Loss train: 0.052397336810827255\n",
            "i 328\n",
            "epoch 43\n",
            " batch Loss train: 0.056694868952035904\n",
            "i 329\n",
            "epoch 43\n",
            " batch Loss train: 0.06513828784227371\n",
            "i 330\n",
            "epoch 43\n",
            " batch Loss train: 0.06292850524187088\n",
            "i 331\n",
            "epoch 43\n",
            " batch Loss train: 0.06383499503135681\n",
            "i 332\n",
            "epoch 43\n",
            " batch Loss train: 0.0777999609708786\n",
            "i 333\n",
            "epoch 43\n",
            " batch Loss train: 0.07201704382896423\n",
            "i 334\n",
            "epoch 43\n",
            " batch Loss train: 0.051138199865818024\n",
            "i 335\n",
            "epoch 43\n",
            " batch Loss train: 0.05240943282842636\n",
            "i 336\n",
            "epoch 43\n",
            " batch Loss train: 0.05067041516304016\n",
            "i 337\n",
            "epoch 43\n",
            " batch Loss train: 0.06987457722425461\n",
            "i 338\n",
            "epoch 43\n",
            " batch Loss train: 0.05191720649600029\n",
            "i 339\n",
            "epoch 43\n",
            " batch Loss train: 0.07211466878652573\n",
            "i 340\n",
            "epoch 43\n",
            " batch Loss train: 0.06750936061143875\n",
            "i 341\n",
            "epoch 43\n",
            " batch Loss train: 0.0717487782239914\n",
            "i 342\n",
            "epoch 43\n",
            " batch Loss train: 0.058189764618873596\n",
            "i 343\n",
            "epoch 43\n",
            " batch Loss train: 0.08879148960113525\n",
            "i 344\n",
            "epoch 43\n",
            " batch Loss train: 0.053338900208473206\n",
            "i 345\n",
            "epoch 43\n",
            " batch Loss train: 0.05146997049450874\n",
            "i 346\n",
            "epoch 43\n",
            " batch Loss train: 0.057807814329862595\n",
            "i 347\n",
            "epoch 43\n",
            " batch Loss train: 0.05534088611602783\n",
            "i 348\n",
            "epoch 43\n",
            " batch Loss train: 0.0682092234492302\n",
            "i 349\n",
            "epoch 43\n",
            " batch Loss train: 0.09095238149166107\n",
            "i 350\n",
            "epoch 43\n",
            " batch Loss train: 0.08600082248449326\n",
            "i 351\n",
            "epoch 43\n",
            " batch Loss train: 0.05962979048490524\n",
            "i 352\n",
            "epoch 43\n",
            " batch Loss train: 0.05941932275891304\n",
            "i 353\n",
            "epoch 43\n",
            " batch Loss train: 0.07207853347063065\n",
            "i 354\n",
            "epoch 43\n",
            " batch Loss train: 0.057009343057870865\n",
            "i 355\n",
            "epoch 43\n",
            " batch Loss train: 0.06383977830410004\n",
            "i 356\n",
            "epoch 43\n",
            " batch Loss train: 0.05834045261144638\n",
            "i 357\n",
            "epoch 43\n",
            " batch Loss train: 0.07338950037956238\n",
            "i 358\n",
            "epoch 43\n",
            " batch Loss train: 0.07995246350765228\n",
            "i 359\n",
            "epoch 43\n",
            " batch Loss train: 0.05038870871067047\n",
            "i 360\n",
            "epoch 43\n",
            " batch Loss train: 0.07313311845064163\n",
            "i 361\n",
            "epoch 43\n",
            " batch Loss train: 0.0512198731303215\n",
            "i 362\n",
            "epoch 43\n",
            " batch Loss train: 0.05486606806516647\n",
            "i 363\n",
            "epoch 43\n",
            " batch Loss train: 0.04665002599358559\n",
            "i 364\n",
            "epoch 43\n",
            " batch Loss train: 0.04966402053833008\n",
            "i 365\n",
            "epoch 43\n",
            " batch Loss train: 0.06460674107074738\n",
            "i 366\n",
            "epoch 43\n",
            " batch Loss train: 0.06451048702001572\n",
            "i 367\n",
            "epoch 43\n",
            " batch Loss train: 0.07027538865804672\n",
            "i 368\n",
            "epoch 43\n",
            " batch Loss train: 0.06890017539262772\n",
            "i 369\n",
            "epoch 43\n",
            " batch Loss train: 0.06265472620725632\n",
            "i 370\n",
            "epoch 43\n",
            " batch Loss train: 0.0741521418094635\n",
            "i 371\n",
            "epoch 43\n",
            " batch Loss train: 0.05458824709057808\n",
            "i 372\n",
            "epoch 43\n",
            " batch Loss train: 0.06914982199668884\n",
            "i 373\n",
            "epoch 43\n",
            " batch Loss train: 0.07876848429441452\n",
            "i 374\n",
            "epoch 43\n",
            " batch Loss train: 0.05694908648729324\n",
            "i 375\n",
            "epoch 43\n",
            " batch Loss train: 0.04791339859366417\n",
            "i 376\n",
            "epoch 43\n",
            " batch Loss train: 0.06011034920811653\n",
            "i 377\n",
            "epoch 43\n",
            " batch Loss train: 0.05087429657578468\n",
            "i 378\n",
            "epoch 43\n",
            " batch Loss train: 0.07528909295797348\n",
            "i 379\n",
            "epoch 43\n",
            " batch Loss train: 0.05493224039673805\n",
            "i 380\n",
            "epoch 43\n",
            " batch Loss train: 0.05214836448431015\n",
            "i 381\n",
            "epoch 43\n",
            " batch Loss train: 0.07074625790119171\n",
            "i 382\n",
            "epoch 43\n",
            " batch Loss train: 0.04551716148853302\n",
            "i 383\n",
            "epoch 43\n",
            " batch Loss train: 0.06260260939598083\n",
            "i 384\n",
            "epoch 43\n",
            " batch Loss train: 0.059848494827747345\n",
            "i 385\n",
            "epoch 43\n",
            " batch Loss train: 0.06277354806661606\n",
            "i 386\n",
            "epoch 43\n",
            " batch Loss train: 0.07927954941987991\n",
            "i 387\n",
            "epoch 43\n",
            " batch Loss train: 0.06125909462571144\n",
            "i 388\n",
            "epoch 43\n",
            " batch Loss train: 0.07046641409397125\n",
            "i 389\n",
            "epoch 43\n",
            " batch Loss train: 0.06442022323608398\n",
            "i 390\n",
            "epoch 43\n",
            " batch Loss train: 0.0699423998594284\n",
            "i 391\n",
            "epoch 43\n",
            " batch Loss train: 0.05948708951473236\n",
            "i 392\n",
            "epoch 43\n",
            " batch Loss train: 0.062376610934734344\n",
            "i 393\n",
            "epoch 43\n",
            " batch Loss train: 0.06832638382911682\n",
            "i 394\n",
            "epoch 43\n",
            " batch Loss train: 0.05772986263036728\n",
            "i 395\n",
            "epoch 43\n",
            " batch Loss train: 0.060182102024555206\n",
            "i 396\n",
            "epoch 43\n",
            " batch Loss train: 0.05290127545595169\n",
            "i 397\n",
            "epoch 43\n",
            " batch Loss train: 0.06759525090456009\n",
            "i 398\n",
            "epoch 43\n",
            " batch Loss train: 0.06853201240301132\n",
            "i 399\n",
            "epoch 43\n",
            " batch Loss train: 0.08124502748250961\n",
            "i 400\n",
            "epoch 43\n",
            " batch Loss train: 0.06137244030833244\n",
            "i 401\n",
            "epoch 43\n",
            " batch Loss train: 0.08121238648891449\n",
            "i 402\n",
            "epoch 43\n",
            " batch Loss train: 0.06637108325958252\n",
            "i 403\n",
            "epoch 43\n",
            " batch Loss train: 0.08227652311325073\n",
            "i 404\n",
            "epoch 43\n",
            " batch Loss train: 0.06260209530591965\n",
            "i 405\n",
            "epoch 43\n",
            " batch Loss train: 0.06745149195194244\n",
            "i 406\n",
            "epoch 43\n",
            " batch Loss train: 0.06639042496681213\n",
            "i 407\n",
            "epoch 43\n",
            " batch Loss train: 0.06585655361413956\n",
            "i 408\n",
            "epoch 43\n",
            " batch Loss train: 0.05621309205889702\n",
            "i 409\n",
            "epoch 43\n",
            " batch Loss train: 0.0664399266242981\n",
            "i 410\n",
            "epoch 43\n",
            " batch Loss train: 0.05699934810400009\n",
            "i 411\n",
            "epoch 43\n",
            " batch Loss train: 0.08803975582122803\n",
            "i 412\n",
            "epoch 43\n",
            " batch Loss train: 0.055657293647527695\n",
            "i 413\n",
            "epoch 43\n",
            " batch Loss train: 0.07941244542598724\n",
            "i 414\n",
            "epoch 43\n",
            " batch Loss train: 0.06214350089430809\n",
            "i 415\n",
            "epoch 43\n",
            " batch Loss train: 0.07533961534500122\n",
            "i 416\n",
            "epoch 43\n",
            " batch Loss train: 0.07976634055376053\n",
            "i 417\n",
            "epoch 43\n",
            " batch Loss train: 0.06428030878305435\n",
            "i 418\n",
            "epoch 43\n",
            " batch Loss train: 0.07420134544372559\n",
            "i 419\n",
            "epoch 43\n",
            " batch Loss train: 0.054171569645404816\n",
            "i 420\n",
            "epoch 43\n",
            " batch Loss train: 0.061882615089416504\n",
            "i 421\n",
            "epoch 43\n",
            " batch Loss train: 0.05996781960129738\n",
            "i 422\n",
            "epoch 43\n",
            " batch Loss train: 0.07969027012586594\n",
            "i 423\n",
            "epoch 43\n",
            " batch Loss train: 0.05348683521151543\n",
            "i 424\n",
            "epoch 43\n",
            " batch Loss train: 0.07027892023324966\n",
            "i 425\n",
            "epoch 43\n",
            " batch Loss train: 0.07405822724103928\n",
            "i 426\n",
            "epoch 43\n",
            " batch Loss train: 0.05658114701509476\n",
            "i 427\n",
            "epoch 43\n",
            " batch Loss train: 0.05912185460329056\n",
            "i 428\n",
            "epoch 43\n",
            " batch Loss train: 0.0652402937412262\n",
            "i 429\n",
            "epoch 43\n",
            " batch Loss train: 0.07187416404485703\n",
            "i 430\n",
            "epoch 43\n",
            " batch Loss train: 0.051258914172649384\n",
            "i 431\n",
            "epoch 43\n",
            " batch Loss train: 0.0714753195643425\n",
            "i 432\n",
            "epoch 43\n",
            " batch Loss train: 0.06392385810613632\n",
            "i 433\n",
            "epoch 43\n",
            " batch Loss train: 0.07031282037496567\n",
            "i 434\n",
            "epoch 43\n",
            " batch Loss train: 0.06887727975845337\n",
            "i 435\n",
            "epoch 43\n",
            " batch Loss train: 0.09562500566244125\n",
            "i 436\n",
            "epoch 43\n",
            " batch Loss train: 0.0626901313662529\n",
            "i 437\n",
            "epoch 43\n",
            " batch Loss train: 0.08523081243038177\n",
            "i 438\n",
            "epoch 43\n",
            " batch Loss train: 0.07017193734645844\n",
            "i 439\n",
            "epoch 43\n",
            " batch Loss train: 0.07260429114103317\n",
            "i 440\n",
            "epoch 43\n",
            " batch Loss train: 0.07295574992895126\n",
            "i 441\n",
            "epoch 43\n",
            " batch Loss train: 0.0578765794634819\n",
            "i 442\n",
            "epoch 43\n",
            " batch Loss train: 0.06599126756191254\n",
            "i 443\n",
            "epoch 43\n",
            " batch Loss train: 0.0738898292183876\n",
            "i 444\n",
            "epoch 43\n",
            " batch Loss train: 0.05951147899031639\n",
            "i 445\n",
            "epoch 43\n",
            " batch Loss train: 0.06063033640384674\n",
            "total epoch Loss train: tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 44\n",
            " batch Loss train: 0.07161901891231537\n",
            "i 1\n",
            "epoch 44\n",
            " batch Loss train: 0.03885403275489807\n",
            "i 2\n",
            "epoch 44\n",
            " batch Loss train: 0.05360836908221245\n",
            "i 3\n",
            "epoch 44\n",
            " batch Loss train: 0.051560427993535995\n",
            "i 4\n",
            "epoch 44\n",
            " batch Loss train: 0.04776044934988022\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 44\n",
            " batch Loss train: 0.05588068068027496\n",
            "i 6\n",
            "epoch 44\n",
            " batch Loss train: 0.05034920573234558\n",
            "i 7\n",
            "epoch 44\n",
            " batch Loss train: 0.05799315497279167\n",
            "i 8\n",
            "epoch 44\n",
            " batch Loss train: 0.08640074729919434\n",
            "i 9\n",
            "epoch 44\n",
            " batch Loss train: 0.04519657790660858\n",
            "i 10\n",
            "epoch 44\n",
            " batch Loss train: 0.06065082550048828\n",
            "i 11\n",
            "epoch 44\n",
            " batch Loss train: 0.06887051463127136\n",
            "i 12\n",
            "epoch 44\n",
            " batch Loss train: 0.056716229766607285\n",
            "i 13\n",
            "epoch 44\n",
            " batch Loss train: 0.04665264114737511\n",
            "i 14\n",
            "epoch 44\n",
            " batch Loss train: 0.05105898901820183\n",
            "i 15\n",
            "epoch 44\n",
            " batch Loss train: 0.06099432334303856\n",
            "i 16\n",
            "epoch 44\n",
            " batch Loss train: 0.05263053998351097\n",
            "i 17\n",
            "epoch 44\n",
            " batch Loss train: 0.05803212150931358\n",
            "i 18\n",
            "epoch 44\n",
            " batch Loss train: 0.060759689658880234\n",
            "i 19\n",
            "epoch 44\n",
            " batch Loss train: 0.05282650142908096\n",
            "i 20\n",
            "epoch 44\n",
            " batch Loss train: 0.046993110328912735\n",
            "i 21\n",
            "epoch 44\n",
            " batch Loss train: 0.07967105507850647\n",
            "i 22\n",
            "epoch 44\n",
            " batch Loss train: 0.040299009531736374\n",
            "i 23\n",
            "epoch 44\n",
            " batch Loss train: 0.04663432762026787\n",
            "i 24\n",
            "epoch 44\n",
            " batch Loss train: 0.0664086788892746\n",
            "i 25\n",
            "epoch 44\n",
            " batch Loss train: 0.05721866711974144\n",
            "i 26\n",
            "epoch 44\n",
            " batch Loss train: 0.0651489645242691\n",
            "i 27\n",
            "epoch 44\n",
            " batch Loss train: 0.04581838473677635\n",
            "i 28\n",
            "epoch 44\n",
            " batch Loss train: 0.05147114768624306\n",
            "i 29\n",
            "epoch 44\n",
            " batch Loss train: 0.051922231912612915\n",
            "i 30\n",
            "epoch 44\n",
            " batch Loss train: 0.06322599947452545\n",
            "i 31\n",
            "epoch 44\n",
            " batch Loss train: 0.04691221937537193\n",
            "i 32\n",
            "epoch 44\n",
            " batch Loss train: 0.06502128392457962\n",
            "i 33\n",
            "epoch 44\n",
            " batch Loss train: 0.06297974288463593\n",
            "i 34\n",
            "epoch 44\n",
            " batch Loss train: 0.06342369318008423\n",
            "i 35\n",
            "epoch 44\n",
            " batch Loss train: 0.048468805849552155\n",
            "i 36\n",
            "epoch 44\n",
            " batch Loss train: 0.05160834267735481\n",
            "i 37\n",
            "epoch 44\n",
            " batch Loss train: 0.03964552283287048\n",
            "i 38\n",
            "epoch 44\n",
            " batch Loss train: 0.041537001729011536\n",
            "i 39\n",
            "epoch 44\n",
            " batch Loss train: 0.04359464347362518\n",
            "i 40\n",
            "epoch 44\n",
            " batch Loss train: 0.05488179251551628\n",
            "i 41\n",
            "epoch 44\n",
            " batch Loss train: 0.04576050862669945\n",
            "i 42\n",
            "epoch 44\n",
            " batch Loss train: 0.03511711210012436\n",
            "i 43\n",
            "epoch 44\n",
            " batch Loss train: 0.0441126748919487\n",
            "i 44\n",
            "epoch 44\n",
            " batch Loss train: 0.06385762989521027\n",
            "i 45\n",
            "epoch 44\n",
            " batch Loss train: 0.04808270186185837\n",
            "i 46\n",
            "epoch 44\n",
            " batch Loss train: 0.07631530612707138\n",
            "i 47\n",
            "epoch 44\n",
            " batch Loss train: 0.04332602024078369\n",
            "i 48\n",
            "epoch 44\n",
            " batch Loss train: 0.05705789476633072\n",
            "i 49\n",
            "epoch 44\n",
            " batch Loss train: 0.053318507969379425\n",
            "i 50\n",
            "epoch 44\n",
            " batch Loss train: 0.07299848645925522\n",
            "i 51\n",
            "epoch 44\n",
            " batch Loss train: 0.04132067784667015\n",
            "i 52\n",
            "epoch 44\n",
            " batch Loss train: 0.08137647062540054\n",
            "i 53\n",
            "epoch 44\n",
            " batch Loss train: 0.07663478702306747\n",
            "i 54\n",
            "epoch 44\n",
            " batch Loss train: 0.07923878729343414\n",
            "i 55\n",
            "epoch 44\n",
            " batch Loss train: 0.053207509219646454\n",
            "i 56\n",
            "epoch 44\n",
            " batch Loss train: 0.0467684268951416\n",
            "i 57\n",
            "epoch 44\n",
            " batch Loss train: 0.06716280430555344\n",
            "i 58\n",
            "epoch 44\n",
            " batch Loss train: 0.04577671363949776\n",
            "i 59\n",
            "epoch 44\n",
            " batch Loss train: 0.047126952558755875\n",
            "i 60\n",
            "epoch 44\n",
            " batch Loss train: 0.05427554249763489\n",
            "i 61\n",
            "epoch 44\n",
            " batch Loss train: 0.057016320526599884\n",
            "i 62\n",
            "epoch 44\n",
            " batch Loss train: 0.07166428864002228\n",
            "i 63\n",
            "epoch 44\n",
            " batch Loss train: 0.0660293847322464\n",
            "i 64\n",
            "epoch 44\n",
            " batch Loss train: 0.05260002613067627\n",
            "i 65\n",
            "epoch 44\n",
            " batch Loss train: 0.07666406780481339\n",
            "i 66\n",
            "epoch 44\n",
            " batch Loss train: 0.055758994072675705\n",
            "i 67\n",
            "epoch 44\n",
            " batch Loss train: 0.05355650931596756\n",
            "i 68\n",
            "epoch 44\n",
            " batch Loss train: 0.06312406063079834\n",
            "i 69\n",
            "epoch 44\n",
            " batch Loss train: 0.061804115772247314\n",
            "i 70\n",
            "epoch 44\n",
            " batch Loss train: 0.059336110949516296\n",
            "i 71\n",
            "epoch 44\n",
            " batch Loss train: 0.055434685200452805\n",
            "i 72\n",
            "epoch 44\n",
            " batch Loss train: 0.04188266769051552\n",
            "i 73\n",
            "epoch 44\n",
            " batch Loss train: 0.04710296541452408\n",
            "i 74\n",
            "epoch 44\n",
            " batch Loss train: 0.0576583556830883\n",
            "i 75\n",
            "epoch 44\n",
            " batch Loss train: 0.06672317534685135\n",
            "i 76\n",
            "epoch 44\n",
            " batch Loss train: 0.059255462139844894\n",
            "i 77\n",
            "epoch 44\n",
            " batch Loss train: 0.0651644766330719\n",
            "i 78\n",
            "epoch 44\n",
            " batch Loss train: 0.05187985673546791\n",
            "i 79\n",
            "epoch 44\n",
            " batch Loss train: 0.04541117697954178\n",
            "i 80\n",
            "epoch 44\n",
            " batch Loss train: 0.045985084027051926\n",
            "i 81\n",
            "epoch 44\n",
            " batch Loss train: 0.0840967521071434\n",
            "i 82\n",
            "epoch 44\n",
            " batch Loss train: 0.04482987895607948\n",
            "i 83\n",
            "epoch 44\n",
            " batch Loss train: 0.04964936524629593\n",
            "i 84\n",
            "epoch 44\n",
            " batch Loss train: 0.07319191098213196\n",
            "i 85\n",
            "epoch 44\n",
            " batch Loss train: 0.03957992047071457\n",
            "i 86\n",
            "epoch 44\n",
            " batch Loss train: 0.08682931214570999\n",
            "i 87\n",
            "epoch 44\n",
            " batch Loss train: 0.04335276409983635\n",
            "i 88\n",
            "epoch 44\n",
            " batch Loss train: 0.05200202018022537\n",
            "i 89\n",
            "epoch 44\n",
            " batch Loss train: 0.0377318300306797\n",
            "i 90\n",
            "epoch 44\n",
            " batch Loss train: 0.06241855025291443\n",
            "i 91\n",
            "epoch 44\n",
            " batch Loss train: 0.050484366714954376\n",
            "i 92\n",
            "epoch 44\n",
            " batch Loss train: 0.0666738823056221\n",
            "i 93\n",
            "epoch 44\n",
            " batch Loss train: 0.055958788841962814\n",
            "i 94\n",
            "epoch 44\n",
            " batch Loss train: 0.05791502818465233\n",
            "i 95\n",
            "epoch 44\n",
            " batch Loss train: 0.036568984389305115\n",
            "i 96\n",
            "epoch 44\n",
            " batch Loss train: 0.048959389328956604\n",
            "i 97\n",
            "epoch 44\n",
            " batch Loss train: 0.05899203196167946\n",
            "i 98\n",
            "epoch 44\n",
            " batch Loss train: 0.050159357488155365\n",
            "i 99\n",
            "epoch 44\n",
            " batch Loss train: 0.06432968378067017\n",
            "i 100\n",
            "epoch 44\n",
            " batch Loss train: 0.04536047577857971\n",
            "i 101\n",
            "epoch 44\n",
            " batch Loss train: 0.048625845462083817\n",
            "i 102\n",
            "epoch 44\n",
            " batch Loss train: 0.04862268269062042\n",
            "i 103\n",
            "epoch 44\n",
            " batch Loss train: 0.03452279791235924\n",
            "i 104\n",
            "epoch 44\n",
            " batch Loss train: 0.053677741438150406\n",
            "i 105\n",
            "epoch 44\n",
            " batch Loss train: 0.06516758352518082\n",
            "i 106\n",
            "epoch 44\n",
            " batch Loss train: 0.0602271668612957\n",
            "i 107\n",
            "epoch 44\n",
            " batch Loss train: 0.05540375038981438\n",
            "i 108\n",
            "epoch 44\n",
            " batch Loss train: 0.042167581617832184\n",
            "i 109\n",
            "epoch 44\n",
            " batch Loss train: 0.05810129642486572\n",
            "i 110\n",
            "epoch 44\n",
            " batch Loss train: 0.03434591740369797\n",
            "i 111\n",
            "epoch 44\n",
            " batch Loss train: 0.09047059714794159\n",
            "i 112\n",
            "epoch 44\n",
            " batch Loss train: 0.0432082898914814\n",
            "i 113\n",
            "epoch 44\n",
            " batch Loss train: 0.04067545384168625\n",
            "i 114\n",
            "epoch 44\n",
            " batch Loss train: 0.05710229277610779\n",
            "i 115\n",
            "epoch 44\n",
            " batch Loss train: 0.055877238512039185\n",
            "i 116\n",
            "epoch 44\n",
            " batch Loss train: 0.06180068477988243\n",
            "i 117\n",
            "epoch 44\n",
            " batch Loss train: 0.05856446176767349\n",
            "i 118\n",
            "epoch 44\n",
            " batch Loss train: 0.055669091641902924\n",
            "i 119\n",
            "epoch 44\n",
            " batch Loss train: 0.05491967499256134\n",
            "i 120\n",
            "epoch 44\n",
            " batch Loss train: 0.043348390609025955\n",
            "i 121\n",
            "epoch 44\n",
            " batch Loss train: 0.04653676599264145\n",
            "i 122\n",
            "epoch 44\n",
            " batch Loss train: 0.057229988276958466\n",
            "i 123\n",
            "epoch 44\n",
            " batch Loss train: 0.05233097821474075\n",
            "i 124\n",
            "epoch 44\n",
            " batch Loss train: 0.053558409214019775\n",
            "i 125\n",
            "epoch 44\n",
            " batch Loss train: 0.054457370191812515\n",
            "i 126\n",
            "epoch 44\n",
            " batch Loss train: 0.05539650842547417\n",
            "i 127\n",
            "epoch 44\n",
            " batch Loss train: 0.0444132424890995\n",
            "i 128\n",
            "epoch 44\n",
            " batch Loss train: 0.04035750404000282\n",
            "i 129\n",
            "epoch 44\n",
            " batch Loss train: 0.05098603665828705\n",
            "i 130\n",
            "epoch 44\n",
            " batch Loss train: 0.04721516743302345\n",
            "i 131\n",
            "epoch 44\n",
            " batch Loss train: 0.04015478864312172\n",
            "i 132\n",
            "epoch 44\n",
            " batch Loss train: 0.05276772007346153\n",
            "i 133\n",
            "epoch 44\n",
            " batch Loss train: 0.051468219608068466\n",
            "i 134\n",
            "epoch 44\n",
            " batch Loss train: 0.05378399044275284\n",
            "i 135\n",
            "epoch 44\n",
            " batch Loss train: 0.04198170453310013\n",
            "i 136\n",
            "epoch 44\n",
            " batch Loss train: 0.07014989107847214\n",
            "i 137\n",
            "epoch 44\n",
            " batch Loss train: 0.03635798767209053\n",
            "i 138\n",
            "epoch 44\n",
            " batch Loss train: 0.04524734988808632\n",
            "i 139\n",
            "epoch 44\n",
            " batch Loss train: 0.05604400858283043\n",
            "i 140\n",
            "epoch 44\n",
            " batch Loss train: 0.048794738948345184\n",
            "i 141\n",
            "epoch 44\n",
            " batch Loss train: 0.05448668450117111\n",
            "i 142\n",
            "epoch 44\n",
            " batch Loss train: 0.05803550034761429\n",
            "i 143\n",
            "epoch 44\n",
            " batch Loss train: 0.05231426656246185\n",
            "i 144\n",
            "epoch 44\n",
            " batch Loss train: 0.05477796122431755\n",
            "i 145\n",
            "epoch 44\n",
            " batch Loss train: 0.03453822061419487\n",
            "i 146\n",
            "epoch 44\n",
            " batch Loss train: 0.06223868578672409\n",
            "i 147\n",
            "epoch 44\n",
            " batch Loss train: 0.049019020050764084\n",
            "i 148\n",
            "epoch 44\n",
            " batch Loss train: 0.0729486346244812\n",
            "i 149\n",
            "epoch 44\n",
            " batch Loss train: 0.0684879720211029\n",
            "i 150\n",
            "epoch 44\n",
            " batch Loss train: 0.05716599151492119\n",
            "i 151\n",
            "epoch 44\n",
            " batch Loss train: 0.055314745754003525\n",
            "i 152\n",
            "epoch 44\n",
            " batch Loss train: 0.05102144181728363\n",
            "i 153\n",
            "epoch 44\n",
            " batch Loss train: 0.06591469049453735\n",
            "i 154\n",
            "epoch 44\n",
            " batch Loss train: 0.044687557965517044\n",
            "i 155\n",
            "epoch 44\n",
            " batch Loss train: 0.07792603969573975\n",
            "i 156\n",
            "epoch 44\n",
            " batch Loss train: 0.046717289835214615\n",
            "i 157\n",
            "epoch 44\n",
            " batch Loss train: 0.05894140526652336\n",
            "i 158\n",
            "epoch 44\n",
            " batch Loss train: 0.05511770769953728\n",
            "i 159\n",
            "epoch 44\n",
            " batch Loss train: 0.06130881980061531\n",
            "i 160\n",
            "epoch 44\n",
            " batch Loss train: 0.07597766816616058\n",
            "i 161\n",
            "epoch 44\n",
            " batch Loss train: 0.06766297668218613\n",
            "i 162\n",
            "epoch 44\n",
            " batch Loss train: 0.0560835525393486\n",
            "i 163\n",
            "epoch 44\n",
            " batch Loss train: 0.057851020246744156\n",
            "i 164\n",
            "epoch 44\n",
            " batch Loss train: 0.057196225970983505\n",
            "i 165\n",
            "epoch 44\n",
            " batch Loss train: 0.07194870710372925\n",
            "i 166\n",
            "epoch 44\n",
            " batch Loss train: 0.04469885304570198\n",
            "i 167\n",
            "epoch 44\n",
            " batch Loss train: 0.061211638152599335\n",
            "i 168\n",
            "epoch 44\n",
            " batch Loss train: 0.05600157752633095\n",
            "i 169\n",
            "epoch 44\n",
            " batch Loss train: 0.059659119695425034\n",
            "i 170\n",
            "epoch 44\n",
            " batch Loss train: 0.08734603226184845\n",
            "i 171\n",
            "epoch 44\n",
            " batch Loss train: 0.06070905178785324\n",
            "i 172\n",
            "epoch 44\n",
            " batch Loss train: 0.08000683039426804\n",
            "i 173\n",
            "epoch 44\n",
            " batch Loss train: 0.06619858741760254\n",
            "i 174\n",
            "epoch 44\n",
            " batch Loss train: 0.057724934071302414\n",
            "i 175\n",
            "epoch 44\n",
            " batch Loss train: 0.05366513505578041\n",
            "i 176\n",
            "epoch 44\n",
            " batch Loss train: 0.09260696172714233\n",
            "i 177\n",
            "epoch 44\n",
            " batch Loss train: 0.06743353605270386\n",
            "i 178\n",
            "epoch 44\n",
            " batch Loss train: 0.05139516294002533\n",
            "i 179\n",
            "epoch 44\n",
            " batch Loss train: 0.05229737237095833\n",
            "i 180\n",
            "epoch 44\n",
            " batch Loss train: 0.05591874569654465\n",
            "i 181\n",
            "epoch 44\n",
            " batch Loss train: 0.06850903481245041\n",
            "i 182\n",
            "epoch 44\n",
            " batch Loss train: 0.061438292264938354\n",
            "i 183\n",
            "epoch 44\n",
            " batch Loss train: 0.061795178800821304\n",
            "i 184\n",
            "epoch 44\n",
            " batch Loss train: 0.07046061754226685\n",
            "i 185\n",
            "epoch 44\n",
            " batch Loss train: 0.04847583547234535\n",
            "i 186\n",
            "epoch 44\n",
            " batch Loss train: 0.05847693979740143\n",
            "i 187\n",
            "epoch 44\n",
            " batch Loss train: 0.06131758913397789\n",
            "i 188\n",
            "epoch 44\n",
            " batch Loss train: 0.05719829350709915\n",
            "i 189\n",
            "epoch 44\n",
            " batch Loss train: 0.0643949806690216\n",
            "i 190\n",
            "epoch 44\n",
            " batch Loss train: 0.05400148779153824\n",
            "i 191\n",
            "epoch 44\n",
            " batch Loss train: 0.05894828587770462\n",
            "i 192\n",
            "epoch 44\n",
            " batch Loss train: 0.05938940867781639\n",
            "i 193\n",
            "epoch 44\n",
            " batch Loss train: 0.054027605801820755\n",
            "i 194\n",
            "epoch 44\n",
            " batch Loss train: 0.06338967382907867\n",
            "i 195\n",
            "epoch 44\n",
            " batch Loss train: 0.05998155102133751\n",
            "i 196\n",
            "epoch 44\n",
            " batch Loss train: 0.055135250091552734\n",
            "i 197\n",
            "epoch 44\n",
            " batch Loss train: 0.06359870731830597\n",
            "i 198\n",
            "epoch 44\n",
            " batch Loss train: 0.09000079333782196\n",
            "i 199\n",
            "epoch 44\n",
            " batch Loss train: 0.03945210948586464\n",
            "i 200\n",
            "epoch 44\n",
            " batch Loss train: 0.07408728450536728\n",
            "i 201\n",
            "epoch 44\n",
            " batch Loss train: 0.04754362627863884\n",
            "i 202\n",
            "epoch 44\n",
            " batch Loss train: 0.060806676745414734\n",
            "i 203\n",
            "epoch 44\n",
            " batch Loss train: 0.060501452535390854\n",
            "i 204\n",
            "epoch 44\n",
            " batch Loss train: 0.08362166583538055\n",
            "i 205\n",
            "epoch 44\n",
            " batch Loss train: 0.04549434408545494\n",
            "i 206\n",
            "epoch 44\n",
            " batch Loss train: 0.07290657609701157\n",
            "i 207\n",
            "epoch 44\n",
            " batch Loss train: 0.0528959184885025\n",
            "i 208\n",
            "epoch 44\n",
            " batch Loss train: 0.06084311380982399\n",
            "i 209\n",
            "epoch 44\n",
            " batch Loss train: 0.05738818272948265\n",
            "i 210\n",
            "epoch 44\n",
            " batch Loss train: 0.06413718312978745\n",
            "i 211\n",
            "epoch 44\n",
            " batch Loss train: 0.06387080252170563\n",
            "i 212\n",
            "epoch 44\n",
            " batch Loss train: 0.04843452572822571\n",
            "i 213\n",
            "epoch 44\n",
            " batch Loss train: 0.07024049758911133\n",
            "i 214\n",
            "epoch 44\n",
            " batch Loss train: 0.0521140992641449\n",
            "i 215\n",
            "epoch 44\n",
            " batch Loss train: 0.04797746241092682\n",
            "i 216\n",
            "epoch 44\n",
            " batch Loss train: 0.06246643140912056\n",
            "i 217\n",
            "epoch 44\n",
            " batch Loss train: 0.06149978190660477\n",
            "i 218\n",
            "epoch 44\n",
            " batch Loss train: 0.0503418855369091\n",
            "i 219\n",
            "epoch 44\n",
            " batch Loss train: 0.04477128013968468\n",
            "i 220\n",
            "epoch 44\n",
            " batch Loss train: 0.07266824692487717\n",
            "i 221\n",
            "epoch 44\n",
            " batch Loss train: 0.05199773237109184\n",
            "i 222\n",
            "epoch 44\n",
            " batch Loss train: 0.06410757452249527\n",
            "i 223\n",
            "epoch 44\n",
            " batch Loss train: 0.051499415189027786\n",
            "i 224\n",
            "epoch 44\n",
            " batch Loss train: 0.0662960484623909\n",
            "i 225\n",
            "epoch 44\n",
            " batch Loss train: 0.0719132125377655\n",
            "i 226\n",
            "epoch 44\n",
            " batch Loss train: 0.054925378412008286\n",
            "i 227\n",
            "epoch 44\n",
            " batch Loss train: 0.05480724200606346\n",
            "i 228\n",
            "epoch 44\n",
            " batch Loss train: 0.11449863761663437\n",
            "i 229\n",
            "epoch 44\n",
            " batch Loss train: 0.04422963410615921\n",
            "i 230\n",
            "epoch 44\n",
            " batch Loss train: 0.07517441362142563\n",
            "i 231\n",
            "epoch 44\n",
            " batch Loss train: 0.05154111981391907\n",
            "i 232\n",
            "epoch 44\n",
            " batch Loss train: 0.0517815463244915\n",
            "i 233\n",
            "epoch 44\n",
            " batch Loss train: 0.0525660440325737\n",
            "i 234\n",
            "epoch 44\n",
            " batch Loss train: 0.058711644262075424\n",
            "i 235\n",
            "epoch 44\n",
            " batch Loss train: 0.0516803041100502\n",
            "i 236\n",
            "epoch 44\n",
            " batch Loss train: 0.05595102533698082\n",
            "i 237\n",
            "epoch 44\n",
            " batch Loss train: 0.0642644390463829\n",
            "i 238\n",
            "epoch 44\n",
            " batch Loss train: 0.057133715599775314\n",
            "i 239\n",
            "epoch 44\n",
            " batch Loss train: 0.05498618260025978\n",
            "i 240\n",
            "epoch 44\n",
            " batch Loss train: 0.05330437049269676\n",
            "i 241\n",
            "epoch 44\n",
            " batch Loss train: 0.05364349111914635\n",
            "i 242\n",
            "epoch 44\n",
            " batch Loss train: 0.06310945749282837\n",
            "i 243\n",
            "epoch 44\n",
            " batch Loss train: 0.053093861788511276\n",
            "i 244\n",
            "epoch 44\n",
            " batch Loss train: 0.05839207023382187\n",
            "i 245\n",
            "epoch 44\n",
            " batch Loss train: 0.05691798776388168\n",
            "i 246\n",
            "epoch 44\n",
            " batch Loss train: 0.05857020244002342\n",
            "i 247\n",
            "epoch 44\n",
            " batch Loss train: 0.06962798535823822\n",
            "i 248\n",
            "epoch 44\n",
            " batch Loss train: 0.05912167578935623\n",
            "i 249\n",
            "epoch 44\n",
            " batch Loss train: 0.04290766641497612\n",
            "i 250\n",
            "epoch 44\n",
            " batch Loss train: 0.06521551311016083\n",
            "i 251\n",
            "epoch 44\n",
            " batch Loss train: 0.04884792119264603\n",
            "i 252\n",
            "epoch 44\n",
            " batch Loss train: 0.059137012809515\n",
            "i 253\n",
            "epoch 44\n",
            " batch Loss train: 0.07552105188369751\n",
            "i 254\n",
            "epoch 44\n",
            " batch Loss train: 0.056957926601171494\n",
            "i 255\n",
            "epoch 44\n",
            " batch Loss train: 0.05788585916161537\n",
            "i 256\n",
            "epoch 44\n",
            " batch Loss train: 0.04498960077762604\n",
            "i 257\n",
            "epoch 44\n",
            " batch Loss train: 0.05284995213150978\n",
            "i 258\n",
            "epoch 44\n",
            " batch Loss train: 0.04910114407539368\n",
            "i 259\n",
            "epoch 44\n",
            " batch Loss train: 0.03859321027994156\n",
            "i 260\n",
            "epoch 44\n",
            " batch Loss train: 0.06677573174238205\n",
            "i 261\n",
            "epoch 44\n",
            " batch Loss train: 0.05762719735503197\n",
            "i 262\n",
            "epoch 44\n",
            " batch Loss train: 0.05705638974905014\n",
            "i 263\n",
            "epoch 44\n",
            " batch Loss train: 0.0738159567117691\n",
            "i 264\n",
            "epoch 44\n",
            " batch Loss train: 0.05117907375097275\n",
            "i 265\n",
            "epoch 44\n",
            " batch Loss train: 0.05227028578519821\n",
            "i 266\n",
            "epoch 44\n",
            " batch Loss train: 0.05818351358175278\n",
            "i 267\n",
            "epoch 44\n",
            " batch Loss train: 0.05234115570783615\n",
            "i 268\n",
            "epoch 44\n",
            " batch Loss train: 0.052788227796554565\n",
            "i 269\n",
            "epoch 44\n",
            " batch Loss train: 0.05838462710380554\n",
            "i 270\n",
            "epoch 44\n",
            " batch Loss train: 0.07077915966510773\n",
            "i 271\n",
            "epoch 44\n",
            " batch Loss train: 0.059083227068185806\n",
            "i 272\n",
            "epoch 44\n",
            " batch Loss train: 0.08260670304298401\n",
            "i 273\n",
            "epoch 44\n",
            " batch Loss train: 0.06438768655061722\n",
            "i 274\n",
            "epoch 44\n",
            " batch Loss train: 0.04465234652161598\n",
            "i 275\n",
            "epoch 44\n",
            " batch Loss train: 0.07845248281955719\n",
            "i 276\n",
            "epoch 44\n",
            " batch Loss train: 0.05899261310696602\n",
            "i 277\n",
            "epoch 44\n",
            " batch Loss train: 0.06982483714818954\n",
            "i 278\n",
            "epoch 44\n",
            " batch Loss train: 0.06527815759181976\n",
            "i 279\n",
            "epoch 44\n",
            " batch Loss train: 0.047507237643003464\n",
            "i 280\n",
            "epoch 44\n",
            " batch Loss train: 0.05252300947904587\n",
            "i 281\n",
            "epoch 44\n",
            " batch Loss train: 0.06950754672288895\n",
            "i 282\n",
            "epoch 44\n",
            " batch Loss train: 0.04187989979982376\n",
            "i 283\n",
            "epoch 44\n",
            " batch Loss train: 0.06265236437320709\n",
            "i 284\n",
            "epoch 44\n",
            " batch Loss train: 0.06268790364265442\n",
            "i 285\n",
            "epoch 44\n",
            " batch Loss train: 0.06291724741458893\n",
            "i 286\n",
            "epoch 44\n",
            " batch Loss train: 0.06472976505756378\n",
            "i 287\n",
            "epoch 44\n",
            " batch Loss train: 0.07860591262578964\n",
            "i 288\n",
            "epoch 44\n",
            " batch Loss train: 0.055638670921325684\n",
            "i 289\n",
            "epoch 44\n",
            " batch Loss train: 0.07766786962747574\n",
            "i 290\n",
            "epoch 44\n",
            " batch Loss train: 0.07611129432916641\n",
            "i 291\n",
            "epoch 44\n",
            " batch Loss train: 0.06799568235874176\n",
            "i 292\n",
            "epoch 44\n",
            " batch Loss train: 0.06296265870332718\n",
            "i 293\n",
            "epoch 44\n",
            " batch Loss train: 0.057245757430791855\n",
            "i 294\n",
            "epoch 44\n",
            " batch Loss train: 0.0965227261185646\n",
            "i 295\n",
            "epoch 44\n",
            " batch Loss train: 0.0779501423239708\n",
            "i 296\n",
            "epoch 44\n",
            " batch Loss train: 0.0839172899723053\n",
            "i 297\n",
            "epoch 44\n",
            " batch Loss train: 0.07842526584863663\n",
            "i 298\n",
            "epoch 44\n",
            " batch Loss train: 0.0737602487206459\n",
            "i 299\n",
            "epoch 44\n",
            " batch Loss train: 0.08488398045301437\n",
            "i 300\n",
            "epoch 44\n",
            " batch Loss train: 0.0831838846206665\n",
            "i 301\n",
            "epoch 44\n",
            " batch Loss train: 0.09031462669372559\n",
            "i 302\n",
            "epoch 44\n",
            " batch Loss train: 0.06730109453201294\n",
            "i 303\n",
            "epoch 44\n",
            " batch Loss train: 0.08481767028570175\n",
            "i 304\n",
            "epoch 44\n",
            " batch Loss train: 0.07685559242963791\n",
            "i 305\n",
            "epoch 44\n",
            " batch Loss train: 0.07401616871356964\n",
            "i 306\n",
            "epoch 44\n",
            " batch Loss train: 0.07028825581073761\n",
            "i 307\n",
            "epoch 44\n",
            " batch Loss train: 0.08398396521806717\n",
            "i 308\n",
            "epoch 44\n",
            " batch Loss train: 0.07528794556856155\n",
            "i 309\n",
            "epoch 44\n",
            " batch Loss train: 0.07612232118844986\n",
            "i 310\n",
            "epoch 44\n",
            " batch Loss train: 0.05807824432849884\n",
            "i 311\n",
            "epoch 44\n",
            " batch Loss train: 0.05895771458745003\n",
            "i 312\n",
            "epoch 44\n",
            " batch Loss train: 0.062258198857307434\n",
            "i 313\n",
            "epoch 44\n",
            " batch Loss train: 0.055996160954236984\n",
            "i 314\n",
            "epoch 44\n",
            " batch Loss train: 0.10136609524488449\n",
            "i 315\n",
            "epoch 44\n",
            " batch Loss train: 0.06746219843626022\n",
            "i 316\n",
            "epoch 44\n",
            " batch Loss train: 0.1080239787697792\n",
            "i 317\n",
            "epoch 44\n",
            " batch Loss train: 0.06324274837970734\n",
            "i 318\n",
            "epoch 44\n",
            " batch Loss train: 0.06642762571573257\n",
            "i 319\n",
            "epoch 44\n",
            " batch Loss train: 0.0734194815158844\n",
            "i 320\n",
            "epoch 44\n",
            " batch Loss train: 0.0569353811442852\n",
            "i 321\n",
            "epoch 44\n",
            " batch Loss train: 0.06450708955526352\n",
            "i 322\n",
            "epoch 44\n",
            " batch Loss train: 0.09420617669820786\n",
            "i 323\n",
            "epoch 44\n",
            " batch Loss train: 0.09710922837257385\n",
            "i 324\n",
            "epoch 44\n",
            " batch Loss train: 0.07789774239063263\n",
            "i 325\n",
            "epoch 44\n",
            " batch Loss train: 0.06980089843273163\n",
            "i 326\n",
            "epoch 44\n",
            " batch Loss train: 0.06231459975242615\n",
            "i 327\n",
            "epoch 44\n",
            " batch Loss train: 0.08364825695753098\n",
            "i 328\n",
            "epoch 44\n",
            " batch Loss train: 0.08982345461845398\n",
            "i 329\n",
            "epoch 44\n",
            " batch Loss train: 0.0667523443698883\n",
            "i 330\n",
            "epoch 44\n",
            " batch Loss train: 0.08395496755838394\n",
            "i 331\n",
            "epoch 44\n",
            " batch Loss train: 0.05999801680445671\n",
            "i 332\n",
            "epoch 44\n",
            " batch Loss train: 0.059625010937452316\n",
            "i 333\n",
            "epoch 44\n",
            " batch Loss train: 0.05731179565191269\n",
            "i 334\n",
            "epoch 44\n",
            " batch Loss train: 0.07403294742107391\n",
            "i 335\n",
            "epoch 44\n",
            " batch Loss train: 0.06751018762588501\n",
            "i 336\n",
            "epoch 44\n",
            " batch Loss train: 0.07444677501916885\n",
            "i 337\n",
            "epoch 44\n",
            " batch Loss train: 0.05110159143805504\n",
            "i 338\n",
            "epoch 44\n",
            " batch Loss train: 0.05043069273233414\n",
            "i 339\n",
            "epoch 44\n",
            " batch Loss train: 0.06854859739542007\n",
            "i 340\n",
            "epoch 44\n",
            " batch Loss train: 0.07276585698127747\n",
            "i 341\n",
            "epoch 44\n",
            " batch Loss train: 0.0790000706911087\n",
            "i 342\n",
            "epoch 44\n",
            " batch Loss train: 0.059679679572582245\n",
            "i 343\n",
            "epoch 44\n",
            " batch Loss train: 0.057206787168979645\n",
            "i 344\n",
            "epoch 44\n",
            " batch Loss train: 0.07389291375875473\n",
            "i 345\n",
            "epoch 44\n",
            " batch Loss train: 0.060973502695560455\n",
            "i 346\n",
            "epoch 44\n",
            " batch Loss train: 0.06791359931230545\n",
            "i 347\n",
            "epoch 44\n",
            " batch Loss train: 0.07315843552350998\n",
            "i 348\n",
            "epoch 44\n",
            " batch Loss train: 0.058009762316942215\n",
            "i 349\n",
            "epoch 44\n",
            " batch Loss train: 0.07543031126260757\n",
            "i 350\n",
            "epoch 44\n",
            " batch Loss train: 0.05954521521925926\n",
            "i 351\n",
            "epoch 44\n",
            " batch Loss train: 0.07608327269554138\n",
            "i 352\n",
            "epoch 44\n",
            " batch Loss train: 0.04043462499976158\n",
            "i 353\n",
            "epoch 44\n",
            " batch Loss train: 0.055013637989759445\n",
            "i 354\n",
            "epoch 44\n",
            " batch Loss train: 0.06398878991603851\n",
            "i 355\n",
            "epoch 44\n",
            " batch Loss train: 0.06124071776866913\n",
            "i 356\n",
            "epoch 44\n",
            " batch Loss train: 0.0649653896689415\n",
            "i 357\n",
            "epoch 44\n",
            " batch Loss train: 0.07195275276899338\n",
            "i 358\n",
            "epoch 44\n",
            " batch Loss train: 0.0754108726978302\n",
            "i 359\n",
            "epoch 44\n",
            " batch Loss train: 0.06396186351776123\n",
            "i 360\n",
            "epoch 44\n",
            " batch Loss train: 0.052964597940444946\n",
            "i 361\n",
            "epoch 44\n",
            " batch Loss train: 0.0680636465549469\n",
            "i 362\n",
            "epoch 44\n",
            " batch Loss train: 0.05320996046066284\n",
            "i 363\n",
            "epoch 44\n",
            " batch Loss train: 0.05860917270183563\n",
            "i 364\n",
            "epoch 44\n",
            " batch Loss train: 0.08319659531116486\n",
            "i 365\n",
            "epoch 44\n",
            " batch Loss train: 0.0588066391646862\n",
            "i 366\n",
            "epoch 44\n",
            " batch Loss train: 0.1026940792798996\n",
            "i 367\n",
            "epoch 44\n",
            " batch Loss train: 0.06823987513780594\n",
            "i 368\n",
            "epoch 44\n",
            " batch Loss train: 0.062061894685029984\n",
            "i 369\n",
            "epoch 44\n",
            " batch Loss train: 0.059061817824840546\n",
            "i 370\n",
            "epoch 44\n",
            " batch Loss train: 0.058805450797080994\n",
            "i 371\n",
            "epoch 44\n",
            " batch Loss train: 0.050070375204086304\n",
            "i 372\n",
            "epoch 44\n",
            " batch Loss train: 0.07103907316923141\n",
            "i 373\n",
            "epoch 44\n",
            " batch Loss train: 0.06417583674192429\n",
            "i 374\n",
            "epoch 44\n",
            " batch Loss train: 0.09498763829469681\n",
            "i 375\n",
            "epoch 44\n",
            " batch Loss train: 0.06159915775060654\n",
            "i 376\n",
            "epoch 44\n",
            " batch Loss train: 0.06516239792108536\n",
            "i 377\n",
            "epoch 44\n",
            " batch Loss train: 0.0780281275510788\n",
            "i 378\n",
            "epoch 44\n",
            " batch Loss train: 0.057114481925964355\n",
            "i 379\n",
            "epoch 44\n",
            " batch Loss train: 0.05512222275137901\n",
            "i 380\n",
            "epoch 44\n",
            " batch Loss train: 0.07196173071861267\n",
            "i 381\n",
            "epoch 44\n",
            " batch Loss train: 0.07527295500040054\n",
            "i 382\n",
            "epoch 44\n",
            " batch Loss train: 0.05999834090471268\n",
            "i 383\n",
            "epoch 44\n",
            " batch Loss train: 0.06782132387161255\n",
            "i 384\n",
            "epoch 44\n",
            " batch Loss train: 0.04591080918908119\n",
            "i 385\n",
            "epoch 44\n",
            " batch Loss train: 0.04792752489447594\n",
            "i 386\n",
            "epoch 44\n",
            " batch Loss train: 0.07182693481445312\n",
            "i 387\n",
            "epoch 44\n",
            " batch Loss train: 0.06506376713514328\n",
            "i 388\n",
            "epoch 44\n",
            " batch Loss train: 0.0682508796453476\n",
            "i 389\n",
            "epoch 44\n",
            " batch Loss train: 0.05059196054935455\n",
            "i 390\n",
            "epoch 44\n",
            " batch Loss train: 0.05468863621354103\n",
            "i 391\n",
            "epoch 44\n",
            " batch Loss train: 0.06350487470626831\n",
            "i 392\n",
            "epoch 44\n",
            " batch Loss train: 0.06452235579490662\n",
            "i 393\n",
            "epoch 44\n",
            " batch Loss train: 0.08192329108715057\n",
            "i 394\n",
            "epoch 44\n",
            " batch Loss train: 0.05140771344304085\n",
            "i 395\n",
            "epoch 44\n",
            " batch Loss train: 0.0655345544219017\n",
            "i 396\n",
            "epoch 44\n",
            " batch Loss train: 0.0454273521900177\n",
            "i 397\n",
            "epoch 44\n",
            " batch Loss train: 0.08084210753440857\n",
            "i 398\n",
            "epoch 44\n",
            " batch Loss train: 0.05808373540639877\n",
            "i 399\n",
            "epoch 44\n",
            " batch Loss train: 0.06638091057538986\n",
            "i 400\n",
            "epoch 44\n",
            " batch Loss train: 0.05667920038104057\n",
            "i 401\n",
            "epoch 44\n",
            " batch Loss train: 0.06598979234695435\n",
            "i 402\n",
            "epoch 44\n",
            " batch Loss train: 0.06699101626873016\n",
            "i 403\n",
            "epoch 44\n",
            " batch Loss train: 0.06540045887231827\n",
            "i 404\n",
            "epoch 44\n",
            " batch Loss train: 0.08182325214147568\n",
            "i 405\n",
            "epoch 44\n",
            " batch Loss train: 0.06729915738105774\n",
            "i 406\n",
            "epoch 44\n",
            " batch Loss train: 0.06675344705581665\n",
            "i 407\n",
            "epoch 44\n",
            " batch Loss train: 0.07869696617126465\n",
            "i 408\n",
            "epoch 44\n",
            " batch Loss train: 0.07339520752429962\n",
            "i 409\n",
            "epoch 44\n",
            " batch Loss train: 0.05663078650832176\n",
            "i 410\n",
            "epoch 44\n",
            " batch Loss train: 0.0786944255232811\n",
            "i 411\n",
            "epoch 44\n",
            " batch Loss train: 0.06604751199483871\n",
            "i 412\n",
            "epoch 44\n",
            " batch Loss train: 0.060472700744867325\n",
            "i 413\n",
            "epoch 44\n",
            " batch Loss train: 0.07959406822919846\n",
            "i 414\n",
            "epoch 44\n",
            " batch Loss train: 0.05791819468140602\n",
            "i 415\n",
            "epoch 44\n",
            " batch Loss train: 0.06665176898241043\n",
            "i 416\n",
            "epoch 44\n",
            " batch Loss train: 0.08122869580984116\n",
            "i 417\n",
            "epoch 44\n",
            " batch Loss train: 0.07309086620807648\n",
            "i 418\n",
            "epoch 44\n",
            " batch Loss train: 0.08493497222661972\n",
            "i 419\n",
            "epoch 44\n",
            " batch Loss train: 0.07176055014133453\n",
            "i 420\n",
            "epoch 44\n",
            " batch Loss train: 0.05113549530506134\n",
            "i 421\n",
            "epoch 44\n",
            " batch Loss train: 0.08137329667806625\n",
            "i 422\n",
            "epoch 44\n",
            " batch Loss train: 0.07269719988107681\n",
            "i 423\n",
            "epoch 44\n",
            " batch Loss train: 0.06585642695426941\n",
            "i 424\n",
            "epoch 44\n",
            " batch Loss train: 0.061165012419223785\n",
            "i 425\n",
            "epoch 44\n",
            " batch Loss train: 0.061309706419706345\n",
            "i 426\n",
            "epoch 44\n",
            " batch Loss train: 0.07161033153533936\n",
            "i 427\n",
            "epoch 44\n",
            " batch Loss train: 0.06273961812257767\n",
            "i 428\n",
            "epoch 44\n",
            " batch Loss train: 0.06463268399238586\n",
            "i 429\n",
            "epoch 44\n",
            " batch Loss train: 0.054374173283576965\n",
            "i 430\n",
            "epoch 44\n",
            " batch Loss train: 0.05676058679819107\n",
            "i 431\n",
            "epoch 44\n",
            " batch Loss train: 0.05615989491343498\n",
            "i 432\n",
            "epoch 44\n",
            " batch Loss train: 0.0649690106511116\n",
            "i 433\n",
            "epoch 44\n",
            " batch Loss train: 0.06582063436508179\n",
            "i 434\n",
            "epoch 44\n",
            " batch Loss train: 0.05821773037314415\n",
            "i 435\n",
            "epoch 44\n",
            " batch Loss train: 0.07827217131853104\n",
            "i 436\n",
            "epoch 44\n",
            " batch Loss train: 0.06306789070367813\n",
            "i 437\n",
            "epoch 44\n",
            " batch Loss train: 0.056046973913908005\n",
            "i 438\n",
            "epoch 44\n",
            " batch Loss train: 0.054547473788261414\n",
            "i 439\n",
            "epoch 44\n",
            " batch Loss train: 0.09614277631044388\n",
            "i 440\n",
            "epoch 44\n",
            " batch Loss train: 0.06600111722946167\n",
            "i 441\n",
            "epoch 44\n",
            " batch Loss train: 0.0602937787771225\n",
            "i 442\n",
            "epoch 44\n",
            " batch Loss train: 0.05542629957199097\n",
            "i 443\n",
            "epoch 44\n",
            " batch Loss train: 0.08065364509820938\n",
            "i 444\n",
            "epoch 44\n",
            " batch Loss train: 0.0640464648604393\n",
            "i 445\n",
            "epoch 44\n",
            " batch Loss train: 0.0513780303299427\n",
            "total epoch Loss train: tensor(0.0514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 45\n",
            " batch Loss train: 0.06565120071172714\n",
            "i 1\n",
            "epoch 45\n",
            " batch Loss train: 0.05835096910595894\n",
            "i 2\n",
            "epoch 45\n",
            " batch Loss train: 0.05743415653705597\n",
            "i 3\n",
            "epoch 45\n",
            " batch Loss train: 0.050149574875831604\n",
            "i 4\n",
            "epoch 45\n",
            " batch Loss train: 0.05699702352285385\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 45\n",
            " batch Loss train: 0.04743105173110962\n",
            "i 6\n",
            "epoch 45\n",
            " batch Loss train: 0.04341655969619751\n",
            "i 7\n",
            "epoch 45\n",
            " batch Loss train: 0.04668029397726059\n",
            "i 8\n",
            "epoch 45\n",
            " batch Loss train: 0.05326712131500244\n",
            "i 9\n",
            "epoch 45\n",
            " batch Loss train: 0.05820237472653389\n",
            "i 10\n",
            "epoch 45\n",
            " batch Loss train: 0.04152049496769905\n",
            "i 11\n",
            "epoch 45\n",
            " batch Loss train: 0.05975916609168053\n",
            "i 12\n",
            "epoch 45\n",
            " batch Loss train: 0.056214481592178345\n",
            "i 13\n",
            "epoch 45\n",
            " batch Loss train: 0.05638880282640457\n",
            "i 14\n",
            "epoch 45\n",
            " batch Loss train: 0.05815981701016426\n",
            "i 15\n",
            "epoch 45\n",
            " batch Loss train: 0.05300465226173401\n",
            "i 16\n",
            "epoch 45\n",
            " batch Loss train: 0.04268999770283699\n",
            "i 17\n",
            "epoch 45\n",
            " batch Loss train: 0.045490752905607224\n",
            "i 18\n",
            "epoch 45\n",
            " batch Loss train: 0.049899693578481674\n",
            "i 19\n",
            "epoch 45\n",
            " batch Loss train: 0.054018713533878326\n",
            "i 20\n",
            "epoch 45\n",
            " batch Loss train: 0.0361555777490139\n",
            "i 21\n",
            "epoch 45\n",
            " batch Loss train: 0.0607793964445591\n",
            "i 22\n",
            "epoch 45\n",
            " batch Loss train: 0.043717317283153534\n",
            "i 23\n",
            "epoch 45\n",
            " batch Loss train: 0.055339209735393524\n",
            "i 24\n",
            "epoch 45\n",
            " batch Loss train: 0.045540742576122284\n",
            "i 25\n",
            "epoch 45\n",
            " batch Loss train: 0.04509221389889717\n",
            "i 26\n",
            "epoch 45\n",
            " batch Loss train: 0.05529376119375229\n",
            "i 27\n",
            "epoch 45\n",
            " batch Loss train: 0.044774092733860016\n",
            "i 28\n",
            "epoch 45\n",
            " batch Loss train: 0.04566154628992081\n",
            "i 29\n",
            "epoch 45\n",
            " batch Loss train: 0.04894736036658287\n",
            "i 30\n",
            "epoch 45\n",
            " batch Loss train: 0.03642921522259712\n",
            "i 31\n",
            "epoch 45\n",
            " batch Loss train: 0.05150775611400604\n",
            "i 32\n",
            "epoch 45\n",
            " batch Loss train: 0.07829390466213226\n",
            "i 33\n",
            "epoch 45\n",
            " batch Loss train: 0.055130355060100555\n",
            "i 34\n",
            "epoch 45\n",
            " batch Loss train: 0.05006782338023186\n",
            "i 35\n",
            "epoch 45\n",
            " batch Loss train: 0.06518181413412094\n",
            "i 36\n",
            "epoch 45\n",
            " batch Loss train: 0.06542091071605682\n",
            "i 37\n",
            "epoch 45\n",
            " batch Loss train: 0.04994628578424454\n",
            "i 38\n",
            "epoch 45\n",
            " batch Loss train: 0.047215454280376434\n",
            "i 39\n",
            "epoch 45\n",
            " batch Loss train: 0.04559499770402908\n",
            "i 40\n",
            "epoch 45\n",
            " batch Loss train: 0.049288004636764526\n",
            "i 41\n",
            "epoch 45\n",
            " batch Loss train: 0.04483876749873161\n",
            "i 42\n",
            "epoch 45\n",
            " batch Loss train: 0.052670057862997055\n",
            "i 43\n",
            "epoch 45\n",
            " batch Loss train: 0.06862210482358932\n",
            "i 44\n",
            "epoch 45\n",
            " batch Loss train: 0.03941607475280762\n",
            "i 45\n",
            "epoch 45\n",
            " batch Loss train: 0.04706640541553497\n",
            "i 46\n",
            "epoch 45\n",
            " batch Loss train: 0.06002392992377281\n",
            "i 47\n",
            "epoch 45\n",
            " batch Loss train: 0.05157572031021118\n",
            "i 48\n",
            "epoch 45\n",
            " batch Loss train: 0.03871295973658562\n",
            "i 49\n",
            "epoch 45\n",
            " batch Loss train: 0.059042882174253464\n",
            "i 50\n",
            "epoch 45\n",
            " batch Loss train: 0.05362587049603462\n",
            "i 51\n",
            "epoch 45\n",
            " batch Loss train: 0.048810847103595734\n",
            "i 52\n",
            "epoch 45\n",
            " batch Loss train: 0.07349197566509247\n",
            "i 53\n",
            "epoch 45\n",
            " batch Loss train: 0.06451959908008575\n",
            "i 54\n",
            "epoch 45\n",
            " batch Loss train: 0.040212925523519516\n",
            "i 55\n",
            "epoch 45\n",
            " batch Loss train: 0.06733819842338562\n",
            "i 56\n",
            "epoch 45\n",
            " batch Loss train: 0.04379953071475029\n",
            "i 57\n",
            "epoch 45\n",
            " batch Loss train: 0.049106571823358536\n",
            "i 58\n",
            "epoch 45\n",
            " batch Loss train: 0.05686106160283089\n",
            "i 59\n",
            "epoch 45\n",
            " batch Loss train: 0.05883355811238289\n",
            "i 60\n",
            "epoch 45\n",
            " batch Loss train: 0.05158139392733574\n",
            "i 61\n",
            "epoch 45\n",
            " batch Loss train: 0.08839815855026245\n",
            "i 62\n",
            "epoch 45\n",
            " batch Loss train: 0.039956118911504745\n",
            "i 63\n",
            "epoch 45\n",
            " batch Loss train: 0.05784297361969948\n",
            "i 64\n",
            "epoch 45\n",
            " batch Loss train: 0.04747277498245239\n",
            "i 65\n",
            "epoch 45\n",
            " batch Loss train: 0.04481594264507294\n",
            "i 66\n",
            "epoch 45\n",
            " batch Loss train: 0.04691583290696144\n",
            "i 67\n",
            "epoch 45\n",
            " batch Loss train: 0.049014072865247726\n",
            "i 68\n",
            "epoch 45\n",
            " batch Loss train: 0.06939759850502014\n",
            "i 69\n",
            "epoch 45\n",
            " batch Loss train: 0.07491596788167953\n",
            "i 70\n",
            "epoch 45\n",
            " batch Loss train: 0.04239505156874657\n",
            "i 71\n",
            "epoch 45\n",
            " batch Loss train: 0.06355076283216476\n",
            "i 72\n",
            "epoch 45\n",
            " batch Loss train: 0.03958101198077202\n",
            "i 73\n",
            "epoch 45\n",
            " batch Loss train: 0.041709572076797485\n",
            "i 74\n",
            "epoch 45\n",
            " batch Loss train: 0.05923447385430336\n",
            "i 75\n",
            "epoch 45\n",
            " batch Loss train: 0.07630246132612228\n",
            "i 76\n",
            "epoch 45\n",
            " batch Loss train: 0.05074390396475792\n",
            "i 77\n",
            "epoch 45\n",
            " batch Loss train: 0.05386598780751228\n",
            "i 78\n",
            "epoch 45\n",
            " batch Loss train: 0.07755690068006516\n",
            "i 79\n",
            "epoch 45\n",
            " batch Loss train: 0.048128221184015274\n",
            "i 80\n",
            "epoch 45\n",
            " batch Loss train: 0.04656591638922691\n",
            "i 81\n",
            "epoch 45\n",
            " batch Loss train: 0.0638635903596878\n",
            "i 82\n",
            "epoch 45\n",
            " batch Loss train: 0.04490907862782478\n",
            "i 83\n",
            "epoch 45\n",
            " batch Loss train: 0.06823848187923431\n",
            "i 84\n",
            "epoch 45\n",
            " batch Loss train: 0.06743995100259781\n",
            "i 85\n",
            "epoch 45\n",
            " batch Loss train: 0.05826025456190109\n",
            "i 86\n",
            "epoch 45\n",
            " batch Loss train: 0.04928054288029671\n",
            "i 87\n",
            "epoch 45\n",
            " batch Loss train: 0.05664939805865288\n",
            "i 88\n",
            "epoch 45\n",
            " batch Loss train: 0.05030948668718338\n",
            "i 89\n",
            "epoch 45\n",
            " batch Loss train: 0.043289851397275925\n",
            "i 90\n",
            "epoch 45\n",
            " batch Loss train: 0.04669877886772156\n",
            "i 91\n",
            "epoch 45\n",
            " batch Loss train: 0.05732596665620804\n",
            "i 92\n",
            "epoch 45\n",
            " batch Loss train: 0.04598276689648628\n",
            "i 93\n",
            "epoch 45\n",
            " batch Loss train: 0.07902276515960693\n",
            "i 94\n",
            "epoch 45\n",
            " batch Loss train: 0.06987518072128296\n",
            "i 95\n",
            "epoch 45\n",
            " batch Loss train: 0.05834709480404854\n",
            "i 96\n",
            "epoch 45\n",
            " batch Loss train: 0.0605282336473465\n",
            "i 97\n",
            "epoch 45\n",
            " batch Loss train: 0.060218583792448044\n",
            "i 98\n",
            "epoch 45\n",
            " batch Loss train: 0.05269602686166763\n",
            "i 99\n",
            "epoch 45\n",
            " batch Loss train: 0.05161314457654953\n",
            "i 100\n",
            "epoch 45\n",
            " batch Loss train: 0.06437965482473373\n",
            "i 101\n",
            "epoch 45\n",
            " batch Loss train: 0.0506562814116478\n",
            "i 102\n",
            "epoch 45\n",
            " batch Loss train: 0.04974719509482384\n",
            "i 103\n",
            "epoch 45\n",
            " batch Loss train: 0.046828001737594604\n",
            "i 104\n",
            "epoch 45\n",
            " batch Loss train: 0.05588347464799881\n",
            "i 105\n",
            "epoch 45\n",
            " batch Loss train: 0.05604080110788345\n",
            "i 106\n",
            "epoch 45\n",
            " batch Loss train: 0.04618169739842415\n",
            "i 107\n",
            "epoch 45\n",
            " batch Loss train: 0.04617562144994736\n",
            "i 108\n",
            "epoch 45\n",
            " batch Loss train: 0.05769029259681702\n",
            "i 109\n",
            "epoch 45\n",
            " batch Loss train: 0.036638449877500534\n",
            "i 110\n",
            "epoch 45\n",
            " batch Loss train: 0.0535067655146122\n",
            "i 111\n",
            "epoch 45\n",
            " batch Loss train: 0.06635958701372147\n",
            "i 112\n",
            "epoch 45\n",
            " batch Loss train: 0.050975680351257324\n",
            "i 113\n",
            "epoch 45\n",
            " batch Loss train: 0.07105205208063126\n",
            "i 114\n",
            "epoch 45\n",
            " batch Loss train: 0.057195086032152176\n",
            "i 115\n",
            "epoch 45\n",
            " batch Loss train: 0.05855717882514\n",
            "i 116\n",
            "epoch 45\n",
            " batch Loss train: 0.06133020669221878\n",
            "i 117\n",
            "epoch 45\n",
            " batch Loss train: 0.0627991333603859\n",
            "i 118\n",
            "epoch 45\n",
            " batch Loss train: 0.06742927432060242\n",
            "i 119\n",
            "epoch 45\n",
            " batch Loss train: 0.07435227930545807\n",
            "i 120\n",
            "epoch 45\n",
            " batch Loss train: 0.07066337764263153\n",
            "i 121\n",
            "epoch 45\n",
            " batch Loss train: 0.05569007247686386\n",
            "i 122\n",
            "epoch 45\n",
            " batch Loss train: 0.03700942173600197\n",
            "i 123\n",
            "epoch 45\n",
            " batch Loss train: 0.05508612468838692\n",
            "i 124\n",
            "epoch 45\n",
            " batch Loss train: 0.06024846062064171\n",
            "i 125\n",
            "epoch 45\n",
            " batch Loss train: 0.05766988918185234\n",
            "i 126\n",
            "epoch 45\n",
            " batch Loss train: 0.05268554762005806\n",
            "i 127\n",
            "epoch 45\n",
            " batch Loss train: 0.060569293797016144\n",
            "i 128\n",
            "epoch 45\n",
            " batch Loss train: 0.05542309954762459\n",
            "i 129\n",
            "epoch 45\n",
            " batch Loss train: 0.061224598437547684\n",
            "i 130\n",
            "epoch 45\n",
            " batch Loss train: 0.07775146514177322\n",
            "i 131\n",
            "epoch 45\n",
            " batch Loss train: 0.06538175046443939\n",
            "i 132\n",
            "epoch 45\n",
            " batch Loss train: 0.054653242230415344\n",
            "i 133\n",
            "epoch 45\n",
            " batch Loss train: 0.04524804279208183\n",
            "i 134\n",
            "epoch 45\n",
            " batch Loss train: 0.04850021004676819\n",
            "i 135\n",
            "epoch 45\n",
            " batch Loss train: 0.07603741437196732\n",
            "i 136\n",
            "epoch 45\n",
            " batch Loss train: 0.06303764134645462\n",
            "i 137\n",
            "epoch 45\n",
            " batch Loss train: 0.05347753316164017\n",
            "i 138\n",
            "epoch 45\n",
            " batch Loss train: 0.049535684287548065\n",
            "i 139\n",
            "epoch 45\n",
            " batch Loss train: 0.05743178725242615\n",
            "i 140\n",
            "epoch 45\n",
            " batch Loss train: 0.055817294865846634\n",
            "i 141\n",
            "epoch 45\n",
            " batch Loss train: 0.07661743462085724\n",
            "i 142\n",
            "epoch 45\n",
            " batch Loss train: 0.04871094971895218\n",
            "i 143\n",
            "epoch 45\n",
            " batch Loss train: 0.05513356626033783\n",
            "i 144\n",
            "epoch 45\n",
            " batch Loss train: 0.05683586746454239\n",
            "i 145\n",
            "epoch 45\n",
            " batch Loss train: 0.049820367246866226\n",
            "i 146\n",
            "epoch 45\n",
            " batch Loss train: 0.05065857991576195\n",
            "i 147\n",
            "epoch 45\n",
            " batch Loss train: 0.05249384418129921\n",
            "i 148\n",
            "epoch 45\n",
            " batch Loss train: 0.0589153990149498\n",
            "i 149\n",
            "epoch 45\n",
            " batch Loss train: 0.06052188202738762\n",
            "i 150\n",
            "epoch 45\n",
            " batch Loss train: 0.08791470527648926\n",
            "i 151\n",
            "epoch 45\n",
            " batch Loss train: 0.052989017218351364\n",
            "i 152\n",
            "epoch 45\n",
            " batch Loss train: 0.06892717629671097\n",
            "i 153\n",
            "epoch 45\n",
            " batch Loss train: 0.04347839206457138\n",
            "i 154\n",
            "epoch 45\n",
            " batch Loss train: 0.06091023236513138\n",
            "i 155\n",
            "epoch 45\n",
            " batch Loss train: 0.0535837784409523\n",
            "i 156\n",
            "epoch 45\n",
            " batch Loss train: 0.0417109914124012\n",
            "i 157\n",
            "epoch 45\n",
            " batch Loss train: 0.05280110239982605\n",
            "i 158\n",
            "epoch 45\n",
            " batch Loss train: 0.055185701698064804\n",
            "i 159\n",
            "epoch 45\n",
            " batch Loss train: 0.07070302963256836\n",
            "i 160\n",
            "epoch 45\n",
            " batch Loss train: 0.05171964690089226\n",
            "i 161\n",
            "epoch 45\n",
            " batch Loss train: 0.08068472146987915\n",
            "i 162\n",
            "epoch 45\n",
            " batch Loss train: 0.05875832214951515\n",
            "i 163\n",
            "epoch 45\n",
            " batch Loss train: 0.06985332071781158\n",
            "i 164\n",
            "epoch 45\n",
            " batch Loss train: 0.03770732507109642\n",
            "i 165\n",
            "epoch 45\n",
            " batch Loss train: 0.08123248815536499\n",
            "i 166\n",
            "epoch 45\n",
            " batch Loss train: 0.06058342009782791\n",
            "i 167\n",
            "epoch 45\n",
            " batch Loss train: 0.052462827414274216\n",
            "i 168\n",
            "epoch 45\n",
            " batch Loss train: 0.04449561983346939\n",
            "i 169\n",
            "epoch 45\n",
            " batch Loss train: 0.058601945638656616\n",
            "i 170\n",
            "epoch 45\n",
            " batch Loss train: 0.06884011626243591\n",
            "i 171\n",
            "epoch 45\n",
            " batch Loss train: 0.07122175395488739\n",
            "i 172\n",
            "epoch 45\n",
            " batch Loss train: 0.0473017655313015\n",
            "i 173\n",
            "epoch 45\n",
            " batch Loss train: 0.05725528299808502\n",
            "i 174\n",
            "epoch 45\n",
            " batch Loss train: 0.0520499162375927\n",
            "i 175\n",
            "epoch 45\n",
            " batch Loss train: 0.04399416968226433\n",
            "i 176\n",
            "epoch 45\n",
            " batch Loss train: 0.06347202509641647\n",
            "i 177\n",
            "epoch 45\n",
            " batch Loss train: 0.04086357727646828\n",
            "i 178\n",
            "epoch 45\n",
            " batch Loss train: 0.0667489692568779\n",
            "i 179\n",
            "epoch 45\n",
            " batch Loss train: 0.06592107564210892\n",
            "i 180\n",
            "epoch 45\n",
            " batch Loss train: 0.0500970333814621\n",
            "i 181\n",
            "epoch 45\n",
            " batch Loss train: 0.06181975454092026\n",
            "i 182\n",
            "epoch 45\n",
            " batch Loss train: 0.06802237778902054\n",
            "i 183\n",
            "epoch 45\n",
            " batch Loss train: 0.057462189346551895\n",
            "i 184\n",
            "epoch 45\n",
            " batch Loss train: 0.052956562489271164\n",
            "i 185\n",
            "epoch 45\n",
            " batch Loss train: 0.07152538746595383\n",
            "i 186\n",
            "epoch 45\n",
            " batch Loss train: 0.06193910166621208\n",
            "i 187\n",
            "epoch 45\n",
            " batch Loss train: 0.06341879069805145\n",
            "i 188\n",
            "epoch 45\n",
            " batch Loss train: 0.09507002681493759\n",
            "i 189\n",
            "epoch 45\n",
            " batch Loss train: 0.08505477756261826\n",
            "i 190\n",
            "epoch 45\n",
            " batch Loss train: 0.0702352300286293\n",
            "i 191\n",
            "epoch 45\n",
            " batch Loss train: 0.053423922508955\n",
            "i 192\n",
            "epoch 45\n",
            " batch Loss train: 0.05195731297135353\n",
            "i 193\n",
            "epoch 45\n",
            " batch Loss train: 0.053989019244909286\n",
            "i 194\n",
            "epoch 45\n",
            " batch Loss train: 0.057244714349508286\n",
            "i 195\n",
            "epoch 45\n",
            " batch Loss train: 0.049532871693372726\n",
            "i 196\n",
            "epoch 45\n",
            " batch Loss train: 0.06988328695297241\n",
            "i 197\n",
            "epoch 45\n",
            " batch Loss train: 0.06615382432937622\n",
            "i 198\n",
            "epoch 45\n",
            " batch Loss train: 0.058130133897066116\n",
            "i 199\n",
            "epoch 45\n",
            " batch Loss train: 0.086148701608181\n",
            "i 200\n",
            "epoch 45\n",
            " batch Loss train: 0.05230848863720894\n",
            "i 201\n",
            "epoch 45\n",
            " batch Loss train: 0.08932436257600784\n",
            "i 202\n",
            "epoch 45\n",
            " batch Loss train: 0.05635608360171318\n",
            "i 203\n",
            "epoch 45\n",
            " batch Loss train: 0.04367564246058464\n",
            "i 204\n",
            "epoch 45\n",
            " batch Loss train: 0.07238558679819107\n",
            "i 205\n",
            "epoch 45\n",
            " batch Loss train: 0.07590959221124649\n",
            "i 206\n",
            "epoch 45\n",
            " batch Loss train: 0.04843234643340111\n",
            "i 207\n",
            "epoch 45\n",
            " batch Loss train: 0.05984317138791084\n",
            "i 208\n",
            "epoch 45\n",
            " batch Loss train: 0.06295478343963623\n",
            "i 209\n",
            "epoch 45\n",
            " batch Loss train: 0.05908622220158577\n",
            "i 210\n",
            "epoch 45\n",
            " batch Loss train: 0.05302073061466217\n",
            "i 211\n",
            "epoch 45\n",
            " batch Loss train: 0.0575832836329937\n",
            "i 212\n",
            "epoch 45\n",
            " batch Loss train: 0.05108615756034851\n",
            "i 213\n",
            "epoch 45\n",
            " batch Loss train: 0.07931932806968689\n",
            "i 214\n",
            "epoch 45\n",
            " batch Loss train: 0.04611636698246002\n",
            "i 215\n",
            "epoch 45\n",
            " batch Loss train: 0.05645986273884773\n",
            "i 216\n",
            "epoch 45\n",
            " batch Loss train: 0.06426812708377838\n",
            "i 217\n",
            "epoch 45\n",
            " batch Loss train: 0.05605577677488327\n",
            "i 218\n",
            "epoch 45\n",
            " batch Loss train: 0.05726160481572151\n",
            "i 219\n",
            "epoch 45\n",
            " batch Loss train: 0.06460843980312347\n",
            "i 220\n",
            "epoch 45\n",
            " batch Loss train: 0.0772733986377716\n",
            "i 221\n",
            "epoch 45\n",
            " batch Loss train: 0.05455638840794563\n",
            "i 222\n",
            "epoch 45\n",
            " batch Loss train: 0.048069216310977936\n",
            "i 223\n",
            "epoch 45\n",
            " batch Loss train: 0.06277918070554733\n",
            "i 224\n",
            "epoch 45\n",
            " batch Loss train: 0.04864421486854553\n",
            "i 225\n",
            "epoch 45\n",
            " batch Loss train: 0.05442975461483002\n",
            "i 226\n",
            "epoch 45\n",
            " batch Loss train: 0.0499267503619194\n",
            "i 227\n",
            "epoch 45\n",
            " batch Loss train: 0.058102868497371674\n",
            "i 228\n",
            "epoch 45\n",
            " batch Loss train: 0.04471543803811073\n",
            "i 229\n",
            "epoch 45\n",
            " batch Loss train: 0.06267747282981873\n",
            "i 230\n",
            "epoch 45\n",
            " batch Loss train: 0.07219259440898895\n",
            "i 231\n",
            "epoch 45\n",
            " batch Loss train: 0.05556735023856163\n",
            "i 232\n",
            "epoch 45\n",
            " batch Loss train: 0.0755479484796524\n",
            "i 233\n",
            "epoch 45\n",
            " batch Loss train: 0.05559483543038368\n",
            "i 234\n",
            "epoch 45\n",
            " batch Loss train: 0.04786890372633934\n",
            "i 235\n",
            "epoch 45\n",
            " batch Loss train: 0.06278064101934433\n",
            "i 236\n",
            "epoch 45\n",
            " batch Loss train: 0.05177650973200798\n",
            "i 237\n",
            "epoch 45\n",
            " batch Loss train: 0.049114108085632324\n",
            "i 238\n",
            "epoch 45\n",
            " batch Loss train: 0.060836177319288254\n",
            "i 239\n",
            "epoch 45\n",
            " batch Loss train: 0.05358609929680824\n",
            "i 240\n",
            "epoch 45\n",
            " batch Loss train: 0.06932397931814194\n",
            "i 241\n",
            "epoch 45\n",
            " batch Loss train: 0.055447354912757874\n",
            "i 242\n",
            "epoch 45\n",
            " batch Loss train: 0.04307346045970917\n",
            "i 243\n",
            "epoch 45\n",
            " batch Loss train: 0.03506302461028099\n",
            "i 244\n",
            "epoch 45\n",
            " batch Loss train: 0.03856329247355461\n",
            "i 245\n",
            "epoch 45\n",
            " batch Loss train: 0.0772172287106514\n",
            "i 246\n",
            "epoch 45\n",
            " batch Loss train: 0.07257656008005142\n",
            "i 247\n",
            "epoch 45\n",
            " batch Loss train: 0.05480380356311798\n",
            "i 248\n",
            "epoch 45\n",
            " batch Loss train: 0.05689186230301857\n",
            "i 249\n",
            "epoch 45\n",
            " batch Loss train: 0.06552057713270187\n",
            "i 250\n",
            "epoch 45\n",
            " batch Loss train: 0.05996600538492203\n",
            "i 251\n",
            "epoch 45\n",
            " batch Loss train: 0.051901452243328094\n",
            "i 252\n",
            "epoch 45\n",
            " batch Loss train: 0.055302251130342484\n",
            "i 253\n",
            "epoch 45\n",
            " batch Loss train: 0.07806035131216049\n",
            "i 254\n",
            "epoch 45\n",
            " batch Loss train: 0.06335420161485672\n",
            "i 255\n",
            "epoch 45\n",
            " batch Loss train: 0.06352798640727997\n",
            "i 256\n",
            "epoch 45\n",
            " batch Loss train: 0.05304328724741936\n",
            "i 257\n",
            "epoch 45\n",
            " batch Loss train: 0.04913358390331268\n",
            "i 258\n",
            "epoch 45\n",
            " batch Loss train: 0.05198116600513458\n",
            "i 259\n",
            "epoch 45\n",
            " batch Loss train: 0.04507765173912048\n",
            "i 260\n",
            "epoch 45\n",
            " batch Loss train: 0.0517633892595768\n",
            "i 261\n",
            "epoch 45\n",
            " batch Loss train: 0.046321503818035126\n",
            "i 262\n",
            "epoch 45\n",
            " batch Loss train: 0.07497464120388031\n",
            "i 263\n",
            "epoch 45\n",
            " batch Loss train: 0.04416085034608841\n",
            "i 264\n",
            "epoch 45\n",
            " batch Loss train: 0.057302359491586685\n",
            "i 265\n",
            "epoch 45\n",
            " batch Loss train: 0.04971778020262718\n",
            "i 266\n",
            "epoch 45\n",
            " batch Loss train: 0.0639130100607872\n",
            "i 267\n",
            "epoch 45\n",
            " batch Loss train: 0.07434428483247757\n",
            "i 268\n",
            "epoch 45\n",
            " batch Loss train: 0.058335307985544205\n",
            "i 269\n",
            "epoch 45\n",
            " batch Loss train: 0.04953252896666527\n",
            "i 270\n",
            "epoch 45\n",
            " batch Loss train: 0.07440222799777985\n",
            "i 271\n",
            "epoch 45\n",
            " batch Loss train: 0.04849817231297493\n",
            "i 272\n",
            "epoch 45\n",
            " batch Loss train: 0.04482444375753403\n",
            "i 273\n",
            "epoch 45\n",
            " batch Loss train: 0.05397021397948265\n",
            "i 274\n",
            "epoch 45\n",
            " batch Loss train: 0.06767399609088898\n",
            "i 275\n",
            "epoch 45\n",
            " batch Loss train: 0.04237651079893112\n",
            "i 276\n",
            "epoch 45\n",
            " batch Loss train: 0.06347724795341492\n",
            "i 277\n",
            "epoch 45\n",
            " batch Loss train: 0.06942509859800339\n",
            "i 278\n",
            "epoch 45\n",
            " batch Loss train: 0.05325166508555412\n",
            "i 279\n",
            "epoch 45\n",
            " batch Loss train: 0.06153422221541405\n",
            "i 280\n",
            "epoch 45\n",
            " batch Loss train: 0.06162003055214882\n",
            "i 281\n",
            "epoch 45\n",
            " batch Loss train: 0.06524356454610825\n",
            "i 282\n",
            "epoch 45\n",
            " batch Loss train: 0.039405107498168945\n",
            "i 283\n",
            "epoch 45\n",
            " batch Loss train: 0.06338624656200409\n",
            "i 284\n",
            "epoch 45\n",
            " batch Loss train: 0.054978352040052414\n",
            "i 285\n",
            "epoch 45\n",
            " batch Loss train: 0.054325710982084274\n",
            "i 286\n",
            "epoch 45\n",
            " batch Loss train: 0.05047948658466339\n",
            "i 287\n",
            "epoch 45\n",
            " batch Loss train: 0.069001205265522\n",
            "i 288\n",
            "epoch 45\n",
            " batch Loss train: 0.06375092267990112\n",
            "i 289\n",
            "epoch 45\n",
            " batch Loss train: 0.060623519122600555\n",
            "i 290\n",
            "epoch 45\n",
            " batch Loss train: 0.04751460626721382\n",
            "i 291\n",
            "epoch 45\n",
            " batch Loss train: 0.044510066509246826\n",
            "i 292\n",
            "epoch 45\n",
            " batch Loss train: 0.05241462215781212\n",
            "i 293\n",
            "epoch 45\n",
            " batch Loss train: 0.0572575107216835\n",
            "i 294\n",
            "epoch 45\n",
            " batch Loss train: 0.05586571246385574\n",
            "i 295\n",
            "epoch 45\n",
            " batch Loss train: 0.051333993673324585\n",
            "i 296\n",
            "epoch 45\n",
            " batch Loss train: 0.040530793368816376\n",
            "i 297\n",
            "epoch 45\n",
            " batch Loss train: 0.07532193511724472\n",
            "i 298\n",
            "epoch 45\n",
            " batch Loss train: 0.07764622569084167\n",
            "i 299\n",
            "epoch 45\n",
            " batch Loss train: 0.06590098142623901\n",
            "i 300\n",
            "epoch 45\n",
            " batch Loss train: 0.04899332672357559\n",
            "i 301\n",
            "epoch 45\n",
            " batch Loss train: 0.05367220565676689\n",
            "i 302\n",
            "epoch 45\n",
            " batch Loss train: 0.05612450838088989\n",
            "i 303\n",
            "epoch 45\n",
            " batch Loss train: 0.06631316989660263\n",
            "i 304\n",
            "epoch 45\n",
            " batch Loss train: 0.05511683225631714\n",
            "i 305\n",
            "epoch 45\n",
            " batch Loss train: 0.06505506485700607\n",
            "i 306\n",
            "epoch 45\n",
            " batch Loss train: 0.06029803678393364\n",
            "i 307\n",
            "epoch 45\n",
            " batch Loss train: 0.050797030329704285\n",
            "i 308\n",
            "epoch 45\n",
            " batch Loss train: 0.0571729801595211\n",
            "i 309\n",
            "epoch 45\n",
            " batch Loss train: 0.06639055907726288\n",
            "i 310\n",
            "epoch 45\n",
            " batch Loss train: 0.04306004196405411\n",
            "i 311\n",
            "epoch 45\n",
            " batch Loss train: 0.06889694929122925\n",
            "i 312\n",
            "epoch 45\n",
            " batch Loss train: 0.05065172538161278\n",
            "i 313\n",
            "epoch 45\n",
            " batch Loss train: 0.08553460985422134\n",
            "i 314\n",
            "epoch 45\n",
            " batch Loss train: 0.05852824077010155\n",
            "i 315\n",
            "epoch 45\n",
            " batch Loss train: 0.06390885263681412\n",
            "i 316\n",
            "epoch 45\n",
            " batch Loss train: 0.060519028455019\n",
            "i 317\n",
            "epoch 45\n",
            " batch Loss train: 0.06178021803498268\n",
            "i 318\n",
            "epoch 45\n",
            " batch Loss train: 0.06983377784490585\n",
            "i 319\n",
            "epoch 45\n",
            " batch Loss train: 0.08101170510053635\n",
            "i 320\n",
            "epoch 45\n",
            " batch Loss train: 0.04377448558807373\n",
            "i 321\n",
            "epoch 45\n",
            " batch Loss train: 0.05272693559527397\n",
            "i 322\n",
            "epoch 45\n",
            " batch Loss train: 0.06988909840583801\n",
            "i 323\n",
            "epoch 45\n",
            " batch Loss train: 0.08740326017141342\n",
            "i 324\n",
            "epoch 45\n",
            " batch Loss train: 0.06087466701865196\n",
            "i 325\n",
            "epoch 45\n",
            " batch Loss train: 0.056020140647888184\n",
            "i 326\n",
            "epoch 45\n",
            " batch Loss train: 0.0797901451587677\n",
            "i 327\n",
            "epoch 45\n",
            " batch Loss train: 0.05908641964197159\n",
            "i 328\n",
            "epoch 45\n",
            " batch Loss train: 0.06158766150474548\n",
            "i 329\n",
            "epoch 45\n",
            " batch Loss train: 0.055022045969963074\n",
            "i 330\n",
            "epoch 45\n",
            " batch Loss train: 0.05056328326463699\n",
            "i 331\n",
            "epoch 45\n",
            " batch Loss train: 0.06527148932218552\n",
            "i 332\n",
            "epoch 45\n",
            " batch Loss train: 0.07454559952020645\n",
            "i 333\n",
            "epoch 45\n",
            " batch Loss train: 0.060184236615896225\n",
            "i 334\n",
            "epoch 45\n",
            " batch Loss train: 0.07394161820411682\n",
            "i 335\n",
            "epoch 45\n",
            " batch Loss train: 0.06586310267448425\n",
            "i 336\n",
            "epoch 45\n",
            " batch Loss train: 0.06717168539762497\n",
            "i 337\n",
            "epoch 45\n",
            " batch Loss train: 0.052199143916368484\n",
            "i 338\n",
            "epoch 45\n",
            " batch Loss train: 0.08970639109611511\n",
            "i 339\n",
            "epoch 45\n",
            " batch Loss train: 0.07350973784923553\n",
            "i 340\n",
            "epoch 45\n",
            " batch Loss train: 0.05851075053215027\n",
            "i 341\n",
            "epoch 45\n",
            " batch Loss train: 0.08780807256698608\n",
            "i 342\n",
            "epoch 45\n",
            " batch Loss train: 0.061082422733306885\n",
            "i 343\n",
            "epoch 45\n",
            " batch Loss train: 0.05442352220416069\n",
            "i 344\n",
            "epoch 45\n",
            " batch Loss train: 0.061750441789627075\n",
            "i 345\n",
            "epoch 45\n",
            " batch Loss train: 0.06453195214271545\n",
            "i 346\n",
            "epoch 45\n",
            " batch Loss train: 0.045852161943912506\n",
            "i 347\n",
            "epoch 45\n",
            " batch Loss train: 0.07055190950632095\n",
            "i 348\n",
            "epoch 45\n",
            " batch Loss train: 0.05787542089819908\n",
            "i 349\n",
            "epoch 45\n",
            " batch Loss train: 0.05952441319823265\n",
            "i 350\n",
            "epoch 45\n",
            " batch Loss train: 0.07088722288608551\n",
            "i 351\n",
            "epoch 45\n",
            " batch Loss train: 0.05172939598560333\n",
            "i 352\n",
            "epoch 45\n",
            " batch Loss train: 0.07687949389219284\n",
            "i 353\n",
            "epoch 45\n",
            " batch Loss train: 0.05929337441921234\n",
            "i 354\n",
            "epoch 45\n",
            " batch Loss train: 0.06373623013496399\n",
            "i 355\n",
            "epoch 45\n",
            " batch Loss train: 0.04644879698753357\n",
            "i 356\n",
            "epoch 45\n",
            " batch Loss train: 0.04885412007570267\n",
            "i 357\n",
            "epoch 45\n",
            " batch Loss train: 0.052112121134996414\n",
            "i 358\n",
            "epoch 45\n",
            " batch Loss train: 0.0621773898601532\n",
            "i 359\n",
            "epoch 45\n",
            " batch Loss train: 0.07363900542259216\n",
            "i 360\n",
            "epoch 45\n",
            " batch Loss train: 0.05769979953765869\n",
            "i 361\n",
            "epoch 45\n",
            " batch Loss train: 0.08435576409101486\n",
            "i 362\n",
            "epoch 45\n",
            " batch Loss train: 0.07405630499124527\n",
            "i 363\n",
            "epoch 45\n",
            " batch Loss train: 0.0718684270977974\n",
            "i 364\n",
            "epoch 45\n",
            " batch Loss train: 0.07172053307294846\n",
            "i 365\n",
            "epoch 45\n",
            " batch Loss train: 0.08458131551742554\n",
            "i 366\n",
            "epoch 45\n",
            " batch Loss train: 0.0685431957244873\n",
            "i 367\n",
            "epoch 45\n",
            " batch Loss train: 0.053553685545921326\n",
            "i 368\n",
            "epoch 45\n",
            " batch Loss train: 0.0526529923081398\n",
            "i 369\n",
            "epoch 45\n",
            " batch Loss train: 0.08776941150426865\n",
            "i 370\n",
            "epoch 45\n",
            " batch Loss train: 0.0586421936750412\n",
            "i 371\n",
            "epoch 45\n",
            " batch Loss train: 0.04754414036870003\n",
            "i 372\n",
            "epoch 45\n",
            " batch Loss train: 0.07696608453989029\n",
            "i 373\n",
            "epoch 45\n",
            " batch Loss train: 0.07347733527421951\n",
            "i 374\n",
            "epoch 45\n",
            " batch Loss train: 0.05574030801653862\n",
            "i 375\n",
            "epoch 45\n",
            " batch Loss train: 0.0740417018532753\n",
            "i 376\n",
            "epoch 45\n",
            " batch Loss train: 0.05724680423736572\n",
            "i 377\n",
            "epoch 45\n",
            " batch Loss train: 0.05796259269118309\n",
            "i 378\n",
            "epoch 45\n",
            " batch Loss train: 0.07299882173538208\n",
            "i 379\n",
            "epoch 45\n",
            " batch Loss train: 0.059048209339380264\n",
            "i 380\n",
            "epoch 45\n",
            " batch Loss train: 0.04812239482998848\n",
            "i 381\n",
            "epoch 45\n",
            " batch Loss train: 0.08180319517850876\n",
            "i 382\n",
            "epoch 45\n",
            " batch Loss train: 0.07336555421352386\n",
            "i 383\n",
            "epoch 45\n",
            " batch Loss train: 0.05491792783141136\n",
            "i 384\n",
            "epoch 45\n",
            " batch Loss train: 0.058991558849811554\n",
            "i 385\n",
            "epoch 45\n",
            " batch Loss train: 0.05463407188653946\n",
            "i 386\n",
            "epoch 45\n",
            " batch Loss train: 0.0770159587264061\n",
            "i 387\n",
            "epoch 45\n",
            " batch Loss train: 0.0949956476688385\n",
            "i 388\n",
            "epoch 45\n",
            " batch Loss train: 0.05472428351640701\n",
            "i 389\n",
            "epoch 45\n",
            " batch Loss train: 0.08990075439214706\n",
            "i 390\n",
            "epoch 45\n",
            " batch Loss train: 0.08113960921764374\n",
            "i 391\n",
            "epoch 45\n",
            " batch Loss train: 0.06189747899770737\n",
            "i 392\n",
            "epoch 45\n",
            " batch Loss train: 0.051389917731285095\n",
            "i 393\n",
            "epoch 45\n",
            " batch Loss train: 0.08381782472133636\n",
            "i 394\n",
            "epoch 45\n",
            " batch Loss train: 0.08243416249752045\n",
            "i 395\n",
            "epoch 45\n",
            " batch Loss train: 0.06951411068439484\n",
            "i 396\n",
            "epoch 45\n",
            " batch Loss train: 0.08654490858316422\n",
            "i 397\n",
            "epoch 45\n",
            " batch Loss train: 0.0642765462398529\n",
            "i 398\n",
            "epoch 45\n",
            " batch Loss train: 0.07323663681745529\n",
            "i 399\n",
            "epoch 45\n",
            " batch Loss train: 0.10118607431650162\n",
            "i 400\n",
            "epoch 45\n",
            " batch Loss train: 0.06394736468791962\n",
            "i 401\n",
            "epoch 45\n",
            " batch Loss train: 0.08538071811199188\n",
            "i 402\n",
            "epoch 45\n",
            " batch Loss train: 0.07033374160528183\n",
            "i 403\n",
            "epoch 45\n",
            " batch Loss train: 0.07403996586799622\n",
            "i 404\n",
            "epoch 45\n",
            " batch Loss train: 0.06896033138036728\n",
            "i 405\n",
            "epoch 45\n",
            " batch Loss train: 0.06458321958780289\n",
            "i 406\n",
            "epoch 45\n",
            " batch Loss train: 0.1008508950471878\n",
            "i 407\n",
            "epoch 45\n",
            " batch Loss train: 0.06389305740594864\n",
            "i 408\n",
            "epoch 45\n",
            " batch Loss train: 0.08754047751426697\n",
            "i 409\n",
            "epoch 45\n",
            " batch Loss train: 0.06575154513120651\n",
            "i 410\n",
            "epoch 45\n",
            " batch Loss train: 0.05350556597113609\n",
            "i 411\n",
            "epoch 45\n",
            " batch Loss train: 0.08723887801170349\n",
            "i 412\n",
            "epoch 45\n",
            " batch Loss train: 0.07191753387451172\n",
            "i 413\n",
            "epoch 45\n",
            " batch Loss train: 0.08841421455144882\n",
            "i 414\n",
            "epoch 45\n",
            " batch Loss train: 0.055626846849918365\n",
            "i 415\n",
            "epoch 45\n",
            " batch Loss train: 0.06974246352910995\n",
            "i 416\n",
            "epoch 45\n",
            " batch Loss train: 0.07221436500549316\n",
            "i 417\n",
            "epoch 45\n",
            " batch Loss train: 0.05998650938272476\n",
            "i 418\n",
            "epoch 45\n",
            " batch Loss train: 0.08805107325315475\n",
            "i 419\n",
            "epoch 45\n",
            " batch Loss train: 0.06850212812423706\n",
            "i 420\n",
            "epoch 45\n",
            " batch Loss train: 0.07805788516998291\n",
            "i 421\n",
            "epoch 45\n",
            " batch Loss train: 0.0555473268032074\n",
            "i 422\n",
            "epoch 45\n",
            " batch Loss train: 0.08314511924982071\n",
            "i 423\n",
            "epoch 45\n",
            " batch Loss train: 0.07831350713968277\n",
            "i 424\n",
            "epoch 45\n",
            " batch Loss train: 0.07759793102741241\n",
            "i 425\n",
            "epoch 45\n",
            " batch Loss train: 0.06110404059290886\n",
            "i 426\n",
            "epoch 45\n",
            " batch Loss train: 0.08065701276063919\n",
            "i 427\n",
            "epoch 45\n",
            " batch Loss train: 0.08206049352884293\n",
            "i 428\n",
            "epoch 45\n",
            " batch Loss train: 0.09544486552476883\n",
            "i 429\n",
            "epoch 45\n",
            " batch Loss train: 0.06867190450429916\n",
            "i 430\n",
            "epoch 45\n",
            " batch Loss train: 0.0660691112279892\n",
            "i 431\n",
            "epoch 45\n",
            " batch Loss train: 0.05733508616685867\n",
            "i 432\n",
            "epoch 45\n",
            " batch Loss train: 0.06438958644866943\n",
            "i 433\n",
            "epoch 45\n",
            " batch Loss train: 0.07391628623008728\n",
            "i 434\n",
            "epoch 45\n",
            " batch Loss train: 0.0627870112657547\n",
            "i 435\n",
            "epoch 45\n",
            " batch Loss train: 0.057028718292713165\n",
            "i 436\n",
            "epoch 45\n",
            " batch Loss train: 0.059232912957668304\n",
            "i 437\n",
            "epoch 45\n",
            " batch Loss train: 0.06069379672408104\n",
            "i 438\n",
            "epoch 45\n",
            " batch Loss train: 0.06431099772453308\n",
            "i 439\n",
            "epoch 45\n",
            " batch Loss train: 0.06362493336200714\n",
            "i 440\n",
            "epoch 45\n",
            " batch Loss train: 0.06035953015089035\n",
            "i 441\n",
            "epoch 45\n",
            " batch Loss train: 0.05826834961771965\n",
            "i 442\n",
            "epoch 45\n",
            " batch Loss train: 0.060101427137851715\n",
            "i 443\n",
            "epoch 45\n",
            " batch Loss train: 0.07293723523616791\n",
            "i 444\n",
            "epoch 45\n",
            " batch Loss train: 0.07240775227546692\n",
            "i 445\n",
            "epoch 45\n",
            " batch Loss train: 0.06302662193775177\n",
            "total epoch Loss train: tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 46\n",
            " batch Loss train: 0.04746992886066437\n",
            "i 1\n",
            "epoch 46\n",
            " batch Loss train: 0.05740349739789963\n",
            "i 2\n",
            "epoch 46\n",
            " batch Loss train: 0.07378863543272018\n",
            "i 3\n",
            "epoch 46\n",
            " batch Loss train: 0.06140593811869621\n",
            "i 4\n",
            "epoch 46\n",
            " batch Loss train: 0.04970305785536766\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 46\n",
            " batch Loss train: 0.05599714815616608\n",
            "i 6\n",
            "epoch 46\n",
            " batch Loss train: 0.05351872742176056\n",
            "i 7\n",
            "epoch 46\n",
            " batch Loss train: 0.048403315246105194\n",
            "i 8\n",
            "epoch 46\n",
            " batch Loss train: 0.06158355250954628\n",
            "i 9\n",
            "epoch 46\n",
            " batch Loss train: 0.06917154043912888\n",
            "i 10\n",
            "epoch 46\n",
            " batch Loss train: 0.051571544259786606\n",
            "i 11\n",
            "epoch 46\n",
            " batch Loss train: 0.06420835852622986\n",
            "i 12\n",
            "epoch 46\n",
            " batch Loss train: 0.04007987305521965\n",
            "i 13\n",
            "epoch 46\n",
            " batch Loss train: 0.05462655797600746\n",
            "i 14\n",
            "epoch 46\n",
            " batch Loss train: 0.05147281289100647\n",
            "i 15\n",
            "epoch 46\n",
            " batch Loss train: 0.03787592798471451\n",
            "i 16\n",
            "epoch 46\n",
            " batch Loss train: 0.05731013044714928\n",
            "i 17\n",
            "epoch 46\n",
            " batch Loss train: 0.041033998131752014\n",
            "i 18\n",
            "epoch 46\n",
            " batch Loss train: 0.061972588300704956\n",
            "i 19\n",
            "epoch 46\n",
            " batch Loss train: 0.052843425422906876\n",
            "i 20\n",
            "epoch 46\n",
            " batch Loss train: 0.06844905763864517\n",
            "i 21\n",
            "epoch 46\n",
            " batch Loss train: 0.05536624416708946\n",
            "i 22\n",
            "epoch 46\n",
            " batch Loss train: 0.05841986462473869\n",
            "i 23\n",
            "epoch 46\n",
            " batch Loss train: 0.04415241256356239\n",
            "i 24\n",
            "epoch 46\n",
            " batch Loss train: 0.04748031869530678\n",
            "i 25\n",
            "epoch 46\n",
            " batch Loss train: 0.03854123875498772\n",
            "i 26\n",
            "epoch 46\n",
            " batch Loss train: 0.060746047645807266\n",
            "i 27\n",
            "epoch 46\n",
            " batch Loss train: 0.046119045466184616\n",
            "i 28\n",
            "epoch 46\n",
            " batch Loss train: 0.03703828901052475\n",
            "i 29\n",
            "epoch 46\n",
            " batch Loss train: 0.06418821960687637\n",
            "i 30\n",
            "epoch 46\n",
            " batch Loss train: 0.054599739611148834\n",
            "i 31\n",
            "epoch 46\n",
            " batch Loss train: 0.0391346774995327\n",
            "i 32\n",
            "epoch 46\n",
            " batch Loss train: 0.046883806586265564\n",
            "i 33\n",
            "epoch 46\n",
            " batch Loss train: 0.05827406421303749\n",
            "i 34\n",
            "epoch 46\n",
            " batch Loss train: 0.03533376753330231\n",
            "i 35\n",
            "epoch 46\n",
            " batch Loss train: 0.053185347467660904\n",
            "i 36\n",
            "epoch 46\n",
            " batch Loss train: 0.06911763548851013\n",
            "i 37\n",
            "epoch 46\n",
            " batch Loss train: 0.04138365760445595\n",
            "i 38\n",
            "epoch 46\n",
            " batch Loss train: 0.06321130692958832\n",
            "i 39\n",
            "epoch 46\n",
            " batch Loss train: 0.06118055805563927\n",
            "i 40\n",
            "epoch 46\n",
            " batch Loss train: 0.04531441256403923\n",
            "i 41\n",
            "epoch 46\n",
            " batch Loss train: 0.06023348867893219\n",
            "i 42\n",
            "epoch 46\n",
            " batch Loss train: 0.06734389811754227\n",
            "i 43\n",
            "epoch 46\n",
            " batch Loss train: 0.05755119398236275\n",
            "i 44\n",
            "epoch 46\n",
            " batch Loss train: 0.06365367025136948\n",
            "i 45\n",
            "epoch 46\n",
            " batch Loss train: 0.07403557002544403\n",
            "i 46\n",
            "epoch 46\n",
            " batch Loss train: 0.040568649768829346\n",
            "i 47\n",
            "epoch 46\n",
            " batch Loss train: 0.039872996509075165\n",
            "i 48\n",
            "epoch 46\n",
            " batch Loss train: 0.05456969514489174\n",
            "i 49\n",
            "epoch 46\n",
            " batch Loss train: 0.06201332062482834\n",
            "i 50\n",
            "epoch 46\n",
            " batch Loss train: 0.056547001004219055\n",
            "i 51\n",
            "epoch 46\n",
            " batch Loss train: 0.06416255980730057\n",
            "i 52\n",
            "epoch 46\n",
            " batch Loss train: 0.03903614729642868\n",
            "i 53\n",
            "epoch 46\n",
            " batch Loss train: 0.04894624277949333\n",
            "i 54\n",
            "epoch 46\n",
            " batch Loss train: 0.046221114695072174\n",
            "i 55\n",
            "epoch 46\n",
            " batch Loss train: 0.05285739526152611\n",
            "i 56\n",
            "epoch 46\n",
            " batch Loss train: 0.04761940985918045\n",
            "i 57\n",
            "epoch 46\n",
            " batch Loss train: 0.05560595169663429\n",
            "i 58\n",
            "epoch 46\n",
            " batch Loss train: 0.06191420182585716\n",
            "i 59\n",
            "epoch 46\n",
            " batch Loss train: 0.05091458186507225\n",
            "i 60\n",
            "epoch 46\n",
            " batch Loss train: 0.05822921544313431\n",
            "i 61\n",
            "epoch 46\n",
            " batch Loss train: 0.0447184294462204\n",
            "i 62\n",
            "epoch 46\n",
            " batch Loss train: 0.05362093821167946\n",
            "i 63\n",
            "epoch 46\n",
            " batch Loss train: 0.0609622448682785\n",
            "i 64\n",
            "epoch 46\n",
            " batch Loss train: 0.04593706503510475\n",
            "i 65\n",
            "epoch 46\n",
            " batch Loss train: 0.06835219264030457\n",
            "i 66\n",
            "epoch 46\n",
            " batch Loss train: 0.05538449063897133\n",
            "i 67\n",
            "epoch 46\n",
            " batch Loss train: 0.04661478474736214\n",
            "i 68\n",
            "epoch 46\n",
            " batch Loss train: 0.04840051010251045\n",
            "i 69\n",
            "epoch 46\n",
            " batch Loss train: 0.05596834048628807\n",
            "i 70\n",
            "epoch 46\n",
            " batch Loss train: 0.048186544328927994\n",
            "i 71\n",
            "epoch 46\n",
            " batch Loss train: 0.05699925869703293\n",
            "i 72\n",
            "epoch 46\n",
            " batch Loss train: 0.058134641498327255\n",
            "i 73\n",
            "epoch 46\n",
            " batch Loss train: 0.06303007900714874\n",
            "i 74\n",
            "epoch 46\n",
            " batch Loss train: 0.06372126191854477\n",
            "i 75\n",
            "epoch 46\n",
            " batch Loss train: 0.06307744979858398\n",
            "i 76\n",
            "epoch 46\n",
            " batch Loss train: 0.04819287732243538\n",
            "i 77\n",
            "epoch 46\n",
            " batch Loss train: 0.09071189165115356\n",
            "i 78\n",
            "epoch 46\n",
            " batch Loss train: 0.06023438274860382\n",
            "i 79\n",
            "epoch 46\n",
            " batch Loss train: 0.068799689412117\n",
            "i 80\n",
            "epoch 46\n",
            " batch Loss train: 0.06665032356977463\n",
            "i 81\n",
            "epoch 46\n",
            " batch Loss train: 0.06799230724573135\n",
            "i 82\n",
            "epoch 46\n",
            " batch Loss train: 0.06795115023851395\n",
            "i 83\n",
            "epoch 46\n",
            " batch Loss train: 0.05081167444586754\n",
            "i 84\n",
            "epoch 46\n",
            " batch Loss train: 0.06461673229932785\n",
            "i 85\n",
            "epoch 46\n",
            " batch Loss train: 0.05584753304719925\n",
            "i 86\n",
            "epoch 46\n",
            " batch Loss train: 0.0425671748816967\n",
            "i 87\n",
            "epoch 46\n",
            " batch Loss train: 0.041546765714883804\n",
            "i 88\n",
            "epoch 46\n",
            " batch Loss train: 0.07497701048851013\n",
            "i 89\n",
            "epoch 46\n",
            " batch Loss train: 0.04760143160820007\n",
            "i 90\n",
            "epoch 46\n",
            " batch Loss train: 0.0652865320444107\n",
            "i 91\n",
            "epoch 46\n",
            " batch Loss train: 0.05071108043193817\n",
            "i 92\n",
            "epoch 46\n",
            " batch Loss train: 0.06626071035861969\n",
            "i 93\n",
            "epoch 46\n",
            " batch Loss train: 0.06575491279363632\n",
            "i 94\n",
            "epoch 46\n",
            " batch Loss train: 0.06237560510635376\n",
            "i 95\n",
            "epoch 46\n",
            " batch Loss train: 0.052310824394226074\n",
            "i 96\n",
            "epoch 46\n",
            " batch Loss train: 0.0725417509675026\n",
            "i 97\n",
            "epoch 46\n",
            " batch Loss train: 0.05008580908179283\n",
            "i 98\n",
            "epoch 46\n",
            " batch Loss train: 0.058258574455976486\n",
            "i 99\n",
            "epoch 46\n",
            " batch Loss train: 0.0917569026350975\n",
            "i 100\n",
            "epoch 46\n",
            " batch Loss train: 0.051909465342760086\n",
            "i 101\n",
            "epoch 46\n",
            " batch Loss train: 0.0450957715511322\n",
            "i 102\n",
            "epoch 46\n",
            " batch Loss train: 0.060158032923936844\n",
            "i 103\n",
            "epoch 46\n",
            " batch Loss train: 0.06306988000869751\n",
            "i 104\n",
            "epoch 46\n",
            " batch Loss train: 0.06479865312576294\n",
            "i 105\n",
            "epoch 46\n",
            " batch Loss train: 0.0602693185210228\n",
            "i 106\n",
            "epoch 46\n",
            " batch Loss train: 0.06272154301404953\n",
            "i 107\n",
            "epoch 46\n",
            " batch Loss train: 0.06605606526136398\n",
            "i 108\n",
            "epoch 46\n",
            " batch Loss train: 0.06354634463787079\n",
            "i 109\n",
            "epoch 46\n",
            " batch Loss train: 0.056694451719522476\n",
            "i 110\n",
            "epoch 46\n",
            " batch Loss train: 0.07153284549713135\n",
            "i 111\n",
            "epoch 46\n",
            " batch Loss train: 0.05066315457224846\n",
            "i 112\n",
            "epoch 46\n",
            " batch Loss train: 0.05596545338630676\n",
            "i 113\n",
            "epoch 46\n",
            " batch Loss train: 0.08825082331895828\n",
            "i 114\n",
            "epoch 46\n",
            " batch Loss train: 0.050736308097839355\n",
            "i 115\n",
            "epoch 46\n",
            " batch Loss train: 0.04970145970582962\n",
            "i 116\n",
            "epoch 46\n",
            " batch Loss train: 0.0600985586643219\n",
            "i 117\n",
            "epoch 46\n",
            " batch Loss train: 0.045528486371040344\n",
            "i 118\n",
            "epoch 46\n",
            " batch Loss train: 0.05905785784125328\n",
            "i 119\n",
            "epoch 46\n",
            " batch Loss train: 0.04774174839258194\n",
            "i 120\n",
            "epoch 46\n",
            " batch Loss train: 0.056426677852869034\n",
            "i 121\n",
            "epoch 46\n",
            " batch Loss train: 0.06603718549013138\n",
            "i 122\n",
            "epoch 46\n",
            " batch Loss train: 0.04960157722234726\n",
            "i 123\n",
            "epoch 46\n",
            " batch Loss train: 0.054683007299900055\n",
            "i 124\n",
            "epoch 46\n",
            " batch Loss train: 0.06573355197906494\n",
            "i 125\n",
            "epoch 46\n",
            " batch Loss train: 0.0619460828602314\n",
            "i 126\n",
            "epoch 46\n",
            " batch Loss train: 0.06589674204587936\n",
            "i 127\n",
            "epoch 46\n",
            " batch Loss train: 0.05900095775723457\n",
            "i 128\n",
            "epoch 46\n",
            " batch Loss train: 0.05927900969982147\n",
            "i 129\n",
            "epoch 46\n",
            " batch Loss train: 0.05393950641155243\n",
            "i 130\n",
            "epoch 46\n",
            " batch Loss train: 0.06022701412439346\n",
            "i 131\n",
            "epoch 46\n",
            " batch Loss train: 0.06228739023208618\n",
            "i 132\n",
            "epoch 46\n",
            " batch Loss train: 0.03976334631443024\n",
            "i 133\n",
            "epoch 46\n",
            " batch Loss train: 0.07003223150968552\n",
            "i 134\n",
            "epoch 46\n",
            " batch Loss train: 0.04445761442184448\n",
            "i 135\n",
            "epoch 46\n",
            " batch Loss train: 0.05872883275151253\n",
            "i 136\n",
            "epoch 46\n",
            " batch Loss train: 0.0452100932598114\n",
            "i 137\n",
            "epoch 46\n",
            " batch Loss train: 0.06814442574977875\n",
            "i 138\n",
            "epoch 46\n",
            " batch Loss train: 0.0759953036904335\n",
            "i 139\n",
            "epoch 46\n",
            " batch Loss train: 0.05675460770726204\n",
            "i 140\n",
            "epoch 46\n",
            " batch Loss train: 0.10412721335887909\n",
            "i 141\n",
            "epoch 46\n",
            " batch Loss train: 0.047070398926734924\n",
            "i 142\n",
            "epoch 46\n",
            " batch Loss train: 0.052735500037670135\n",
            "i 143\n",
            "epoch 46\n",
            " batch Loss train: 0.04754442349076271\n",
            "i 144\n",
            "epoch 46\n",
            " batch Loss train: 0.05808008089661598\n",
            "i 145\n",
            "epoch 46\n",
            " batch Loss train: 0.05971319228410721\n",
            "i 146\n",
            "epoch 46\n",
            " batch Loss train: 0.07510306686162949\n",
            "i 147\n",
            "epoch 46\n",
            " batch Loss train: 0.06257946789264679\n",
            "i 148\n",
            "epoch 46\n",
            " batch Loss train: 0.09276467561721802\n",
            "i 149\n",
            "epoch 46\n",
            " batch Loss train: 0.05376205965876579\n",
            "i 150\n",
            "epoch 46\n",
            " batch Loss train: 0.04927744343876839\n",
            "i 151\n",
            "epoch 46\n",
            " batch Loss train: 0.06576701253652573\n",
            "i 152\n",
            "epoch 46\n",
            " batch Loss train: 0.04981386289000511\n",
            "i 153\n",
            "epoch 46\n",
            " batch Loss train: 0.0621962808072567\n",
            "i 154\n",
            "epoch 46\n",
            " batch Loss train: 0.060401540249586105\n",
            "i 155\n",
            "epoch 46\n",
            " batch Loss train: 0.0790962278842926\n",
            "i 156\n",
            "epoch 46\n",
            " batch Loss train: 0.0702342838048935\n",
            "i 157\n",
            "epoch 46\n",
            " batch Loss train: 0.05064299330115318\n",
            "i 158\n",
            "epoch 46\n",
            " batch Loss train: 0.04432394728064537\n",
            "i 159\n",
            "epoch 46\n",
            " batch Loss train: 0.061988748610019684\n",
            "i 160\n",
            "epoch 46\n",
            " batch Loss train: 0.04862756282091141\n",
            "i 161\n",
            "epoch 46\n",
            " batch Loss train: 0.06887669861316681\n",
            "i 162\n",
            "epoch 46\n",
            " batch Loss train: 0.06441375613212585\n",
            "i 163\n",
            "epoch 46\n",
            " batch Loss train: 0.05661102011799812\n",
            "i 164\n",
            "epoch 46\n",
            " batch Loss train: 0.056261759251356125\n",
            "i 165\n",
            "epoch 46\n",
            " batch Loss train: 0.06583656370639801\n",
            "i 166\n",
            "epoch 46\n",
            " batch Loss train: 0.050757996737957\n",
            "i 167\n",
            "epoch 46\n",
            " batch Loss train: 0.06489043682813644\n",
            "i 168\n",
            "epoch 46\n",
            " batch Loss train: 0.05321546643972397\n",
            "i 169\n",
            "epoch 46\n",
            " batch Loss train: 0.05441071838140488\n",
            "i 170\n",
            "epoch 46\n",
            " batch Loss train: 0.05012664571404457\n",
            "i 171\n",
            "epoch 46\n",
            " batch Loss train: 0.06154429912567139\n",
            "i 172\n",
            "epoch 46\n",
            " batch Loss train: 0.062152519822120667\n",
            "i 173\n",
            "epoch 46\n",
            " batch Loss train: 0.0616445429623127\n",
            "i 174\n",
            "epoch 46\n",
            " batch Loss train: 0.049013834446668625\n",
            "i 175\n",
            "epoch 46\n",
            " batch Loss train: 0.05934475362300873\n",
            "i 176\n",
            "epoch 46\n",
            " batch Loss train: 0.05204714089632034\n",
            "i 177\n",
            "epoch 46\n",
            " batch Loss train: 0.06449244171380997\n",
            "i 178\n",
            "epoch 46\n",
            " batch Loss train: 0.05262010544538498\n",
            "i 179\n",
            "epoch 46\n",
            " batch Loss train: 0.059172626584768295\n",
            "i 180\n",
            "epoch 46\n",
            " batch Loss train: 0.05594351887702942\n",
            "i 181\n",
            "epoch 46\n",
            " batch Loss train: 0.08023436367511749\n",
            "i 182\n",
            "epoch 46\n",
            " batch Loss train: 0.04199995845556259\n",
            "i 183\n",
            "epoch 46\n",
            " batch Loss train: 0.04189879819750786\n",
            "i 184\n",
            "epoch 46\n",
            " batch Loss train: 0.05588661506772041\n",
            "i 185\n",
            "epoch 46\n",
            " batch Loss train: 0.045134954154491425\n",
            "i 186\n",
            "epoch 46\n",
            " batch Loss train: 0.05710160359740257\n",
            "i 187\n",
            "epoch 46\n",
            " batch Loss train: 0.0686299130320549\n",
            "i 188\n",
            "epoch 46\n",
            " batch Loss train: 0.053544145077466965\n",
            "i 189\n",
            "epoch 46\n",
            " batch Loss train: 0.06281520426273346\n",
            "i 190\n",
            "epoch 46\n",
            " batch Loss train: 0.05727649852633476\n",
            "i 191\n",
            "epoch 46\n",
            " batch Loss train: 0.07671090960502625\n",
            "i 192\n",
            "epoch 46\n",
            " batch Loss train: 0.04876384511590004\n",
            "i 193\n",
            "epoch 46\n",
            " batch Loss train: 0.05120232701301575\n",
            "i 194\n",
            "epoch 46\n",
            " batch Loss train: 0.04368838667869568\n",
            "i 195\n",
            "epoch 46\n",
            " batch Loss train: 0.05684671923518181\n",
            "i 196\n",
            "epoch 46\n",
            " batch Loss train: 0.05339130014181137\n",
            "i 197\n",
            "epoch 46\n",
            " batch Loss train: 0.054228030145168304\n",
            "i 198\n",
            "epoch 46\n",
            " batch Loss train: 0.07043043524026871\n",
            "i 199\n",
            "epoch 46\n",
            " batch Loss train: 0.06775809824466705\n",
            "i 200\n",
            "epoch 46\n",
            " batch Loss train: 0.048916783183813095\n",
            "i 201\n",
            "epoch 46\n",
            " batch Loss train: 0.06350502371788025\n",
            "i 202\n",
            "epoch 46\n",
            " batch Loss train: 0.06826725602149963\n",
            "i 203\n",
            "epoch 46\n",
            " batch Loss train: 0.04753415286540985\n",
            "i 204\n",
            "epoch 46\n",
            " batch Loss train: 0.05052891746163368\n",
            "i 205\n",
            "epoch 46\n",
            " batch Loss train: 0.07736828178167343\n",
            "i 206\n",
            "epoch 46\n",
            " batch Loss train: 0.059150952845811844\n",
            "i 207\n",
            "epoch 46\n",
            " batch Loss train: 0.06094277650117874\n",
            "i 208\n",
            "epoch 46\n",
            " batch Loss train: 0.06934403628110886\n",
            "i 209\n",
            "epoch 46\n",
            " batch Loss train: 0.05767163261771202\n",
            "i 210\n",
            "epoch 46\n",
            " batch Loss train: 0.051497284322977066\n",
            "i 211\n",
            "epoch 46\n",
            " batch Loss train: 0.05462528392672539\n",
            "i 212\n",
            "epoch 46\n",
            " batch Loss train: 0.051679160445928574\n",
            "i 213\n",
            "epoch 46\n",
            " batch Loss train: 0.05218212679028511\n",
            "i 214\n",
            "epoch 46\n",
            " batch Loss train: 0.07566291838884354\n",
            "i 215\n",
            "epoch 46\n",
            " batch Loss train: 0.07271374762058258\n",
            "i 216\n",
            "epoch 46\n",
            " batch Loss train: 0.06708147376775742\n",
            "i 217\n",
            "epoch 46\n",
            " batch Loss train: 0.06122221052646637\n",
            "i 218\n",
            "epoch 46\n",
            " batch Loss train: 0.04817727208137512\n",
            "i 219\n",
            "epoch 46\n",
            " batch Loss train: 0.0455172024667263\n",
            "i 220\n",
            "epoch 46\n",
            " batch Loss train: 0.07393236458301544\n",
            "i 221\n",
            "epoch 46\n",
            " batch Loss train: 0.059492092579603195\n",
            "i 222\n",
            "epoch 46\n",
            " batch Loss train: 0.054499972611665726\n",
            "i 223\n",
            "epoch 46\n",
            " batch Loss train: 0.06083402410149574\n",
            "i 224\n",
            "epoch 46\n",
            " batch Loss train: 0.056184690445661545\n",
            "i 225\n",
            "epoch 46\n",
            " batch Loss train: 0.05861598625779152\n",
            "i 226\n",
            "epoch 46\n",
            " batch Loss train: 0.05359911546111107\n",
            "i 227\n",
            "epoch 46\n",
            " batch Loss train: 0.08692211657762527\n",
            "i 228\n",
            "epoch 46\n",
            " batch Loss train: 0.07804235816001892\n",
            "i 229\n",
            "epoch 46\n",
            " batch Loss train: 0.04228542000055313\n",
            "i 230\n",
            "epoch 46\n",
            " batch Loss train: 0.07516859471797943\n",
            "i 231\n",
            "epoch 46\n",
            " batch Loss train: 0.06273676455020905\n",
            "i 232\n",
            "epoch 46\n",
            " batch Loss train: 0.0584886334836483\n",
            "i 233\n",
            "epoch 46\n",
            " batch Loss train: 0.05437248945236206\n",
            "i 234\n",
            "epoch 46\n",
            " batch Loss train: 0.05522860214114189\n",
            "i 235\n",
            "epoch 46\n",
            " batch Loss train: 0.057346001267433167\n",
            "i 236\n",
            "epoch 46\n",
            " batch Loss train: 0.06133130565285683\n",
            "i 237\n",
            "epoch 46\n",
            " batch Loss train: 0.054834961891174316\n",
            "i 238\n",
            "epoch 46\n",
            " batch Loss train: 0.06622493267059326\n",
            "i 239\n",
            "epoch 46\n",
            " batch Loss train: 0.08285882323980331\n",
            "i 240\n",
            "epoch 46\n",
            " batch Loss train: 0.059204161167144775\n",
            "i 241\n",
            "epoch 46\n",
            " batch Loss train: 0.06137039512395859\n",
            "i 242\n",
            "epoch 46\n",
            " batch Loss train: 0.05212891846895218\n",
            "i 243\n",
            "epoch 46\n",
            " batch Loss train: 0.05993305519223213\n",
            "i 244\n",
            "epoch 46\n",
            " batch Loss train: 0.059801388531923294\n",
            "i 245\n",
            "epoch 46\n",
            " batch Loss train: 0.05021874979138374\n",
            "i 246\n",
            "epoch 46\n",
            " batch Loss train: 0.057207245379686356\n",
            "i 247\n",
            "epoch 46\n",
            " batch Loss train: 0.06098466366529465\n",
            "i 248\n",
            "epoch 46\n",
            " batch Loss train: 0.07147752493619919\n",
            "i 249\n",
            "epoch 46\n",
            " batch Loss train: 0.053666725754737854\n",
            "i 250\n",
            "epoch 46\n",
            " batch Loss train: 0.051085345447063446\n",
            "i 251\n",
            "epoch 46\n",
            " batch Loss train: 0.06639901548624039\n",
            "i 252\n",
            "epoch 46\n",
            " batch Loss train: 0.06354963034391403\n",
            "i 253\n",
            "epoch 46\n",
            " batch Loss train: 0.04039057344198227\n",
            "i 254\n",
            "epoch 46\n",
            " batch Loss train: 0.0625208243727684\n",
            "i 255\n",
            "epoch 46\n",
            " batch Loss train: 0.053642112761735916\n",
            "i 256\n",
            "epoch 46\n",
            " batch Loss train: 0.07695768028497696\n",
            "i 257\n",
            "epoch 46\n",
            " batch Loss train: 0.06262772530317307\n",
            "i 258\n",
            "epoch 46\n",
            " batch Loss train: 0.06397876143455505\n",
            "i 259\n",
            "epoch 46\n",
            " batch Loss train: 0.06984277814626694\n",
            "i 260\n",
            "epoch 46\n",
            " batch Loss train: 0.049737922847270966\n",
            "i 261\n",
            "epoch 46\n",
            " batch Loss train: 0.06716180592775345\n",
            "i 262\n",
            "epoch 46\n",
            " batch Loss train: 0.067937932908535\n",
            "i 263\n",
            "epoch 46\n",
            " batch Loss train: 0.061100829392671585\n",
            "i 264\n",
            "epoch 46\n",
            " batch Loss train: 0.039028845727443695\n",
            "i 265\n",
            "epoch 46\n",
            " batch Loss train: 0.08026944100856781\n",
            "i 266\n",
            "epoch 46\n",
            " batch Loss train: 0.06595651060342789\n",
            "i 267\n",
            "epoch 46\n",
            " batch Loss train: 0.045346248894929886\n",
            "i 268\n",
            "epoch 46\n",
            " batch Loss train: 0.05851622298359871\n",
            "i 269\n",
            "epoch 46\n",
            " batch Loss train: 0.05116485804319382\n",
            "i 270\n",
            "epoch 46\n",
            " batch Loss train: 0.07103614509105682\n",
            "i 271\n",
            "epoch 46\n",
            " batch Loss train: 0.049143701791763306\n",
            "i 272\n",
            "epoch 46\n",
            " batch Loss train: 0.057987939566373825\n",
            "i 273\n",
            "epoch 46\n",
            " batch Loss train: 0.05207910016179085\n",
            "i 274\n",
            "epoch 46\n",
            " batch Loss train: 0.061180759221315384\n",
            "i 275\n",
            "epoch 46\n",
            " batch Loss train: 0.05656445771455765\n",
            "i 276\n",
            "epoch 46\n",
            " batch Loss train: 0.053133610635995865\n",
            "i 277\n",
            "epoch 46\n",
            " batch Loss train: 0.06427653133869171\n",
            "i 278\n",
            "epoch 46\n",
            " batch Loss train: 0.06826221197843552\n",
            "i 279\n",
            "epoch 46\n",
            " batch Loss train: 0.052375245839357376\n",
            "i 280\n",
            "epoch 46\n",
            " batch Loss train: 0.06896330416202545\n",
            "i 281\n",
            "epoch 46\n",
            " batch Loss train: 0.05220039188861847\n",
            "i 282\n",
            "epoch 46\n",
            " batch Loss train: 0.05152842774987221\n",
            "i 283\n",
            "epoch 46\n",
            " batch Loss train: 0.06652446836233139\n",
            "i 284\n",
            "epoch 46\n",
            " batch Loss train: 0.06593309342861176\n",
            "i 285\n",
            "epoch 46\n",
            " batch Loss train: 0.05454958230257034\n",
            "i 286\n",
            "epoch 46\n",
            " batch Loss train: 0.038820140063762665\n",
            "i 287\n",
            "epoch 46\n",
            " batch Loss train: 0.06304651498794556\n",
            "i 288\n",
            "epoch 46\n",
            " batch Loss train: 0.05895600840449333\n",
            "i 289\n",
            "epoch 46\n",
            " batch Loss train: 0.05369073152542114\n",
            "i 290\n",
            "epoch 46\n",
            " batch Loss train: 0.05938148498535156\n",
            "i 291\n",
            "epoch 46\n",
            " batch Loss train: 0.05686279386281967\n",
            "i 292\n",
            "epoch 46\n",
            " batch Loss train: 0.05939492583274841\n",
            "i 293\n",
            "epoch 46\n",
            " batch Loss train: 0.0562470406293869\n",
            "i 294\n",
            "epoch 46\n",
            " batch Loss train: 0.07691802829504013\n",
            "i 295\n",
            "epoch 46\n",
            " batch Loss train: 0.04699081555008888\n",
            "i 296\n",
            "epoch 46\n",
            " batch Loss train: 0.04216673970222473\n",
            "i 297\n",
            "epoch 46\n",
            " batch Loss train: 0.0517168752849102\n",
            "i 298\n",
            "epoch 46\n",
            " batch Loss train: 0.06383275240659714\n",
            "i 299\n",
            "epoch 46\n",
            " batch Loss train: 0.07443177700042725\n",
            "i 300\n",
            "epoch 46\n",
            " batch Loss train: 0.04821499064564705\n",
            "i 301\n",
            "epoch 46\n",
            " batch Loss train: 0.04657964035868645\n",
            "i 302\n",
            "epoch 46\n",
            " batch Loss train: 0.060697659850120544\n",
            "i 303\n",
            "epoch 46\n",
            " batch Loss train: 0.06702455878257751\n",
            "i 304\n",
            "epoch 46\n",
            " batch Loss train: 0.049923546612262726\n",
            "i 305\n",
            "epoch 46\n",
            " batch Loss train: 0.09390545636415482\n",
            "i 306\n",
            "epoch 46\n",
            " batch Loss train: 0.041578810662031174\n",
            "i 307\n",
            "epoch 46\n",
            " batch Loss train: 0.06140541285276413\n",
            "i 308\n",
            "epoch 46\n",
            " batch Loss train: 0.044450171291828156\n",
            "i 309\n",
            "epoch 46\n",
            " batch Loss train: 0.04904783517122269\n",
            "i 310\n",
            "epoch 46\n",
            " batch Loss train: 0.059430040419101715\n",
            "i 311\n",
            "epoch 46\n",
            " batch Loss train: 0.05833787843585014\n",
            "i 312\n",
            "epoch 46\n",
            " batch Loss train: 0.055283159017562866\n",
            "i 313\n",
            "epoch 46\n",
            " batch Loss train: 0.03713154420256615\n",
            "i 314\n",
            "epoch 46\n",
            " batch Loss train: 0.05456943437457085\n",
            "i 315\n",
            "epoch 46\n",
            " batch Loss train: 0.05617612972855568\n",
            "i 316\n",
            "epoch 46\n",
            " batch Loss train: 0.07789358496665955\n",
            "i 317\n",
            "epoch 46\n",
            " batch Loss train: 0.06352587789297104\n",
            "i 318\n",
            "epoch 46\n",
            " batch Loss train: 0.06289548426866531\n",
            "i 319\n",
            "epoch 46\n",
            " batch Loss train: 0.08311536908149719\n",
            "i 320\n",
            "epoch 46\n",
            " batch Loss train: 0.06272220611572266\n",
            "i 321\n",
            "epoch 46\n",
            " batch Loss train: 0.0628056526184082\n",
            "i 322\n",
            "epoch 46\n",
            " batch Loss train: 0.06466414779424667\n",
            "i 323\n",
            "epoch 46\n",
            " batch Loss train: 0.07469763606786728\n",
            "i 324\n",
            "epoch 46\n",
            " batch Loss train: 0.046205032616853714\n",
            "i 325\n",
            "epoch 46\n",
            " batch Loss train: 0.061031341552734375\n",
            "i 326\n",
            "epoch 46\n",
            " batch Loss train: 0.05162515118718147\n",
            "i 327\n",
            "epoch 46\n",
            " batch Loss train: 0.05871334671974182\n",
            "i 328\n",
            "epoch 46\n",
            " batch Loss train: 0.056546200066804886\n",
            "i 329\n",
            "epoch 46\n",
            " batch Loss train: 0.059717100113630295\n",
            "i 330\n",
            "epoch 46\n",
            " batch Loss train: 0.058020565658807755\n",
            "i 331\n",
            "epoch 46\n",
            " batch Loss train: 0.05910016596317291\n",
            "i 332\n",
            "epoch 46\n",
            " batch Loss train: 0.08120694756507874\n",
            "i 333\n",
            "epoch 46\n",
            " batch Loss train: 0.0793253481388092\n",
            "i 334\n",
            "epoch 46\n",
            " batch Loss train: 0.07328534871339798\n",
            "i 335\n",
            "epoch 46\n",
            " batch Loss train: 0.07616943120956421\n",
            "i 336\n",
            "epoch 46\n",
            " batch Loss train: 0.06355351954698563\n",
            "i 337\n",
            "epoch 46\n",
            " batch Loss train: 0.05078952759504318\n",
            "i 338\n",
            "epoch 46\n",
            " batch Loss train: 0.06540294736623764\n",
            "i 339\n",
            "epoch 46\n",
            " batch Loss train: 0.05743337422609329\n",
            "i 340\n",
            "epoch 46\n",
            " batch Loss train: 0.06100696697831154\n",
            "i 341\n",
            "epoch 46\n",
            " batch Loss train: 0.05926971882581711\n",
            "i 342\n",
            "epoch 46\n",
            " batch Loss train: 0.04071709141135216\n",
            "i 343\n",
            "epoch 46\n",
            " batch Loss train: 0.04040282219648361\n",
            "i 344\n",
            "epoch 46\n",
            " batch Loss train: 0.04185153543949127\n",
            "i 345\n",
            "epoch 46\n",
            " batch Loss train: 0.054587945342063904\n",
            "i 346\n",
            "epoch 46\n",
            " batch Loss train: 0.07178104668855667\n",
            "i 347\n",
            "epoch 46\n",
            " batch Loss train: 0.04975061118602753\n",
            "i 348\n",
            "epoch 46\n",
            " batch Loss train: 0.04242854565382004\n",
            "i 349\n",
            "epoch 46\n",
            " batch Loss train: 0.08619533479213715\n",
            "i 350\n",
            "epoch 46\n",
            " batch Loss train: 0.08313244581222534\n",
            "i 351\n",
            "epoch 46\n",
            " batch Loss train: 0.0735480785369873\n",
            "i 352\n",
            "epoch 46\n",
            " batch Loss train: 0.07179716229438782\n",
            "i 353\n",
            "epoch 46\n",
            " batch Loss train: 0.0561031699180603\n",
            "i 354\n",
            "epoch 46\n",
            " batch Loss train: 0.07254400849342346\n",
            "i 355\n",
            "epoch 46\n",
            " batch Loss train: 0.05255092307925224\n",
            "i 356\n",
            "epoch 46\n",
            " batch Loss train: 0.05344448611140251\n",
            "i 357\n",
            "epoch 46\n",
            " batch Loss train: 0.06809671968221664\n",
            "i 358\n",
            "epoch 46\n",
            " batch Loss train: 0.0796738862991333\n",
            "i 359\n",
            "epoch 46\n",
            " batch Loss train: 0.06464170664548874\n",
            "i 360\n",
            "epoch 46\n",
            " batch Loss train: 0.07344372570514679\n",
            "i 361\n",
            "epoch 46\n",
            " batch Loss train: 0.050499241799116135\n",
            "i 362\n",
            "epoch 46\n",
            " batch Loss train: 0.04426800087094307\n",
            "i 363\n",
            "epoch 46\n",
            " batch Loss train: 0.08633647114038467\n",
            "i 364\n",
            "epoch 46\n",
            " batch Loss train: 0.06880724430084229\n",
            "i 365\n",
            "epoch 46\n",
            " batch Loss train: 0.06604405492544174\n",
            "i 366\n",
            "epoch 46\n",
            " batch Loss train: 0.05209028720855713\n",
            "i 367\n",
            "epoch 46\n",
            " batch Loss train: 0.052226319909095764\n",
            "i 368\n",
            "epoch 46\n",
            " batch Loss train: 0.06002171337604523\n",
            "i 369\n",
            "epoch 46\n",
            " batch Loss train: 0.06658180058002472\n",
            "i 370\n",
            "epoch 46\n",
            " batch Loss train: 0.05598394572734833\n",
            "i 371\n",
            "epoch 46\n",
            " batch Loss train: 0.0495876781642437\n",
            "i 372\n",
            "epoch 46\n",
            " batch Loss train: 0.06351988762617111\n",
            "i 373\n",
            "epoch 46\n",
            " batch Loss train: 0.06930585950613022\n",
            "i 374\n",
            "epoch 46\n",
            " batch Loss train: 0.07080178707838058\n",
            "i 375\n",
            "epoch 46\n",
            " batch Loss train: 0.05980297550559044\n",
            "i 376\n",
            "epoch 46\n",
            " batch Loss train: 0.061311498284339905\n",
            "i 377\n",
            "epoch 46\n",
            " batch Loss train: 0.06111789494752884\n",
            "i 378\n",
            "epoch 46\n",
            " batch Loss train: 0.0727245956659317\n",
            "i 379\n",
            "epoch 46\n",
            " batch Loss train: 0.07187851518392563\n",
            "i 380\n",
            "epoch 46\n",
            " batch Loss train: 0.058359213173389435\n",
            "i 381\n",
            "epoch 46\n",
            " batch Loss train: 0.06396448612213135\n",
            "i 382\n",
            "epoch 46\n",
            " batch Loss train: 0.10586947947740555\n",
            "i 383\n",
            "epoch 46\n",
            " batch Loss train: 0.07736311852931976\n",
            "i 384\n",
            "epoch 46\n",
            " batch Loss train: 0.0661667212843895\n",
            "i 385\n",
            "epoch 46\n",
            " batch Loss train: 0.04682257026433945\n",
            "i 386\n",
            "epoch 46\n",
            " batch Loss train: 0.06561267375946045\n",
            "i 387\n",
            "epoch 46\n",
            " batch Loss train: 0.07355581223964691\n",
            "i 388\n",
            "epoch 46\n",
            " batch Loss train: 0.054421842098236084\n",
            "i 389\n",
            "epoch 46\n",
            " batch Loss train: 0.05527230352163315\n",
            "i 390\n",
            "epoch 46\n",
            " batch Loss train: 0.052337534725666046\n",
            "i 391\n",
            "epoch 46\n",
            " batch Loss train: 0.08488541841506958\n",
            "i 392\n",
            "epoch 46\n",
            " batch Loss train: 0.0554325208067894\n",
            "i 393\n",
            "epoch 46\n",
            " batch Loss train: 0.06756249070167542\n",
            "i 394\n",
            "epoch 46\n",
            " batch Loss train: 0.056528814136981964\n",
            "i 395\n",
            "epoch 46\n",
            " batch Loss train: 0.06225646287202835\n",
            "i 396\n",
            "epoch 46\n",
            " batch Loss train: 0.07280036807060242\n",
            "i 397\n",
            "epoch 46\n",
            " batch Loss train: 0.073776014149189\n",
            "i 398\n",
            "epoch 46\n",
            " batch Loss train: 0.0611083023250103\n",
            "i 399\n",
            "epoch 46\n",
            " batch Loss train: 0.05347761511802673\n",
            "i 400\n",
            "epoch 46\n",
            " batch Loss train: 0.08344275504350662\n",
            "i 401\n",
            "epoch 46\n",
            " batch Loss train: 0.0795498713850975\n",
            "i 402\n",
            "epoch 46\n",
            " batch Loss train: 0.053952932357788086\n",
            "i 403\n",
            "epoch 46\n",
            " batch Loss train: 0.051091473549604416\n",
            "i 404\n",
            "epoch 46\n",
            " batch Loss train: 0.06606533378362656\n",
            "i 405\n",
            "epoch 46\n",
            " batch Loss train: 0.050721265375614166\n",
            "i 406\n",
            "epoch 46\n",
            " batch Loss train: 0.04875984042882919\n",
            "i 407\n",
            "epoch 46\n",
            " batch Loss train: 0.060797423124313354\n",
            "i 408\n",
            "epoch 46\n",
            " batch Loss train: 0.07886002957820892\n",
            "i 409\n",
            "epoch 46\n",
            " batch Loss train: 0.06746575236320496\n",
            "i 410\n",
            "epoch 46\n",
            " batch Loss train: 0.055898524820804596\n",
            "i 411\n",
            "epoch 46\n",
            " batch Loss train: 0.05003879591822624\n",
            "i 412\n",
            "epoch 46\n",
            " batch Loss train: 0.05669521912932396\n",
            "i 413\n",
            "epoch 46\n",
            " batch Loss train: 0.0681658685207367\n",
            "i 414\n",
            "epoch 46\n",
            " batch Loss train: 0.06617126613855362\n",
            "i 415\n",
            "epoch 46\n",
            " batch Loss train: 0.04936905577778816\n",
            "i 416\n",
            "epoch 46\n",
            " batch Loss train: 0.060179103165864944\n",
            "i 417\n",
            "epoch 46\n",
            " batch Loss train: 0.04997920244932175\n",
            "i 418\n",
            "epoch 46\n",
            " batch Loss train: 0.05816539749503136\n",
            "i 419\n",
            "epoch 46\n",
            " batch Loss train: 0.07820110768079758\n",
            "i 420\n",
            "epoch 46\n",
            " batch Loss train: 0.052067432552576065\n",
            "i 421\n",
            "epoch 46\n",
            " batch Loss train: 0.07187774777412415\n",
            "i 422\n",
            "epoch 46\n",
            " batch Loss train: 0.07178442180156708\n",
            "i 423\n",
            "epoch 46\n",
            " batch Loss train: 0.06819134950637817\n",
            "i 424\n",
            "epoch 46\n",
            " batch Loss train: 0.05303683876991272\n",
            "i 425\n",
            "epoch 46\n",
            " batch Loss train: 0.09033457189798355\n",
            "i 426\n",
            "epoch 46\n",
            " batch Loss train: 0.05084264650940895\n",
            "i 427\n",
            "epoch 46\n",
            " batch Loss train: 0.06422064453363419\n",
            "i 428\n",
            "epoch 46\n",
            " batch Loss train: 0.052579671144485474\n",
            "i 429\n",
            "epoch 46\n",
            " batch Loss train: 0.07253111898899078\n",
            "i 430\n",
            "epoch 46\n",
            " batch Loss train: 0.06376001983880997\n",
            "i 431\n",
            "epoch 46\n",
            " batch Loss train: 0.06594783067703247\n",
            "i 432\n",
            "epoch 46\n",
            " batch Loss train: 0.07241129130125046\n",
            "i 433\n",
            "epoch 46\n",
            " batch Loss train: 0.06573038548231125\n",
            "i 434\n",
            "epoch 46\n",
            " batch Loss train: 0.08638717979192734\n",
            "i 435\n",
            "epoch 46\n",
            " batch Loss train: 0.0696641281247139\n",
            "i 436\n",
            "epoch 46\n",
            " batch Loss train: 0.07711353152990341\n",
            "i 437\n",
            "epoch 46\n",
            " batch Loss train: 0.07249481230974197\n",
            "i 438\n",
            "epoch 46\n",
            " batch Loss train: 0.09028022736310959\n",
            "i 439\n",
            "epoch 46\n",
            " batch Loss train: 0.048588354140520096\n",
            "i 440\n",
            "epoch 46\n",
            " batch Loss train: 0.10489705950021744\n",
            "i 441\n",
            "epoch 46\n",
            " batch Loss train: 0.07012039422988892\n",
            "i 442\n",
            "epoch 46\n",
            " batch Loss train: 0.06407970935106277\n",
            "i 443\n",
            "epoch 46\n",
            " batch Loss train: 0.07243676483631134\n",
            "i 444\n",
            "epoch 46\n",
            " batch Loss train: 0.08849971741437912\n",
            "i 445\n",
            "epoch 46\n",
            " batch Loss train: 0.04076561704277992\n",
            "total epoch Loss train: tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 47\n",
            " batch Loss train: 0.052675552666187286\n",
            "i 1\n",
            "epoch 47\n",
            " batch Loss train: 0.06420236825942993\n",
            "i 2\n",
            "epoch 47\n",
            " batch Loss train: 0.04424323886632919\n",
            "i 3\n",
            "epoch 47\n",
            " batch Loss train: 0.04264719784259796\n",
            "i 4\n",
            "epoch 47\n",
            " batch Loss train: 0.05140576884150505\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 47\n",
            " batch Loss train: 0.04379225894808769\n",
            "i 6\n",
            "epoch 47\n",
            " batch Loss train: 0.08705158531665802\n",
            "i 7\n",
            "epoch 47\n",
            " batch Loss train: 0.04425494745373726\n",
            "i 8\n",
            "epoch 47\n",
            " batch Loss train: 0.056672919541597366\n",
            "i 9\n",
            "epoch 47\n",
            " batch Loss train: 0.05894706770777702\n",
            "i 10\n",
            "epoch 47\n",
            " batch Loss train: 0.06340805441141129\n",
            "i 11\n",
            "epoch 47\n",
            " batch Loss train: 0.059505511075258255\n",
            "i 12\n",
            "epoch 47\n",
            " batch Loss train: 0.05801188573241234\n",
            "i 13\n",
            "epoch 47\n",
            " batch Loss train: 0.05629779398441315\n",
            "i 14\n",
            "epoch 47\n",
            " batch Loss train: 0.058183614164590836\n",
            "i 15\n",
            "epoch 47\n",
            " batch Loss train: 0.06325210630893707\n",
            "i 16\n",
            "epoch 47\n",
            " batch Loss train: 0.057423416525125504\n",
            "i 17\n",
            "epoch 47\n",
            " batch Loss train: 0.0481235533952713\n",
            "i 18\n",
            "epoch 47\n",
            " batch Loss train: 0.05874459818005562\n",
            "i 19\n",
            "epoch 47\n",
            " batch Loss train: 0.05959424749016762\n",
            "i 20\n",
            "epoch 47\n",
            " batch Loss train: 0.041051674634218216\n",
            "i 21\n",
            "epoch 47\n",
            " batch Loss train: 0.05231522396206856\n",
            "i 22\n",
            "epoch 47\n",
            " batch Loss train: 0.05118648335337639\n",
            "i 23\n",
            "epoch 47\n",
            " batch Loss train: 0.05976826325058937\n",
            "i 24\n",
            "epoch 47\n",
            " batch Loss train: 0.05520377308130264\n",
            "i 25\n",
            "epoch 47\n",
            " batch Loss train: 0.039726123213768005\n",
            "i 26\n",
            "epoch 47\n",
            " batch Loss train: 0.038341373205184937\n",
            "i 27\n",
            "epoch 47\n",
            " batch Loss train: 0.07171739637851715\n",
            "i 28\n",
            "epoch 47\n",
            " batch Loss train: 0.05925537273287773\n",
            "i 29\n",
            "epoch 47\n",
            " batch Loss train: 0.04994909465312958\n",
            "i 30\n",
            "epoch 47\n",
            " batch Loss train: 0.04639706015586853\n",
            "i 31\n",
            "epoch 47\n",
            " batch Loss train: 0.038445018231868744\n",
            "i 32\n",
            "epoch 47\n",
            " batch Loss train: 0.04151694476604462\n",
            "i 33\n",
            "epoch 47\n",
            " batch Loss train: 0.05788785591721535\n",
            "i 34\n",
            "epoch 47\n",
            " batch Loss train: 0.048544447869062424\n",
            "i 35\n",
            "epoch 47\n",
            " batch Loss train: 0.0788322314620018\n",
            "i 36\n",
            "epoch 47\n",
            " batch Loss train: 0.05848122388124466\n",
            "i 37\n",
            "epoch 47\n",
            " batch Loss train: 0.06186860799789429\n",
            "i 38\n",
            "epoch 47\n",
            " batch Loss train: 0.04185770824551582\n",
            "i 39\n",
            "epoch 47\n",
            " batch Loss train: 0.06152836233377457\n",
            "i 40\n",
            "epoch 47\n",
            " batch Loss train: 0.06705202907323837\n",
            "i 41\n",
            "epoch 47\n",
            " batch Loss train: 0.043211083859205246\n",
            "i 42\n",
            "epoch 47\n",
            " batch Loss train: 0.04710144177079201\n",
            "i 43\n",
            "epoch 47\n",
            " batch Loss train: 0.05668831616640091\n",
            "i 44\n",
            "epoch 47\n",
            " batch Loss train: 0.06027071550488472\n",
            "i 45\n",
            "epoch 47\n",
            " batch Loss train: 0.04773412644863129\n",
            "i 46\n",
            "epoch 47\n",
            " batch Loss train: 0.05487953498959541\n",
            "i 47\n",
            "epoch 47\n",
            " batch Loss train: 0.05344581604003906\n",
            "i 48\n",
            "epoch 47\n",
            " batch Loss train: 0.04595683142542839\n",
            "i 49\n",
            "epoch 47\n",
            " batch Loss train: 0.05404246225953102\n",
            "i 50\n",
            "epoch 47\n",
            " batch Loss train: 0.04387933760881424\n",
            "i 51\n",
            "epoch 47\n",
            " batch Loss train: 0.05224274471402168\n",
            "i 52\n",
            "epoch 47\n",
            " batch Loss train: 0.04251076281070709\n",
            "i 53\n",
            "epoch 47\n",
            " batch Loss train: 0.0507107749581337\n",
            "i 54\n",
            "epoch 47\n",
            " batch Loss train: 0.04917912185192108\n",
            "i 55\n",
            "epoch 47\n",
            " batch Loss train: 0.05361756682395935\n",
            "i 56\n",
            "epoch 47\n",
            " batch Loss train: 0.07857437431812286\n",
            "i 57\n",
            "epoch 47\n",
            " batch Loss train: 0.06929314881563187\n",
            "i 58\n",
            "epoch 47\n",
            " batch Loss train: 0.04033002629876137\n",
            "i 59\n",
            "epoch 47\n",
            " batch Loss train: 0.03802378103137016\n",
            "i 60\n",
            "epoch 47\n",
            " batch Loss train: 0.06375450640916824\n",
            "i 61\n",
            "epoch 47\n",
            " batch Loss train: 0.056272123008966446\n",
            "i 62\n",
            "epoch 47\n",
            " batch Loss train: 0.054349303245544434\n",
            "i 63\n",
            "epoch 47\n",
            " batch Loss train: 0.046311311423778534\n",
            "i 64\n",
            "epoch 47\n",
            " batch Loss train: 0.0728503093123436\n",
            "i 65\n",
            "epoch 47\n",
            " batch Loss train: 0.04798353090882301\n",
            "i 66\n",
            "epoch 47\n",
            " batch Loss train: 0.06721238046884537\n",
            "i 67\n",
            "epoch 47\n",
            " batch Loss train: 0.04587652534246445\n",
            "i 68\n",
            "epoch 47\n",
            " batch Loss train: 0.04424349591135979\n",
            "i 69\n",
            "epoch 47\n",
            " batch Loss train: 0.044177863746881485\n",
            "i 70\n",
            "epoch 47\n",
            " batch Loss train: 0.056523315608501434\n",
            "i 71\n",
            "epoch 47\n",
            " batch Loss train: 0.05192047357559204\n",
            "i 72\n",
            "epoch 47\n",
            " batch Loss train: 0.0483630895614624\n",
            "i 73\n",
            "epoch 47\n",
            " batch Loss train: 0.07380007952451706\n",
            "i 74\n",
            "epoch 47\n",
            " batch Loss train: 0.05189977586269379\n",
            "i 75\n",
            "epoch 47\n",
            " batch Loss train: 0.0500379242002964\n",
            "i 76\n",
            "epoch 47\n",
            " batch Loss train: 0.06643055379390717\n",
            "i 77\n",
            "epoch 47\n",
            " batch Loss train: 0.05174621194601059\n",
            "i 78\n",
            "epoch 47\n",
            " batch Loss train: 0.0649089366197586\n",
            "i 79\n",
            "epoch 47\n",
            " batch Loss train: 0.05115627869963646\n",
            "i 80\n",
            "epoch 47\n",
            " batch Loss train: 0.05749699845910072\n",
            "i 81\n",
            "epoch 47\n",
            " batch Loss train: 0.07376600801944733\n",
            "i 82\n",
            "epoch 47\n",
            " batch Loss train: 0.054545216262340546\n",
            "i 83\n",
            "epoch 47\n",
            " batch Loss train: 0.06007549539208412\n",
            "i 84\n",
            "epoch 47\n",
            " batch Loss train: 0.06706059724092484\n",
            "i 85\n",
            "epoch 47\n",
            " batch Loss train: 0.06417994201183319\n",
            "i 86\n",
            "epoch 47\n",
            " batch Loss train: 0.06075868383049965\n",
            "i 87\n",
            "epoch 47\n",
            " batch Loss train: 0.04371781274676323\n",
            "i 88\n",
            "epoch 47\n",
            " batch Loss train: 0.06089094281196594\n",
            "i 89\n",
            "epoch 47\n",
            " batch Loss train: 0.07642363756895065\n",
            "i 90\n",
            "epoch 47\n",
            " batch Loss train: 0.036789100617170334\n",
            "i 91\n",
            "epoch 47\n",
            " batch Loss train: 0.06589227169752121\n",
            "i 92\n",
            "epoch 47\n",
            " batch Loss train: 0.036248937249183655\n",
            "i 93\n",
            "epoch 47\n",
            " batch Loss train: 0.05371244251728058\n",
            "i 94\n",
            "epoch 47\n",
            " batch Loss train: 0.07363724708557129\n",
            "i 95\n",
            "epoch 47\n",
            " batch Loss train: 0.052969466894865036\n",
            "i 96\n",
            "epoch 47\n",
            " batch Loss train: 0.05189131572842598\n",
            "i 97\n",
            "epoch 47\n",
            " batch Loss train: 0.035205669701099396\n",
            "i 98\n",
            "epoch 47\n",
            " batch Loss train: 0.05210984870791435\n",
            "i 99\n",
            "epoch 47\n",
            " batch Loss train: 0.05662856996059418\n",
            "i 100\n",
            "epoch 47\n",
            " batch Loss train: 0.059678323566913605\n",
            "i 101\n",
            "epoch 47\n",
            " batch Loss train: 0.045607779175043106\n",
            "i 102\n",
            "epoch 47\n",
            " batch Loss train: 0.04178754612803459\n",
            "i 103\n",
            "epoch 47\n",
            " batch Loss train: 0.05946999788284302\n",
            "i 104\n",
            "epoch 47\n",
            " batch Loss train: 0.0800849050283432\n",
            "i 105\n",
            "epoch 47\n",
            " batch Loss train: 0.05315876752138138\n",
            "i 106\n",
            "epoch 47\n",
            " batch Loss train: 0.06220154091715813\n",
            "i 107\n",
            "epoch 47\n",
            " batch Loss train: 0.06640709191560745\n",
            "i 108\n",
            "epoch 47\n",
            " batch Loss train: 0.04680091142654419\n",
            "i 109\n",
            "epoch 47\n",
            " batch Loss train: 0.05513746291399002\n",
            "i 110\n",
            "epoch 47\n",
            " batch Loss train: 0.03574693202972412\n",
            "i 111\n",
            "epoch 47\n",
            " batch Loss train: 0.05139534920454025\n",
            "i 112\n",
            "epoch 47\n",
            " batch Loss train: 0.04371311515569687\n",
            "i 113\n",
            "epoch 47\n",
            " batch Loss train: 0.05115329474210739\n",
            "i 114\n",
            "epoch 47\n",
            " batch Loss train: 0.05786664038896561\n",
            "i 115\n",
            "epoch 47\n",
            " batch Loss train: 0.05544927716255188\n",
            "i 116\n",
            "epoch 47\n",
            " batch Loss train: 0.04270043596625328\n",
            "i 117\n",
            "epoch 47\n",
            " batch Loss train: 0.05913839489221573\n",
            "i 118\n",
            "epoch 47\n",
            " batch Loss train: 0.05356374755501747\n",
            "i 119\n",
            "epoch 47\n",
            " batch Loss train: 0.05269407108426094\n",
            "i 120\n",
            "epoch 47\n",
            " batch Loss train: 0.047689810395240784\n",
            "i 121\n",
            "epoch 47\n",
            " batch Loss train: 0.04559880495071411\n",
            "i 122\n",
            "epoch 47\n",
            " batch Loss train: 0.048646267503499985\n",
            "i 123\n",
            "epoch 47\n",
            " batch Loss train: 0.05286944657564163\n",
            "i 124\n",
            "epoch 47\n",
            " batch Loss train: 0.04482200741767883\n",
            "i 125\n",
            "epoch 47\n",
            " batch Loss train: 0.05407257005572319\n",
            "i 126\n",
            "epoch 47\n",
            " batch Loss train: 0.04657693952322006\n",
            "i 127\n",
            "epoch 47\n",
            " batch Loss train: 0.048963237553834915\n",
            "i 128\n",
            "epoch 47\n",
            " batch Loss train: 0.058014292269945145\n",
            "i 129\n",
            "epoch 47\n",
            " batch Loss train: 0.0637412890791893\n",
            "i 130\n",
            "epoch 47\n",
            " batch Loss train: 0.0664072036743164\n",
            "i 131\n",
            "epoch 47\n",
            " batch Loss train: 0.0471450500190258\n",
            "i 132\n",
            "epoch 47\n",
            " batch Loss train: 0.04615221545100212\n",
            "i 133\n",
            "epoch 47\n",
            " batch Loss train: 0.03851855546236038\n",
            "i 134\n",
            "epoch 47\n",
            " batch Loss train: 0.0412479043006897\n",
            "i 135\n",
            "epoch 47\n",
            " batch Loss train: 0.04541934281587601\n",
            "i 136\n",
            "epoch 47\n",
            " batch Loss train: 0.04876890406012535\n",
            "i 137\n",
            "epoch 47\n",
            " batch Loss train: 0.044241320341825485\n",
            "i 138\n",
            "epoch 47\n",
            " batch Loss train: 0.05408451333642006\n",
            "i 139\n",
            "epoch 47\n",
            " batch Loss train: 0.08285560458898544\n",
            "i 140\n",
            "epoch 47\n",
            " batch Loss train: 0.06117922067642212\n",
            "i 141\n",
            "epoch 47\n",
            " batch Loss train: 0.04499199613928795\n",
            "i 142\n",
            "epoch 47\n",
            " batch Loss train: 0.05789639800786972\n",
            "i 143\n",
            "epoch 47\n",
            " batch Loss train: 0.0446552075445652\n",
            "i 144\n",
            "epoch 47\n",
            " batch Loss train: 0.04515919089317322\n",
            "i 145\n",
            "epoch 47\n",
            " batch Loss train: 0.06317697465419769\n",
            "i 146\n",
            "epoch 47\n",
            " batch Loss train: 0.06626538932323456\n",
            "i 147\n",
            "epoch 47\n",
            " batch Loss train: 0.053054895251989365\n",
            "i 148\n",
            "epoch 47\n",
            " batch Loss train: 0.04018006846308708\n",
            "i 149\n",
            "epoch 47\n",
            " batch Loss train: 0.051904987543821335\n",
            "i 150\n",
            "epoch 47\n",
            " batch Loss train: 0.053711310029029846\n",
            "i 151\n",
            "epoch 47\n",
            " batch Loss train: 0.07202319800853729\n",
            "i 152\n",
            "epoch 47\n",
            " batch Loss train: 0.03856678307056427\n",
            "i 153\n",
            "epoch 47\n",
            " batch Loss train: 0.06236080825328827\n",
            "i 154\n",
            "epoch 47\n",
            " batch Loss train: 0.050627272576093674\n",
            "i 155\n",
            "epoch 47\n",
            " batch Loss train: 0.04093501344323158\n",
            "i 156\n",
            "epoch 47\n",
            " batch Loss train: 0.03885863348841667\n",
            "i 157\n",
            "epoch 47\n",
            " batch Loss train: 0.05313475430011749\n",
            "i 158\n",
            "epoch 47\n",
            " batch Loss train: 0.04812309518456459\n",
            "i 159\n",
            "epoch 47\n",
            " batch Loss train: 0.059361014515161514\n",
            "i 160\n",
            "epoch 47\n",
            " batch Loss train: 0.046053096652030945\n",
            "i 161\n",
            "epoch 47\n",
            " batch Loss train: 0.05590217188000679\n",
            "i 162\n",
            "epoch 47\n",
            " batch Loss train: 0.04216249659657478\n",
            "i 163\n",
            "epoch 47\n",
            " batch Loss train: 0.046542033553123474\n",
            "i 164\n",
            "epoch 47\n",
            " batch Loss train: 0.04482989385724068\n",
            "i 165\n",
            "epoch 47\n",
            " batch Loss train: 0.04811162129044533\n",
            "i 166\n",
            "epoch 47\n",
            " batch Loss train: 0.058800987899303436\n",
            "i 167\n",
            "epoch 47\n",
            " batch Loss train: 0.04442545771598816\n",
            "i 168\n",
            "epoch 47\n",
            " batch Loss train: 0.05985156074166298\n",
            "i 169\n",
            "epoch 47\n",
            " batch Loss train: 0.03981329873204231\n",
            "i 170\n",
            "epoch 47\n",
            " batch Loss train: 0.03131962940096855\n",
            "i 171\n",
            "epoch 47\n",
            " batch Loss train: 0.07748997211456299\n",
            "i 172\n",
            "epoch 47\n",
            " batch Loss train: 0.04639719799160957\n",
            "i 173\n",
            "epoch 47\n",
            " batch Loss train: 0.06572389602661133\n",
            "i 174\n",
            "epoch 47\n",
            " batch Loss train: 0.059039127081632614\n",
            "i 175\n",
            "epoch 47\n",
            " batch Loss train: 0.0505586676299572\n",
            "i 176\n",
            "epoch 47\n",
            " batch Loss train: 0.05101644620299339\n",
            "i 177\n",
            "epoch 47\n",
            " batch Loss train: 0.05969860777258873\n",
            "i 178\n",
            "epoch 47\n",
            " batch Loss train: 0.060537245124578476\n",
            "i 179\n",
            "epoch 47\n",
            " batch Loss train: 0.04314033314585686\n",
            "i 180\n",
            "epoch 47\n",
            " batch Loss train: 0.04476957023143768\n",
            "i 181\n",
            "epoch 47\n",
            " batch Loss train: 0.052739813923835754\n",
            "i 182\n",
            "epoch 47\n",
            " batch Loss train: 0.05787568539381027\n",
            "i 183\n",
            "epoch 47\n",
            " batch Loss train: 0.04677082970738411\n",
            "i 184\n",
            "epoch 47\n",
            " batch Loss train: 0.055542826652526855\n",
            "i 185\n",
            "epoch 47\n",
            " batch Loss train: 0.07378604263067245\n",
            "i 186\n",
            "epoch 47\n",
            " batch Loss train: 0.049756769090890884\n",
            "i 187\n",
            "epoch 47\n",
            " batch Loss train: 0.04718352481722832\n",
            "i 188\n",
            "epoch 47\n",
            " batch Loss train: 0.05953780561685562\n",
            "i 189\n",
            "epoch 47\n",
            " batch Loss train: 0.04486057907342911\n",
            "i 190\n",
            "epoch 47\n",
            " batch Loss train: 0.06747590750455856\n",
            "i 191\n",
            "epoch 47\n",
            " batch Loss train: 0.0671810433268547\n",
            "i 192\n",
            "epoch 47\n",
            " batch Loss train: 0.03633991628885269\n",
            "i 193\n",
            "epoch 47\n",
            " batch Loss train: 0.05689249932765961\n",
            "i 194\n",
            "epoch 47\n",
            " batch Loss train: 0.043456725776195526\n",
            "i 195\n",
            "epoch 47\n",
            " batch Loss train: 0.056594498455524445\n",
            "i 196\n",
            "epoch 47\n",
            " batch Loss train: 0.06141633912920952\n",
            "i 197\n",
            "epoch 47\n",
            " batch Loss train: 0.046472009271383286\n",
            "i 198\n",
            "epoch 47\n",
            " batch Loss train: 0.0661265105009079\n",
            "i 199\n",
            "epoch 47\n",
            " batch Loss train: 0.06447618454694748\n",
            "i 200\n",
            "epoch 47\n",
            " batch Loss train: 0.07226239889860153\n",
            "i 201\n",
            "epoch 47\n",
            " batch Loss train: 0.06382865458726883\n",
            "i 202\n",
            "epoch 47\n",
            " batch Loss train: 0.05838729441165924\n",
            "i 203\n",
            "epoch 47\n",
            " batch Loss train: 0.07925038784742355\n",
            "i 204\n",
            "epoch 47\n",
            " batch Loss train: 0.04754934087395668\n",
            "i 205\n",
            "epoch 47\n",
            " batch Loss train: 0.07992542535066605\n",
            "i 206\n",
            "epoch 47\n",
            " batch Loss train: 0.04848429188132286\n",
            "i 207\n",
            "epoch 47\n",
            " batch Loss train: 0.06178218498826027\n",
            "i 208\n",
            "epoch 47\n",
            " batch Loss train: 0.058511752635240555\n",
            "i 209\n",
            "epoch 47\n",
            " batch Loss train: 0.057562489062547684\n",
            "i 210\n",
            "epoch 47\n",
            " batch Loss train: 0.05961630493402481\n",
            "i 211\n",
            "epoch 47\n",
            " batch Loss train: 0.054415300488471985\n",
            "i 212\n",
            "epoch 47\n",
            " batch Loss train: 0.0451742559671402\n",
            "i 213\n",
            "epoch 47\n",
            " batch Loss train: 0.05976588651537895\n",
            "i 214\n",
            "epoch 47\n",
            " batch Loss train: 0.05590552091598511\n",
            "i 215\n",
            "epoch 47\n",
            " batch Loss train: 0.053295619785785675\n",
            "i 216\n",
            "epoch 47\n",
            " batch Loss train: 0.06379952281713486\n",
            "i 217\n",
            "epoch 47\n",
            " batch Loss train: 0.05242379754781723\n",
            "i 218\n",
            "epoch 47\n",
            " batch Loss train: 0.06324095278978348\n",
            "i 219\n",
            "epoch 47\n",
            " batch Loss train: 0.06645021587610245\n",
            "i 220\n",
            "epoch 47\n",
            " batch Loss train: 0.058481357991695404\n",
            "i 221\n",
            "epoch 47\n",
            " batch Loss train: 0.040852583944797516\n",
            "i 222\n",
            "epoch 47\n",
            " batch Loss train: 0.07940271496772766\n",
            "i 223\n",
            "epoch 47\n",
            " batch Loss train: 0.06032116711139679\n",
            "i 224\n",
            "epoch 47\n",
            " batch Loss train: 0.06482226401567459\n",
            "i 225\n",
            "epoch 47\n",
            " batch Loss train: 0.06575038284063339\n",
            "i 226\n",
            "epoch 47\n",
            " batch Loss train: 0.03799916058778763\n",
            "i 227\n",
            "epoch 47\n",
            " batch Loss train: 0.0701560527086258\n",
            "i 228\n",
            "epoch 47\n",
            " batch Loss train: 0.06524685025215149\n",
            "i 229\n",
            "epoch 47\n",
            " batch Loss train: 0.06269480288028717\n",
            "i 230\n",
            "epoch 47\n",
            " batch Loss train: 0.0797690898180008\n",
            "i 231\n",
            "epoch 47\n",
            " batch Loss train: 0.049195729196071625\n",
            "i 232\n",
            "epoch 47\n",
            " batch Loss train: 0.06083540990948677\n",
            "i 233\n",
            "epoch 47\n",
            " batch Loss train: 0.04969731345772743\n",
            "i 234\n",
            "epoch 47\n",
            " batch Loss train: 0.055701497942209244\n",
            "i 235\n",
            "epoch 47\n",
            " batch Loss train: 0.05879746377468109\n",
            "i 236\n",
            "epoch 47\n",
            " batch Loss train: 0.06886806339025497\n",
            "i 237\n",
            "epoch 47\n",
            " batch Loss train: 0.058017659932374954\n",
            "i 238\n",
            "epoch 47\n",
            " batch Loss train: 0.04508909210562706\n",
            "i 239\n",
            "epoch 47\n",
            " batch Loss train: 0.061511505395174026\n",
            "i 240\n",
            "epoch 47\n",
            " batch Loss train: 0.043121278285980225\n",
            "i 241\n",
            "epoch 47\n",
            " batch Loss train: 0.06062353029847145\n",
            "i 242\n",
            "epoch 47\n",
            " batch Loss train: 0.044233717024326324\n",
            "i 243\n",
            "epoch 47\n",
            " batch Loss train: 0.05539602413773537\n",
            "i 244\n",
            "epoch 47\n",
            " batch Loss train: 0.06491975486278534\n",
            "i 245\n",
            "epoch 47\n",
            " batch Loss train: 0.06192176416516304\n",
            "i 246\n",
            "epoch 47\n",
            " batch Loss train: 0.06051064282655716\n",
            "i 247\n",
            "epoch 47\n",
            " batch Loss train: 0.058488935232162476\n",
            "i 248\n",
            "epoch 47\n",
            " batch Loss train: 0.0549667552113533\n",
            "i 249\n",
            "epoch 47\n",
            " batch Loss train: 0.044934868812561035\n",
            "i 250\n",
            "epoch 47\n",
            " batch Loss train: 0.060057077556848526\n",
            "i 251\n",
            "epoch 47\n",
            " batch Loss train: 0.07438947260379791\n",
            "i 252\n",
            "epoch 47\n",
            " batch Loss train: 0.05091831088066101\n",
            "i 253\n",
            "epoch 47\n",
            " batch Loss train: 0.06861820071935654\n",
            "i 254\n",
            "epoch 47\n",
            " batch Loss train: 0.06309834867715836\n",
            "i 255\n",
            "epoch 47\n",
            " batch Loss train: 0.053460001945495605\n",
            "i 256\n",
            "epoch 47\n",
            " batch Loss train: 0.04583972319960594\n",
            "i 257\n",
            "epoch 47\n",
            " batch Loss train: 0.07225363701581955\n",
            "i 258\n",
            "epoch 47\n",
            " batch Loss train: 0.06359760463237762\n",
            "i 259\n",
            "epoch 47\n",
            " batch Loss train: 0.06032611429691315\n",
            "i 260\n",
            "epoch 47\n",
            " batch Loss train: 0.06037408486008644\n",
            "i 261\n",
            "epoch 47\n",
            " batch Loss train: 0.05874178558588028\n",
            "i 262\n",
            "epoch 47\n",
            " batch Loss train: 0.05944863334298134\n",
            "i 263\n",
            "epoch 47\n",
            " batch Loss train: 0.05276263877749443\n",
            "i 264\n",
            "epoch 47\n",
            " batch Loss train: 0.06303185969591141\n",
            "i 265\n",
            "epoch 47\n",
            " batch Loss train: 0.05814695358276367\n",
            "i 266\n",
            "epoch 47\n",
            " batch Loss train: 0.043300360441207886\n",
            "i 267\n",
            "epoch 47\n",
            " batch Loss train: 0.04625747352838516\n",
            "i 268\n",
            "epoch 47\n",
            " batch Loss train: 0.06964849680662155\n",
            "i 269\n",
            "epoch 47\n",
            " batch Loss train: 0.06183158978819847\n",
            "i 270\n",
            "epoch 47\n",
            " batch Loss train: 0.07614456862211227\n",
            "i 271\n",
            "epoch 47\n",
            " batch Loss train: 0.06591659039258957\n",
            "i 272\n",
            "epoch 47\n",
            " batch Loss train: 0.0602041594684124\n",
            "i 273\n",
            "epoch 47\n",
            " batch Loss train: 0.06664290279150009\n",
            "i 274\n",
            "epoch 47\n",
            " batch Loss train: 0.06382061541080475\n",
            "i 275\n",
            "epoch 47\n",
            " batch Loss train: 0.07437190413475037\n",
            "i 276\n",
            "epoch 47\n",
            " batch Loss train: 0.07053189724683762\n",
            "i 277\n",
            "epoch 47\n",
            " batch Loss train: 0.05758468061685562\n",
            "i 278\n",
            "epoch 47\n",
            " batch Loss train: 0.05482792109251022\n",
            "i 279\n",
            "epoch 47\n",
            " batch Loss train: 0.08187713474035263\n",
            "i 280\n",
            "epoch 47\n",
            " batch Loss train: 0.06308753043413162\n",
            "i 281\n",
            "epoch 47\n",
            " batch Loss train: 0.06630237400531769\n",
            "i 282\n",
            "epoch 47\n",
            " batch Loss train: 0.045937176793813705\n",
            "i 283\n",
            "epoch 47\n",
            " batch Loss train: 0.06228524073958397\n",
            "i 284\n",
            "epoch 47\n",
            " batch Loss train: 0.05794304981827736\n",
            "i 285\n",
            "epoch 47\n",
            " batch Loss train: 0.08448200672864914\n",
            "i 286\n",
            "epoch 47\n",
            " batch Loss train: 0.0597669780254364\n",
            "i 287\n",
            "epoch 47\n",
            " batch Loss train: 0.054013803601264954\n",
            "i 288\n",
            "epoch 47\n",
            " batch Loss train: 0.047810427844524384\n",
            "i 289\n",
            "epoch 47\n",
            " batch Loss train: 0.03540730103850365\n",
            "i 290\n",
            "epoch 47\n",
            " batch Loss train: 0.06312563270330429\n",
            "i 291\n",
            "epoch 47\n",
            " batch Loss train: 0.05630764365196228\n",
            "i 292\n",
            "epoch 47\n",
            " batch Loss train: 0.04494417458772659\n",
            "i 293\n",
            "epoch 47\n",
            " batch Loss train: 0.047549400478601456\n",
            "i 294\n",
            "epoch 47\n",
            " batch Loss train: 0.05920414254069328\n",
            "i 295\n",
            "epoch 47\n",
            " batch Loss train: 0.04546605795621872\n",
            "i 296\n",
            "epoch 47\n",
            " batch Loss train: 0.07599493116140366\n",
            "i 297\n",
            "epoch 47\n",
            " batch Loss train: 0.06127281114459038\n",
            "i 298\n",
            "epoch 47\n",
            " batch Loss train: 0.03928631171584129\n",
            "i 299\n",
            "epoch 47\n",
            " batch Loss train: 0.047450631856918335\n",
            "i 300\n",
            "epoch 47\n",
            " batch Loss train: 0.06788430362939835\n",
            "i 301\n",
            "epoch 47\n",
            " batch Loss train: 0.057958394289016724\n",
            "i 302\n",
            "epoch 47\n",
            " batch Loss train: 0.04337361827492714\n",
            "i 303\n",
            "epoch 47\n",
            " batch Loss train: 0.04702959209680557\n",
            "i 304\n",
            "epoch 47\n",
            " batch Loss train: 0.05860573798418045\n",
            "i 305\n",
            "epoch 47\n",
            " batch Loss train: 0.0628967434167862\n",
            "i 306\n",
            "epoch 47\n",
            " batch Loss train: 0.05592851713299751\n",
            "i 307\n",
            "epoch 47\n",
            " batch Loss train: 0.06661497056484222\n",
            "i 308\n",
            "epoch 47\n",
            " batch Loss train: 0.06479612737894058\n",
            "i 309\n",
            "epoch 47\n",
            " batch Loss train: 0.06161656603217125\n",
            "i 310\n",
            "epoch 47\n",
            " batch Loss train: 0.03856061398983002\n",
            "i 311\n",
            "epoch 47\n",
            " batch Loss train: 0.0501520149409771\n",
            "i 312\n",
            "epoch 47\n",
            " batch Loss train: 0.042108841240406036\n",
            "i 313\n",
            "epoch 47\n",
            " batch Loss train: 0.05324937775731087\n",
            "i 314\n",
            "epoch 47\n",
            " batch Loss train: 0.08128815144300461\n",
            "i 315\n",
            "epoch 47\n",
            " batch Loss train: 0.06610342860221863\n",
            "i 316\n",
            "epoch 47\n",
            " batch Loss train: 0.05734756216406822\n",
            "i 317\n",
            "epoch 47\n",
            " batch Loss train: 0.0545845553278923\n",
            "i 318\n",
            "epoch 47\n",
            " batch Loss train: 0.061120208352804184\n",
            "i 319\n",
            "epoch 47\n",
            " batch Loss train: 0.06089052930474281\n",
            "i 320\n",
            "epoch 47\n",
            " batch Loss train: 0.05438277870416641\n",
            "i 321\n",
            "epoch 47\n",
            " batch Loss train: 0.0636327937245369\n",
            "i 322\n",
            "epoch 47\n",
            " batch Loss train: 0.04441307857632637\n",
            "i 323\n",
            "epoch 47\n",
            " batch Loss train: 0.06538327038288116\n",
            "i 324\n",
            "epoch 47\n",
            " batch Loss train: 0.08391030877828598\n",
            "i 325\n",
            "epoch 47\n",
            " batch Loss train: 0.05057685449719429\n",
            "i 326\n",
            "epoch 47\n",
            " batch Loss train: 0.07986138015985489\n",
            "i 327\n",
            "epoch 47\n",
            " batch Loss train: 0.06533912569284439\n",
            "i 328\n",
            "epoch 47\n",
            " batch Loss train: 0.06293009221553802\n",
            "i 329\n",
            "epoch 47\n",
            " batch Loss train: 0.09922713786363602\n",
            "i 330\n",
            "epoch 47\n",
            " batch Loss train: 0.06420906633138657\n",
            "i 331\n",
            "epoch 47\n",
            " batch Loss train: 0.06191185489296913\n",
            "i 332\n",
            "epoch 47\n",
            " batch Loss train: 0.06274241209030151\n",
            "i 333\n",
            "epoch 47\n",
            " batch Loss train: 0.05741451680660248\n",
            "i 334\n",
            "epoch 47\n",
            " batch Loss train: 0.07235278189182281\n",
            "i 335\n",
            "epoch 47\n",
            " batch Loss train: 0.06906300783157349\n",
            "i 336\n",
            "epoch 47\n",
            " batch Loss train: 0.06825921684503555\n",
            "i 337\n",
            "epoch 47\n",
            " batch Loss train: 0.07164859026670456\n",
            "i 338\n",
            "epoch 47\n",
            " batch Loss train: 0.09215077757835388\n",
            "i 339\n",
            "epoch 47\n",
            " batch Loss train: 0.04809171333909035\n",
            "i 340\n",
            "epoch 47\n",
            " batch Loss train: 0.044135551899671555\n",
            "i 341\n",
            "epoch 47\n",
            " batch Loss train: 0.055047594010829926\n",
            "i 342\n",
            "epoch 47\n",
            " batch Loss train: 0.057487305253744125\n",
            "i 343\n",
            "epoch 47\n",
            " batch Loss train: 0.07165117561817169\n",
            "i 344\n",
            "epoch 47\n",
            " batch Loss train: 0.05663266032934189\n",
            "i 345\n",
            "epoch 47\n",
            " batch Loss train: 0.07149478793144226\n",
            "i 346\n",
            "epoch 47\n",
            " batch Loss train: 0.0689074844121933\n",
            "i 347\n",
            "epoch 47\n",
            " batch Loss train: 0.05651860311627388\n",
            "i 348\n",
            "epoch 47\n",
            " batch Loss train: 0.06308804452419281\n",
            "i 349\n",
            "epoch 47\n",
            " batch Loss train: 0.05373711138963699\n",
            "i 350\n",
            "epoch 47\n",
            " batch Loss train: 0.0475732684135437\n",
            "i 351\n",
            "epoch 47\n",
            " batch Loss train: 0.06140110641717911\n",
            "i 352\n",
            "epoch 47\n",
            " batch Loss train: 0.08221488445997238\n",
            "i 353\n",
            "epoch 47\n",
            " batch Loss train: 0.05811421200633049\n",
            "i 354\n",
            "epoch 47\n",
            " batch Loss train: 0.060942407697439194\n",
            "i 355\n",
            "epoch 47\n",
            " batch Loss train: 0.06732127815485\n",
            "i 356\n",
            "epoch 47\n",
            " batch Loss train: 0.08436240255832672\n",
            "i 357\n",
            "epoch 47\n",
            " batch Loss train: 0.06519415229558945\n",
            "i 358\n",
            "epoch 47\n",
            " batch Loss train: 0.04778394103050232\n",
            "i 359\n",
            "epoch 47\n",
            " batch Loss train: 0.08150855451822281\n",
            "i 360\n",
            "epoch 47\n",
            " batch Loss train: 0.05079358443617821\n",
            "i 361\n",
            "epoch 47\n",
            " batch Loss train: 0.05281924083828926\n",
            "i 362\n",
            "epoch 47\n",
            " batch Loss train: 0.04904036223888397\n",
            "i 363\n",
            "epoch 47\n",
            " batch Loss train: 0.0635400116443634\n",
            "i 364\n",
            "epoch 47\n",
            " batch Loss train: 0.06554380804300308\n",
            "i 365\n",
            "epoch 47\n",
            " batch Loss train: 0.046084336936473846\n",
            "i 366\n",
            "epoch 47\n",
            " batch Loss train: 0.06903643906116486\n",
            "i 367\n",
            "epoch 47\n",
            " batch Loss train: 0.085142582654953\n",
            "i 368\n",
            "epoch 47\n",
            " batch Loss train: 0.0763559341430664\n",
            "i 369\n",
            "epoch 47\n",
            " batch Loss train: 0.09462731331586838\n",
            "i 370\n",
            "epoch 47\n",
            " batch Loss train: 0.04993951693177223\n",
            "i 371\n",
            "epoch 47\n",
            " batch Loss train: 0.062220294028520584\n",
            "i 372\n",
            "epoch 47\n",
            " batch Loss train: 0.04938223212957382\n",
            "i 373\n",
            "epoch 47\n",
            " batch Loss train: 0.05679778382182121\n",
            "i 374\n",
            "epoch 47\n",
            " batch Loss train: 0.06362687051296234\n",
            "i 375\n",
            "epoch 47\n",
            " batch Loss train: 0.06087334081530571\n",
            "i 376\n",
            "epoch 47\n",
            " batch Loss train: 0.07074422389268875\n",
            "i 377\n",
            "epoch 47\n",
            " batch Loss train: 0.061137259006500244\n",
            "i 378\n",
            "epoch 47\n",
            " batch Loss train: 0.08240311592817307\n",
            "i 379\n",
            "epoch 47\n",
            " batch Loss train: 0.08526524901390076\n",
            "i 380\n",
            "epoch 47\n",
            " batch Loss train: 0.04543531313538551\n",
            "i 381\n",
            "epoch 47\n",
            " batch Loss train: 0.09266600012779236\n",
            "i 382\n",
            "epoch 47\n",
            " batch Loss train: 0.07532493770122528\n",
            "i 383\n",
            "epoch 47\n",
            " batch Loss train: 0.06928165256977081\n",
            "i 384\n",
            "epoch 47\n",
            " batch Loss train: 0.08318273723125458\n",
            "i 385\n",
            "epoch 47\n",
            " batch Loss train: 0.06518978625535965\n",
            "i 386\n",
            "epoch 47\n",
            " batch Loss train: 0.047456368803977966\n",
            "i 387\n",
            "epoch 47\n",
            " batch Loss train: 0.05099336802959442\n",
            "i 388\n",
            "epoch 47\n",
            " batch Loss train: 0.050889939069747925\n",
            "i 389\n",
            "epoch 47\n",
            " batch Loss train: 0.09242969006299973\n",
            "i 390\n",
            "epoch 47\n",
            " batch Loss train: 0.08013898879289627\n",
            "i 391\n",
            "epoch 47\n",
            " batch Loss train: 0.06192302331328392\n",
            "i 392\n",
            "epoch 47\n",
            " batch Loss train: 0.04921771213412285\n",
            "i 393\n",
            "epoch 47\n",
            " batch Loss train: 0.06522857397794724\n",
            "i 394\n",
            "epoch 47\n",
            " batch Loss train: 0.07321373373270035\n",
            "i 395\n",
            "epoch 47\n",
            " batch Loss train: 0.06881686300039291\n",
            "i 396\n",
            "epoch 47\n",
            " batch Loss train: 0.07065708935260773\n",
            "i 397\n",
            "epoch 47\n",
            " batch Loss train: 0.08453987538814545\n",
            "i 398\n",
            "epoch 47\n",
            " batch Loss train: 0.0664292648434639\n",
            "i 399\n",
            "epoch 47\n",
            " batch Loss train: 0.061667732894420624\n",
            "i 400\n",
            "epoch 47\n",
            " batch Loss train: 0.05570480599999428\n",
            "i 401\n",
            "epoch 47\n",
            " batch Loss train: 0.07365044206380844\n",
            "i 402\n",
            "epoch 47\n",
            " batch Loss train: 0.052099164575338364\n",
            "i 403\n",
            "epoch 47\n",
            " batch Loss train: 0.052464257925748825\n",
            "i 404\n",
            "epoch 47\n",
            " batch Loss train: 0.06404338777065277\n",
            "i 405\n",
            "epoch 47\n",
            " batch Loss train: 0.060057543218135834\n",
            "i 406\n",
            "epoch 47\n",
            " batch Loss train: 0.0588156022131443\n",
            "i 407\n",
            "epoch 47\n",
            " batch Loss train: 0.0675487145781517\n",
            "i 408\n",
            "epoch 47\n",
            " batch Loss train: 0.06562484800815582\n",
            "i 409\n",
            "epoch 47\n",
            " batch Loss train: 0.06007882580161095\n",
            "i 410\n",
            "epoch 47\n",
            " batch Loss train: 0.05121452361345291\n",
            "i 411\n",
            "epoch 47\n",
            " batch Loss train: 0.07314367592334747\n",
            "i 412\n",
            "epoch 47\n",
            " batch Loss train: 0.045562684535980225\n",
            "i 413\n",
            "epoch 47\n",
            " batch Loss train: 0.07820039242506027\n",
            "i 414\n",
            "epoch 47\n",
            " batch Loss train: 0.07724764198064804\n",
            "i 415\n",
            "epoch 47\n",
            " batch Loss train: 0.05675758048892021\n",
            "i 416\n",
            "epoch 47\n",
            " batch Loss train: 0.0602114163339138\n",
            "i 417\n",
            "epoch 47\n",
            " batch Loss train: 0.07629477232694626\n",
            "i 418\n",
            "epoch 47\n",
            " batch Loss train: 0.04629752039909363\n",
            "i 419\n",
            "epoch 47\n",
            " batch Loss train: 0.059666674584150314\n",
            "i 420\n",
            "epoch 47\n",
            " batch Loss train: 0.05645736679434776\n",
            "i 421\n",
            "epoch 47\n",
            " batch Loss train: 0.0744045153260231\n",
            "i 422\n",
            "epoch 47\n",
            " batch Loss train: 0.05545833334326744\n",
            "i 423\n",
            "epoch 47\n",
            " batch Loss train: 0.051593899726867676\n",
            "i 424\n",
            "epoch 47\n",
            " batch Loss train: 0.059645477682352066\n",
            "i 425\n",
            "epoch 47\n",
            " batch Loss train: 0.048551227897405624\n",
            "i 426\n",
            "epoch 47\n",
            " batch Loss train: 0.07471667230129242\n",
            "i 427\n",
            "epoch 47\n",
            " batch Loss train: 0.0659453347325325\n",
            "i 428\n",
            "epoch 47\n",
            " batch Loss train: 0.08381229639053345\n",
            "i 429\n",
            "epoch 47\n",
            " batch Loss train: 0.05740346759557724\n",
            "i 430\n",
            "epoch 47\n",
            " batch Loss train: 0.037821248173713684\n",
            "i 431\n",
            "epoch 47\n",
            " batch Loss train: 0.0502602756023407\n",
            "i 432\n",
            "epoch 47\n",
            " batch Loss train: 0.0601225271821022\n",
            "i 433\n",
            "epoch 47\n",
            " batch Loss train: 0.05782978609204292\n",
            "i 434\n",
            "epoch 47\n",
            " batch Loss train: 0.05725664272904396\n",
            "i 435\n",
            "epoch 47\n",
            " batch Loss train: 0.056879010051488876\n",
            "i 436\n",
            "epoch 47\n",
            " batch Loss train: 0.07560762017965317\n",
            "i 437\n",
            "epoch 47\n",
            " batch Loss train: 0.0542340911924839\n",
            "i 438\n",
            "epoch 47\n",
            " batch Loss train: 0.0705358237028122\n",
            "i 439\n",
            "epoch 47\n",
            " batch Loss train: 0.059858981519937515\n",
            "i 440\n",
            "epoch 47\n",
            " batch Loss train: 0.08141034841537476\n",
            "i 441\n",
            "epoch 47\n",
            " batch Loss train: 0.07349932193756104\n",
            "i 442\n",
            "epoch 47\n",
            " batch Loss train: 0.05605233833193779\n",
            "i 443\n",
            "epoch 47\n",
            " batch Loss train: 0.07566127181053162\n",
            "i 444\n",
            "epoch 47\n",
            " batch Loss train: 0.057985614985227585\n",
            "i 445\n",
            "epoch 47\n",
            " batch Loss train: 0.06347830593585968\n",
            "total epoch Loss train: tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 48\n",
            " batch Loss train: 0.054476045072078705\n",
            "i 1\n",
            "epoch 48\n",
            " batch Loss train: 0.07053312659263611\n",
            "i 2\n",
            "epoch 48\n",
            " batch Loss train: 0.06665542721748352\n",
            "i 3\n",
            "epoch 48\n",
            " batch Loss train: 0.05063895136117935\n",
            "i 4\n",
            "epoch 48\n",
            " batch Loss train: 0.043931856751441956\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAD8CAYAAACRvtrKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZRU9ZXA8e+laUQ6IDYiiKKAtBoiiWFwjUuM4o7IGHGJwS1DYhyjcTyJxnHikkWNexYNmkUTd6OGGDdEE3VQBFFkE0EwAaSBSFgC09IUd/6473VXN1W9VNWr96rqfs7p01Wvlver6q5bv/2KquKccwBd4i6Acy45PCA455p4QHDONfGA4Jxr4gHBOdfEA4JzrklkAUFEjhORBSKySESuiOo8zrnCkSjmIYhIFfA+MApYBkwHzlTVeQU/mXOuYKKqIRwALFLVxaq6GXgYGBPRuZxzBdI1oufdFViadn0ZcGC2O3cR0RG7AY2wZCX8C0gBPofSucLbCqiqZLotqoDQLhGZAEwAEOCoZXARsPuJMPnPcBmwOK7COVfGGtq4Laomw3JgYNr13YJjTVR1oqqOVNWRAtwF3AIwA0YNhfOA/SMqXGtVQPcincu5JIsqIEwH6kRksIh0A84AJrX1gBRwL/CZlcAhcNmnYTz2YSX4XZXtwXnqAvSO6LmdKyWRBARV3QL8J/A8MB94VFXnZrt/emOmHjjnfuDLcH4X2CE43g+7HMU3eSPwcQTP6zonyqAfqgEGtHOeqMuQZJEMO3ZWlYimf9B7AvXaC85ZT6/7rfYQ/pFSxS+eKzPV2JdApWoAUlk6FRM5U3EDwNfXQxd4MziWwoOBK4xKDgbtSWRAAPj+RGA17PN23CVxrnIkNiDcDFZV2O/iim7TOVdMiQ0IAPu9Ar+Wn7Jer+akuAvjXAVIdEBYCEwF4EsMwYcGnYtaogMCwEqAFUfybeBoKntIyLmoJXLYscVtQA+gXkfD8D/Rf04wCuGcy0nJDTumSxEGgP+FnvC5eIvjXFlLfEAAm504V9bAAHj+xOz3q8HXJDiXj8QHhHA661cBFgGnw9nYbEbvT3CusEqiDwFsuukfgC8eA2yAEa/b8mifdeZc55R8H0L4cwUw+QVg6pk+BOlcBBIfEEKNWI3gNoBuD/HSNfADirNCzrlKkfgmQ2u1wAnAL3UA78tHHA5swhc+OddRJd1kaG0N8AgA3dlrJzgEDwbOFUrJ1RBCvYHlTwN1ULN3BIVyrkyVfA2hJzbKEKoiGF1YDaz1uQfOFUrOAUFEBorIyyIyT0TmisglwfFaEZksIguD3zvmW8hwaDG9A3Ez8H/nATfZ9IS+tNx/0TnXefnUELYA/6Wqw4CDgItEZBg2OjhFVeuAKcH1NmWsu6QJA0KXtN9bgXHAB3+AHRdZQKjGgkFJVHtczjzgRyfnz46qrlDVmcHlDdhmqrtiGZruC+52H3BKu8/Vzu0pLChsDa43BsdeIsgGs+e9LeYrbM3wHK58eCdydAryZSoig4DPA9OAfqq6IripHtswuSAy7at4G6DyNWY+DZek3c8513l5BwQR+RQ2q/hSVV2ffpvaEEbGCoCITBCRGSIyoyPjHNmqiXOwLE+c+FmGYQucvErpXG7yCggiEi4xeEBVnwgOrxSRXYLbdwFWZXps68xNHZHpg74aeBpgyrucvh1ciPchOJerfEYZBPgVMF9Vb027aRJwTnD5HOCPuRevWbZt2FPAR8DAo4G5cO1EH4Z0Llc5T0wSkUOBV4HZNPfjfQ/rR3gU2B34GzBOVde09VwdmZjUVqKWpl2VaoERUP8iHIbVHip1NWQV3pfiMmtrYlLJzFSsoXm4sYGW/+xhstaxWP644UfDHS/C77GItLHwRXauZJVFQOhO87deOOyYrhoLGl8GbgiO3Q88A/yVyq0pONdaWQSEdO3l5hsALDwZ2ADPvQwXAGvzKaBzZaTk1zK01t63/UrgM5OAC+G4k22mVEe1t79CdTu3O1fKSrKG0BHdgWXA9icC66DmtcI8r3fWuVJXdjWEjmgErgPq/ww0wE+B4dgGK/nszuzBwJWzkq0hdOSbuhprLlwIHDQSvjsD3sE2WUlhNYjWIxbOlbuy61TsrBpgJHAm0Aebs9ATq0HMwuYrOFcpEh8QuorodhE+fzhPoRboDwwFjgC+egzc8YItkAprDc6Vu8T3IXSleZ4BWFU/7M1vb9OT8MOeTbg/QgP2oV8KzANexw4OBIa08xxR81ELlxSJCAjCtlukdWajk858oDZiQWE6sPwVa0qchzUj0s9dTF4zcUmRiCZD6z6EqIf2wucfACy8AdgbRoyFBRGe07mkSHwfQtSdilnPi4009DoXmAV93rY3y7lylvg+hLikCDZXeQCY2a2p2eBcparogAC2uco9jcBbm3kKGBFzeZyLU0U3GULVwDBgqj7Hc3Icp8ZYFuei5k2GdjRiC6JgT99tyVU0ryEEaoBVxwO94YOH4LMxl8e5qERaQxCRKhF5W0SeDq4PFpFpIrJIRB4RkW75nqMYGoAfPQucCXue6ZOFXGUqRJPhEixJS+hG4DZVHQr8E9ufJPFSwA8BdgCOhoPjLY5zsch3G/bdgBOBe4PrAnwJeDy4S4cyNyXJrkcAF8PzerPXElzFybeGcDvwHZp3Xe4DrFXVLcH1ZVh6t5KxAdi8CaCBgbScUu1cucsnL8NJwCpVfSvHx3cqc1OxpIA7Ac7+b+Z2sXUOdfEWybmiyScvw4+Br2JZoLsDvYAngWOB/qq6RUQOBq5R1WPbeq4kjDKEwtWTI4Fn1sD7tXA1QXYo58pAJKMMqnqlqu6mqoOAM4CXVPUrwMvYbuhQwMxNxZLCVkSuAdhxL6rwLdxd5YhiYtJ3gctEZBHWp/CrCM4RuXnAKHmfPbUbT/jURVchfGJSOzbq4XD5K9TcEndJnCsMn7qcjy2vwEbrWKyJuyzORcwDQgYtdk06F1gEPwH2xjNLu/LmASGD9NTzAx8AlsIorfX9ElzZ84DQjjXANxfAZlnD8/NsONK5cuUBoQMeBC4F+PRz7IfldHCuHHlAaEc1Ni/bkrlcxxHAPnEWyLkI+bBjO/rTvPHqBOD7r0P9wRYUkjZhKewI9W3dXVt82DEPa7GZixuBFwFugv6fhsdiLZVz0fCA0I5GrMmwFfgQeO5JYDiMmhBnqTJLHx1xLhceENqRSvtZB4wH2AP4Za0vjXZlxwNCJ1QTfAN/BKxfw3B8qzVXXjwgdEIjFhS+/gDU7wCvLrZM0h4UXLnwgNAJKSwovAT8CWDwZzkM6BtnoZwrIA8IndSI9SUsAPjLu3wb+BxeS3DlwechdEKYor4vNluxCpiutbwvazgc24/RuaTzeQgF1oDVEpYCz8ka9joV6kdCbczlci5fHhA6IYXNR9iATVjaBNwCMByYAL3xpoMrbfnmZegtIo+LyHsiMl9EDhaRWhGZLCILg987FqqwSdFIUO0CpoHtGjkLjgbGAQPwwOBKU741hDuA51R1H6xvbT5wBTBFVeuAKcH1spL+YU8BO78N5/8cbquFe8+Ffwd64EHBlZ58tmHfAXgHGKJpTyIiC4AvquoKEdkF+Iuq7t3Wc5VKp2JbumM1g9l7AB/BE43wA8Ks0iaFNTPSZzg24FxxtdWp2DWP5x2MrQr+jYh8DngLy/PYT1VXBPepB/rlcY6S0YB1Mp71N/vgr8WaFilsxWQXmtNbjQ2ObQB+HzwuvK9zcconIHQFRgAXq+o0EbmDVs0DVVURyVgFEZEJ2IpiMoaqEtRIc0KX7lizYWtwvAvNMx03BZd3AE7CgslrwEK8xuDilU+ToT/wRpCoBRE5DAsIQymhJkMh9xDI1mcQ1g7Cc/TERiSGAzcAe+4Nty6wBBZLW5WlKrjuex24Qokqc1M9sFREwg/7UVh+k0lYxiYogcxN1RQ2oWsqw0/r5sAG7IP/PHA4wFi4rJ9t09b6DxI+rjrDba212C3auRzkNVNRRPbDUsF3AxZjuVG7AI8CuwN/A8ap6pq2nifuTsXwWziuc58EPHgqMBC+dru9ea3L03pkw7lctVVD8KnLCVANrN0J+DK8czd8BeuNbd2fEE6dhuRt3+ZKh09dTrgq4Hf/AL0b9usHuwEDgUHB71BYM+hGYZs5zoW8hpCmvaZDFB17NdiMrnA69FosQ+4PgSOC++yW4XF9sdGKjQUsi6sM3mTooJ7Yh6zYbfTuNK+TCEcU+gLvAjU6gt4yc5smQjUtRy6c6yhvMnRQXHMAGmg5EpHC+hCmAbw1k8ew5kM6n8jkouABIU3SPmTPAHwVRv0Q9sWHFF30PCAk2F3Al+YD33uZo8GTzbrIeUBIgGqyf/uvBPj6kfzHp+FHZJ985JOSXCF4QEi4j4HfTQSuhPN72B8sSc0aV148ICTAVuwPkWluwSZsjQN1wMhtOxedKyQPCAkQDjV2y3J7Pdje791sR6ZMTQNP4+YKwechJEQ4LbmKzFOWRwB/2Rd4AeoG2EYUPn3Z5cLnIZSAcFVkI9vWAFLYMtIb5wBDYWEP+B9gf+LrTPQOzPLkASFhslX7NwKPA9/dBO9vgstOhQvbeYxzneVNhhJUA6zSUbD/ZOpmWO5Z5zrKmwxlZiPAfpNhCCz07DCugDwglKjRs7AVUK/YoiznCsGbDAlXhU1Zbr0KszvwAvBv44DFMGiGjTw4157Imgwi8m0RmSsic0TkIRHpLiKDRWSaiCwSkUdEJNvwuuuAFJknLDUApwIPPgpMv5v+RS2VK1c5BwQR2RX4FjBSVcPFeGcANwK3qepQ4J/ABYUoaCVbR+aRhNWAbVY5hu74UKDLX759CF2B7UWkK1azXQF8CRshA7gPOCXPc1S8thY/PQisl134y0S4vY37OdcR+WzDvhy4Gfg7FgjWYdmb1qrqluBuy4Bd8y2ky241tkyao0tznUNN3AVwLeTTZNgRGIOldBuA/W2P68TjJ4jIDBGZEX+3ZrK1NfFoA5b1icED6Fuc4hRMFW3Xflzx5dNkOBpYoqqrVbUReAL4AtA7aEKA7Q+6PNODVXWiqo5U1ZHlksotKm3t5LQBeBWAWxlarAIVSAorv8+0TI58AsLfgYNEpIeICM2Zm14GvhzcJ/GZm8qBfcPeyvY9YGTMZeksDwbJkm/mpmuB04EtwNvA17A+g4eB2uDY2ar6SVvP4/MQ8lMFPAt84VJgHtS8EHOBIlaDbz+fD9+GvQLUAksfAIZCrwPL+5vXA0J+fC1DBVgH9tfsTcl1LnaWB4PoeA2hTFRhmXV3HA9Mgz4LmjdaiTOZrUseryFUgBQ2lfmD+4H3ulCHrXdozXdndm3xgFBGZhBMEX1qK/djOSMz7b5EhuPOgQeEspICngfeGQt7vWYTRcLjre/n2aNdJh4Qysw04DSAL7zNIW3cL648li7ZPCCUIduN+UfsAxwYb1FcifFRhjJUA6zaGxgK9X+2KaQfxlsklyA+ylBhNgL7LwB6Qf+vwKXYNmvekeja4zWEMlYDzAf66CH8u0xlKraYyFU2ryFUqI3AvwGXyFSe0N825XFwLhuvIZSJcBhxK9sOMw4AFo4E+sKrz8JZhFuvZeezG8uX1xAqSKY/6Gpg1AzgGjjscGtK+IxFl4kHhDLRiNUOtma5bSrYHmsDrTYRTk7yoODSeUAoI/tiWaKzea8fMANm72bX29qJqZRnM5ZquZPAA0IZ+QhYSvZv/cvCC0/Zwqf0YJDpMY3t3J5Uje3fxWXhAaGMrA1+spkG8DEwzLZaK/dvUu8n6bx2A4KI/FpEVonInLRjtSIyWUQWBr93DI6LiNwZZG16V0TaqsE2nyP38rs0jQQ9yFlubwA+/AfwPFwJ9E67rb0RhVIbcQh3dC73oFdoHakh/JZtt1e/ApiiqnXAlOA6wPFAXfAzgSBlQHviH/isHF8EfjQWvqg/Y/8s96ki814KpSSFBUBfxNU57QYEVX2FbYetx2BZmaBldqYxwP1q3sC2ZN+lvXN41a7wsr2fa4FFANxPLZkzR6ew2kbvVs9Tat+2VdjrGxRzOUpJrn0I/VR1RXC5HugXXN4V69cKdShzk9cQCi9bFb8RmA2wz5v88ni4vo3Ht+6cyzSkmWTha/A9GDsu705FtamOnf5Mp2duSlF6bdRSNg/otQB45mL+4/PZaxMbafl3KcW/UQM2Mct1TK4BYWXYFAh+rwqOLwcGpt3PMzcllH24p0CDVe/CDrjWwcGbcpUl14AwCcvKBC2zM00CxgejDQcB69KaFi5hTpN5MAIWvmhbt2daB1GKtQKXu3YXN4nIQ1jn9E7ASuD7wFPAo8Du2O7f41R1TZDS7WfYqMQm4DxVndFeIXxxUzyqsXTyJxwO9IThf7YOoNZ9B2EtwYNDefDMTS6rgcC1wOk6mjHyJ6ZhkTxF9h2bXWnz1Y4uq6VYJyNsYR+aJytVAT2wfxDv9K0cXkNw1GLpum9rhDeqrb23FetsXINP7ik3ia8hdMF7swuts+/nJoBj4K/Yfgk9sAk94eSlvsDZwfVwWrArP4kICD7sGI/0oJECpr9sU5YPoeWOSWEAGAB0o7AB3L8IkqVr3AUAb6NGoSPvZ3ifT4DF2ELIb2EZn44EZrW67wxsslIhZyx2p/2ZhL6dW/F4H0KJ6syHpCP3DZsAaw8FjoTvXw93YEOQrb/Fw2plIfYdqMGnFhdb4ocdq0V0e/xbIAnuAc4aDIyG/ndm3ra9J/bNXogpwf7tX3yJ71R0yfF74NUlwAC4HBgS8fk8GCRLIgKC4v8YcQubBX8FfglwL1w+Dg5j2yZDuBELGW5zpS0RAcHFL30X5ieBukXAI3czlub+hfDD30BzU8IDeXlJRECIvxfDQctdmFcDP5NvMGoifHxk9tWQxeC1kOJJREBwyRF++BqBm8HGI/u2TCtf7B2u9samVPtkqOglIiD4xKT4hR/y9ElHq4HNNwBT4RLsQxn+wxTyH6et4FKFLcAKs1d7bSFaiQgI3mSIX3pfQPhP0R1b8/7qMjhhjk1a6hPcVsjJSeHKykwf9hTwPLYIy9dURC8RAcElQ7gHYTjhKOxTOAv45r5wid7L2UQzs7Sjz+mdmNHygOCyCj98awiXSB/IDljNwWeWlicPCK5D1gP8czh1wFhsvYMrP7lmbvqJiLwXZGd6UkR6p912ZZC5aYGIHBtVwV1xLQL618LoWrj3ZHhkcNwlclHINXPTZGBfVf0s8D6WGQwRGQacAXwmeMwvRMQ7hstACutI3H8N1oZYPMCHActQTpmbVPUFVd0SXH0D224dLHPTw6r6iaouwb5YDihgeV2MGrAM00wF3v2Is4BhtMwR6UpbIfoQzgeeDS7nlLnJlYYUtlT51a3Ar2BfLCD0w5Yx12R5XPfgNq8qJl9eG6SIyFXAFuCBHB47AUsI6xOTSswfgQF32mShETQPVTYCm9l2jkLf4PcGmjdY8eHDZMo5IIjIucBJwFHavKlCpzI3ARPBNkjJtRyuuBqxiUL1wAnAHtgHvREbmuwZXN6KTTXugQWBbth+jOmTizwoJE9OAUFEjgO+AxyhqpvSbpoEPCgit2Jb8NUBb+ZdSpcoS7Es0nOwNucn2Ic+XAGZvu5gGDYFOtyT8Twss89iYCEWWAqx85IrjHYDQnrmJhFZhmVuuhLYDphsyZp4Q1W/oapzReRR7MtiC3CRqvoXQZnZCqwLfsIPfjgK0QVrNoRrIsLaA9jOzg1YLWIg1rcwAniJ5uQwLl6J2EKtq4huF3chXIeFy6Ab2TYghPsqhLMZwwVT3bBvkP5YTSHc2n00NjU6Uwo5F43Eb6EWf0hymWQbFdhKy/UOYZ9B+u0N2Lf+xzR3Jm4AZgMvBj8rgeGPwv5YgHDxS0RAcMmUrQqfauMnvL0RCwKNWCBYgwWAdcHlpVg/AsBPsQ5KFz8PCK4oWgeORqxTcfY4qHkWvh1n4VyTRPQheF6GylUNrNVucNhmxrxmTQkXrcT3IbjK1Qg8J5thX/jj1XGXxnlAcLG7AGxu83U9sk5/dsWRiNyOrrKtBT64BfZs3MSqfrDPSltEFeW8BM8YlZnXEFwi/Bj48E7gejidlvPfXfF4QHCJ8DjwPYCpcO3RcHLE5/PaQWY+yuASoxpbKz9Xb+ZjuZzd4y5QmfJRBlcSGgkzSi+nFpvi7IrLA4LLKo4NTRoA7rkN+RxcG1MZKpkHhDJViA9SHPkcU8DlE4CT4KyrbD8FDwrF4wGhTBWi06yBlglgi+UusPbCyfbLg0Lx+DwE12FV2JLmLtiKxo1Rnuw64EA4CAtM07HFUQ34CEGUvIbgOiVcmBT1h/LGlbB5EvziTLseR02lEnlAcB2Wwr6hw58oXQc8A/DgaXSned8FDwrRyilzU9pt/yUiKiI7BddFRO4MMje9KyIjoii0qwyvA/zgMX7Rz7Zac9HLNXMTIjIQOAb4e9rh47GNVeuwLdbvyr+ILm7ZUrVH7T5g9NVA/ZWMx6uzxZBT5qbAbdjOy+lTHccA96t5A+gtIrsUpKQuNlGkf++IDcA0gOE/ZvSp8B62lbuLTk5BV0TGAMtVdVarmzxzkyuojUDdHKAP9P8W3Bp3gcpcpwOCiPTA1qH8Tz4nFpEJIjJDRGbEv5rCJdlHQK+JwF1wmv7ek8xGKJcawp7AYGCWiHyIZWeaKSL96WTmJlUdqaojPZWba08KWN8IMI19yJ5H0uWn0wFBVWer6s6qOkhVB2HNghGqWo9lbhofjDYcBKxT1RWFLbKrVN8A1stPeeNpOBMPClHoyLDjQ9gI0N4iskxELmjj7s9gu2svAu4BvlmQUjqHJZk9EeDE0xiHpYlzheX7IbiSMgiY+xvgJvjZfPhuzOUpRW3th+ABwZWUauBY4JFFwLnQ57XoZ02Wm8RvkOKdismUxBWGjcBUsPHIAT6DsdASERCc64x1AKcADTD5U8kMXKUqEQEh/kaLyySpC4lSQN0SbILChiHsAD43oUASERCc66yPgNQMYMliLgf64DWFQvCA4ErWUcCDQ+CSx+F6YCSZg4IHio7zUQZXsrpjy2pfBrb/FLzzL3gSuDneYiWeDzu6srdxMLbf2qvQa1ly+z+SIPHDjs7la9cl2Nrov3s+h3x4QHBlYS2w389hchd4T2/3ac058oDgysZCLGksfS7lEGAVsCTWEpUeDwiurEwDeq2xf+yam2DnW6B33IUqIR4QKlQ5d+KmgIlgUxqHw5H40GNH+SiDK1t12AYdu+vVDJLrg0SyzkcZXEVais1RYP313I7vn9ARHhBc2WoAXgIYBadcAYfEXJ5S4E0GV9aqgSHATH2dyXIwp8RdoATwJoOrWI0Ey6XZyVdEdkDOqdxE5GIReU9E5orITWnHrwxSuS0QkWOjKLRznbEZgMsYBhwab1ESr90mg4gcDvwLy8i0b3DsSOAq4ERV/UREdlbVVSIyDHgIOACbQfoisJeqtjm13JsMLko1wKoDgH2g/n7Yi8pe65BXkyFLKrcLgRtU9ZPgPquC42OAh1X1E1Vdgu2+fECuBXeuEDYCO78JXA/9f+Pt5Lbk+t7sBRwmItNE5K8isn9wvMOp3DxzkyumjQA9gJ4+SaktXfN4XC224HR/4FERGdKZJ1DViQQTyqpEPCa46B0OHAYfV0OvxspuNmSTaw1hGfBEkOX5TWArsBOdSOXmSk9caeEL5Z75wGTgBhiKZ37KJNeA8BQ2RRwR2QvoBvwDmyl6hohsJyKDsdmjbxaioC5eVZR+2/vnwPQlQF+4Fuv1LuUAF4VcU7n9GhgSDEU+DJwT1BbmAo8C84DngIvaG2FwpSGFjemX8h9zIXA+cOt4GK0XchKl/Xqi4DMVXYdVUfofoGqs2jp9PLAYHnsNzo23SEXneyo6l6Yn8HXg2nnAmVAzK+YCFZlPXXYFVU1p76ewgWBn5gXAUDgH6I8newEPCK4NmT4gVVjvfG2RyxKFa8cCk+AX46yHvCfeyZiIJoOIrMbmjvwjxmLsFPP5vQzJOH8SyhD1+fdQ1b6ZbkhEQAAQkRmqOrJSz+9lSMb5k1CGOM/vTQbnXBMPCM65JkkKCBMr/PzgZUjC+SH+MsR2/sT0ITjn4pekGoJzLmaxBwQROS7Ybm2RiFxRpHMOFJGXRWResAXcJcHxa0RkuYi8E/ycEGEZPhSR2cF5ZgTHakVksogsDH7vGOH59057ne+IyHoRuTTq9yDTlnzZXreYO4P/jXdFZERE5/9JsB3guyLypIj0Do4PEpH/S3sv7s73/G2UIev7XtRtCVU1th9sHsgH2Ma43YBZwLAinHcXYERwuSfwPrZt/zXA5UV67R8CO7U6dhNwRXD5CuDGIv4d6oE9on4PsF0JRgBz2nvdwAnAs4Bge29Mi+j8xwBdg8s3pp1/UPr9In4PMr7vwf/lLGA7YHDweamK6u8Tdw3hAGCRqi5W1c3YyskxUZ9UVVeo6szg8gZgPll2diqyMcB9weX7oGi7hh8FfKCqf4v6RJp5S75sr3sMtpenquobQG8R2aXQ51fVF1R1S3D1DWwfj8hkeQ+yKeq2hHEHhA5vuRYVERkEfB7LEwrwn0HV8ddRVtkBBV4QkbdEZEJwrJ+qrggu1wP9Ijx/ujOwzXFDxXoPQtledxz/H+djtZLQYBF5O9gq8LCIz53pfS/qexB3QIiViHwK+ANwqaquB+4C9gT2A1YAt0R4+kNVdQRwPHBRsLt1E7X6YuRDQCLSDTgZeCw4VMz3YBvFet2ZiMhVwBbggeDQCmB3Vf08cBnwoIj0iuj0sb7vobgDQmxbrolINRYMHlDVJwBUdaWqplR1K3APEVbNVHV58HsV8GRwrpVhlTj4vSr7MxTM8cBMVV0ZlKdo70GabK+7aP8fInIucBLwlSAoEVTTPw4uv4W13/eK4vxtvO9F/YzEHRCmA3UiMjj4pjoD24YtUiIiwK+A+ap6a9rx9PbpWGBO68cW6Pw1ItIzvIx1as3BXvs5wd3OAf4YxflbOZO05kKx3oNWsr3uScD4YLThIGBdWtOiYETkOOA7wMmquinteF8RqQouD8H2Vllc6PMHz5/tfS/utoRR9VZ2osf1BKyX/wPgqiKd81CsWvou8E7wcwLwOxIVyIQAAACdSURBVGB2cHwSsEtE5x+C9RzPAuaGrxvoA0zBdvt6EaiN+H2oAT4Gdkg7Ful7gAWfFdiObMuAC7K9bmx04efB/8ZsYGRE51+EtdPD/4W7g/ueGvx93gFmAqMjfA+yvu9YUqQPsB0cjo/yf8JnKjrnmsTdZHDOJYgHBOdcEw8IzrkmHhCcc008IDjnmnhAcM418YDgnGviAcE51+T/AVqr+ZR/GKfuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 48\n",
            " batch Loss train: 0.05317401885986328\n",
            "i 6\n",
            "epoch 48\n",
            " batch Loss train: 0.058946117758750916\n",
            "i 7\n",
            "epoch 48\n",
            " batch Loss train: 0.05421118065714836\n",
            "i 8\n",
            "epoch 48\n",
            " batch Loss train: 0.046248674392700195\n",
            "i 9\n",
            "epoch 48\n",
            " batch Loss train: 0.04512946307659149\n",
            "i 10\n",
            "epoch 48\n",
            " batch Loss train: 0.053171075880527496\n",
            "i 11\n",
            "epoch 48\n",
            " batch Loss train: 0.0506550557911396\n",
            "i 12\n",
            "epoch 48\n",
            " batch Loss train: 0.04044535011053085\n",
            "i 13\n",
            "epoch 48\n",
            " batch Loss train: 0.03962096944451332\n",
            "i 14\n",
            "epoch 48\n",
            " batch Loss train: 0.05822792649269104\n",
            "i 15\n",
            "epoch 48\n",
            " batch Loss train: 0.04458010941743851\n",
            "i 16\n",
            "epoch 48\n",
            " batch Loss train: 0.04396837204694748\n",
            "i 17\n",
            "epoch 48\n",
            " batch Loss train: 0.04440827667713165\n",
            "i 18\n",
            "epoch 48\n",
            " batch Loss train: 0.05349106341600418\n",
            "i 19\n",
            "epoch 48\n",
            " batch Loss train: 0.045509256422519684\n",
            "i 20\n",
            "epoch 48\n",
            " batch Loss train: 0.05141369253396988\n",
            "i 21\n",
            "epoch 48\n",
            " batch Loss train: 0.056788865476846695\n",
            "i 22\n",
            "epoch 48\n",
            " batch Loss train: 0.043018266558647156\n",
            "i 23\n",
            "epoch 48\n",
            " batch Loss train: 0.04702959209680557\n",
            "i 24\n",
            "epoch 48\n",
            " batch Loss train: 0.0658252090215683\n",
            "i 25\n",
            "epoch 48\n",
            " batch Loss train: 0.0759163573384285\n",
            "i 26\n",
            "epoch 48\n",
            " batch Loss train: 0.048537563532590866\n",
            "i 27\n",
            "epoch 48\n",
            " batch Loss train: 0.04463840276002884\n",
            "i 28\n",
            "epoch 48\n",
            " batch Loss train: 0.04459245130419731\n",
            "i 29\n",
            "epoch 48\n",
            " batch Loss train: 0.04018060863018036\n",
            "i 30\n",
            "epoch 48\n",
            " batch Loss train: 0.04317766800522804\n",
            "i 31\n",
            "epoch 48\n",
            " batch Loss train: 0.046653103083372116\n",
            "i 32\n",
            "epoch 48\n",
            " batch Loss train: 0.04354843869805336\n",
            "i 33\n",
            "epoch 48\n",
            " batch Loss train: 0.04694173485040665\n",
            "i 34\n",
            "epoch 48\n",
            " batch Loss train: 0.048819296061992645\n",
            "i 35\n",
            "epoch 48\n",
            " batch Loss train: 0.048600565642118454\n",
            "i 36\n",
            "epoch 48\n",
            " batch Loss train: 0.048677463084459305\n",
            "i 37\n",
            "epoch 48\n",
            " batch Loss train: 0.0526404082775116\n",
            "i 38\n",
            "epoch 48\n",
            " batch Loss train: 0.057815372943878174\n",
            "i 39\n",
            "epoch 48\n",
            " batch Loss train: 0.041307233273983\n",
            "i 40\n",
            "epoch 48\n",
            " batch Loss train: 0.04464661329984665\n",
            "i 41\n",
            "epoch 48\n",
            " batch Loss train: 0.056254372000694275\n",
            "i 42\n",
            "epoch 48\n",
            " batch Loss train: 0.05124374106526375\n",
            "i 43\n",
            "epoch 48\n",
            " batch Loss train: 0.03675772249698639\n",
            "i 44\n",
            "epoch 48\n",
            " batch Loss train: 0.05301584675908089\n",
            "i 45\n",
            "epoch 48\n",
            " batch Loss train: 0.046966198831796646\n",
            "i 46\n",
            "epoch 48\n",
            " batch Loss train: 0.04764607548713684\n",
            "i 47\n",
            "epoch 48\n",
            " batch Loss train: 0.05169922113418579\n",
            "i 48\n",
            "epoch 48\n",
            " batch Loss train: 0.06886418163776398\n",
            "i 49\n",
            "epoch 48\n",
            " batch Loss train: 0.031338464468717575\n",
            "i 50\n",
            "epoch 48\n",
            " batch Loss train: 0.07028606534004211\n",
            "i 51\n",
            "epoch 48\n",
            " batch Loss train: 0.03992721438407898\n",
            "i 52\n",
            "epoch 48\n",
            " batch Loss train: 0.05638498812913895\n",
            "i 53\n",
            "epoch 48\n",
            " batch Loss train: 0.04157936945557594\n",
            "i 54\n",
            "epoch 48\n",
            " batch Loss train: 0.06175597012042999\n",
            "i 55\n",
            "epoch 48\n",
            " batch Loss train: 0.05043359845876694\n",
            "i 56\n",
            "epoch 48\n",
            " batch Loss train: 0.04790912941098213\n",
            "i 57\n",
            "epoch 48\n",
            " batch Loss train: 0.06890147179365158\n",
            "i 58\n",
            "epoch 48\n",
            " batch Loss train: 0.04729514569044113\n",
            "i 59\n",
            "epoch 48\n",
            " batch Loss train: 0.04785090684890747\n",
            "i 60\n",
            "epoch 48\n",
            " batch Loss train: 0.0710931271314621\n",
            "i 61\n",
            "epoch 48\n",
            " batch Loss train: 0.044522784650325775\n",
            "i 62\n",
            "epoch 48\n",
            " batch Loss train: 0.05598035827279091\n",
            "i 63\n",
            "epoch 48\n",
            " batch Loss train: 0.060209259390830994\n",
            "i 64\n",
            "epoch 48\n",
            " batch Loss train: 0.051944270730018616\n",
            "i 65\n",
            "epoch 48\n",
            " batch Loss train: 0.03859629109501839\n",
            "i 66\n",
            "epoch 48\n",
            " batch Loss train: 0.07355645298957825\n",
            "i 67\n",
            "epoch 48\n",
            " batch Loss train: 0.05342195928096771\n",
            "i 68\n",
            "epoch 48\n",
            " batch Loss train: 0.05199011042714119\n",
            "i 69\n",
            "epoch 48\n",
            " batch Loss train: 0.0534542016685009\n",
            "i 70\n",
            "epoch 48\n",
            " batch Loss train: 0.05244540050625801\n",
            "i 71\n",
            "epoch 48\n",
            " batch Loss train: 0.055229365825653076\n",
            "i 72\n",
            "epoch 48\n",
            " batch Loss train: 0.046496059745550156\n",
            "i 73\n",
            "epoch 48\n",
            " batch Loss train: 0.04586223140358925\n",
            "i 74\n",
            "epoch 48\n",
            " batch Loss train: 0.03484601527452469\n",
            "i 75\n",
            "epoch 48\n",
            " batch Loss train: 0.05850138142704964\n",
            "i 76\n",
            "epoch 48\n",
            " batch Loss train: 0.054701488465070724\n",
            "i 77\n",
            "epoch 48\n",
            " batch Loss train: 0.07130788266658783\n",
            "i 78\n",
            "epoch 48\n",
            " batch Loss train: 0.05933160334825516\n",
            "i 79\n",
            "epoch 48\n",
            " batch Loss train: 0.04557318240404129\n",
            "i 80\n",
            "epoch 48\n",
            " batch Loss train: 0.04557771980762482\n",
            "i 81\n",
            "epoch 48\n",
            " batch Loss train: 0.03433985635638237\n",
            "i 82\n",
            "epoch 48\n",
            " batch Loss train: 0.03803279250860214\n",
            "i 83\n",
            "epoch 48\n",
            " batch Loss train: 0.08428342640399933\n",
            "i 84\n",
            "epoch 48\n",
            " batch Loss train: 0.042483434081077576\n",
            "i 85\n",
            "epoch 48\n",
            " batch Loss train: 0.05172113701701164\n",
            "i 86\n",
            "epoch 48\n",
            " batch Loss train: 0.04349220544099808\n",
            "i 87\n",
            "epoch 48\n",
            " batch Loss train: 0.05815353989601135\n",
            "i 88\n",
            "epoch 48\n",
            " batch Loss train: 0.040660176426172256\n",
            "i 89\n",
            "epoch 48\n",
            " batch Loss train: 0.03799235820770264\n",
            "i 90\n",
            "epoch 48\n",
            " batch Loss train: 0.0488382913172245\n",
            "i 91\n",
            "epoch 48\n",
            " batch Loss train: 0.051906704902648926\n",
            "i 92\n",
            "epoch 48\n",
            " batch Loss train: 0.04639716073870659\n",
            "i 93\n",
            "epoch 48\n",
            " batch Loss train: 0.0762338861823082\n",
            "i 94\n",
            "epoch 48\n",
            " batch Loss train: 0.05764524266123772\n",
            "i 95\n",
            "epoch 48\n",
            " batch Loss train: 0.037115808576345444\n",
            "i 96\n",
            "epoch 48\n",
            " batch Loss train: 0.05051146820187569\n",
            "i 97\n",
            "epoch 48\n",
            " batch Loss train: 0.044281598180532455\n",
            "i 98\n",
            "epoch 48\n",
            " batch Loss train: 0.0500505194067955\n",
            "i 99\n",
            "epoch 48\n",
            " batch Loss train: 0.053323984146118164\n",
            "i 100\n",
            "epoch 48\n",
            " batch Loss train: 0.04778759554028511\n",
            "i 101\n",
            "epoch 48\n",
            " batch Loss train: 0.031069818884134293\n",
            "i 102\n",
            "epoch 48\n",
            " batch Loss train: 0.03821854665875435\n",
            "i 103\n",
            "epoch 48\n",
            " batch Loss train: 0.05944644659757614\n",
            "i 104\n",
            "epoch 48\n",
            " batch Loss train: 0.05353773757815361\n",
            "i 105\n",
            "epoch 48\n",
            " batch Loss train: 0.056942302733659744\n",
            "i 106\n",
            "epoch 48\n",
            " batch Loss train: 0.04611225426197052\n",
            "i 107\n",
            "epoch 48\n",
            " batch Loss train: 0.053651828318834305\n",
            "i 108\n",
            "epoch 48\n",
            " batch Loss train: 0.040404606610536575\n",
            "i 109\n",
            "epoch 48\n",
            " batch Loss train: 0.047361936420202255\n",
            "i 110\n",
            "epoch 48\n",
            " batch Loss train: 0.046623580157756805\n",
            "i 111\n",
            "epoch 48\n",
            " batch Loss train: 0.06474880874156952\n",
            "i 112\n",
            "epoch 48\n",
            " batch Loss train: 0.06350932270288467\n",
            "i 113\n",
            "epoch 48\n",
            " batch Loss train: 0.058005671948194504\n",
            "i 114\n",
            "epoch 48\n",
            " batch Loss train: 0.06757421791553497\n",
            "i 115\n",
            "epoch 48\n",
            " batch Loss train: 0.04398177191615105\n",
            "i 116\n",
            "epoch 48\n",
            " batch Loss train: 0.07025753706693649\n",
            "i 117\n",
            "epoch 48\n",
            " batch Loss train: 0.042737964540719986\n",
            "i 118\n",
            "epoch 48\n",
            " batch Loss train: 0.042987823486328125\n",
            "i 119\n",
            "epoch 48\n",
            " batch Loss train: 0.049792397767305374\n",
            "i 120\n",
            "epoch 48\n",
            " batch Loss train: 0.05008822679519653\n",
            "i 121\n",
            "epoch 48\n",
            " batch Loss train: 0.04999011382460594\n",
            "i 122\n",
            "epoch 48\n",
            " batch Loss train: 0.049787987023591995\n",
            "i 123\n",
            "epoch 48\n",
            " batch Loss train: 0.05684715136885643\n",
            "i 124\n",
            "epoch 48\n",
            " batch Loss train: 0.0684419721364975\n",
            "i 125\n",
            "epoch 48\n",
            " batch Loss train: 0.05118787661194801\n",
            "i 126\n",
            "epoch 48\n",
            " batch Loss train: 0.04973101243376732\n",
            "i 127\n",
            "epoch 48\n",
            " batch Loss train: 0.06309081614017487\n",
            "i 128\n",
            "epoch 48\n",
            " batch Loss train: 0.046832408756017685\n",
            "i 129\n",
            "epoch 48\n",
            " batch Loss train: 0.06144018471240997\n",
            "i 130\n",
            "epoch 48\n",
            " batch Loss train: 0.052427977323532104\n",
            "i 131\n",
            "epoch 48\n",
            " batch Loss train: 0.04379059374332428\n",
            "i 132\n",
            "epoch 48\n",
            " batch Loss train: 0.035919468849897385\n",
            "i 133\n",
            "epoch 48\n",
            " batch Loss train: 0.06516855210065842\n",
            "i 134\n",
            "epoch 48\n",
            " batch Loss train: 0.051010627299547195\n",
            "i 135\n",
            "epoch 48\n",
            " batch Loss train: 0.04406612738966942\n",
            "i 136\n",
            "epoch 48\n",
            " batch Loss train: 0.052377138286828995\n",
            "i 137\n",
            "epoch 48\n",
            " batch Loss train: 0.060625169426202774\n",
            "i 138\n",
            "epoch 48\n",
            " batch Loss train: 0.052710749208927155\n",
            "i 139\n",
            "epoch 48\n",
            " batch Loss train: 0.06219572201371193\n",
            "i 140\n",
            "epoch 48\n",
            " batch Loss train: 0.05739719048142433\n",
            "i 141\n",
            "epoch 48\n",
            " batch Loss train: 0.03934405744075775\n",
            "i 142\n",
            "epoch 48\n",
            " batch Loss train: 0.04094259813427925\n",
            "i 143\n",
            "epoch 48\n",
            " batch Loss train: 0.039583005011081696\n",
            "i 144\n",
            "epoch 48\n",
            " batch Loss train: 0.05297624319791794\n",
            "i 145\n",
            "epoch 48\n",
            " batch Loss train: 0.05114046111702919\n",
            "i 146\n",
            "epoch 48\n",
            " batch Loss train: 0.04992985352873802\n",
            "i 147\n",
            "epoch 48\n",
            " batch Loss train: 0.07150813937187195\n",
            "i 148\n",
            "epoch 48\n",
            " batch Loss train: 0.041072312742471695\n",
            "i 149\n",
            "epoch 48\n",
            " batch Loss train: 0.05472956597805023\n",
            "i 150\n",
            "epoch 48\n",
            " batch Loss train: 0.053488146513700485\n",
            "i 151\n",
            "epoch 48\n",
            " batch Loss train: 0.04480529576539993\n",
            "i 152\n",
            "epoch 48\n",
            " batch Loss train: 0.05774850770831108\n",
            "i 153\n",
            "epoch 48\n",
            " batch Loss train: 0.039682451635599136\n",
            "i 154\n",
            "epoch 48\n",
            " batch Loss train: 0.06166417896747589\n",
            "i 155\n",
            "epoch 48\n",
            " batch Loss train: 0.04471970722079277\n",
            "i 156\n",
            "epoch 48\n",
            " batch Loss train: 0.0722159743309021\n",
            "i 157\n",
            "epoch 48\n",
            " batch Loss train: 0.055173296481370926\n",
            "i 158\n",
            "epoch 48\n",
            " batch Loss train: 0.06806956976652145\n",
            "i 159\n",
            "epoch 48\n",
            " batch Loss train: 0.051065392792224884\n",
            "i 160\n",
            "epoch 48\n",
            " batch Loss train: 0.05206559970974922\n",
            "i 161\n",
            "epoch 48\n",
            " batch Loss train: 0.04345780983567238\n",
            "i 162\n",
            "epoch 48\n",
            " batch Loss train: 0.07605497539043427\n",
            "i 163\n",
            "epoch 48\n",
            " batch Loss train: 0.03423045203089714\n",
            "i 164\n",
            "epoch 48\n",
            " batch Loss train: 0.06524837017059326\n",
            "i 165\n",
            "epoch 48\n",
            " batch Loss train: 0.06608197838068008\n",
            "i 166\n",
            "epoch 48\n",
            " batch Loss train: 0.05603181943297386\n",
            "i 167\n",
            "epoch 48\n",
            " batch Loss train: 0.05209215730428696\n",
            "i 168\n",
            "epoch 48\n",
            " batch Loss train: 0.04972181096673012\n",
            "i 169\n",
            "epoch 48\n",
            " batch Loss train: 0.04074564576148987\n",
            "i 170\n",
            "epoch 48\n",
            " batch Loss train: 0.04807714372873306\n",
            "i 171\n",
            "epoch 48\n",
            " batch Loss train: 0.041622430086135864\n",
            "i 172\n",
            "epoch 48\n",
            " batch Loss train: 0.07065381854772568\n",
            "i 173\n",
            "epoch 48\n",
            " batch Loss train: 0.06024150550365448\n",
            "i 174\n",
            "epoch 48\n",
            " batch Loss train: 0.05445275828242302\n",
            "i 175\n",
            "epoch 48\n",
            " batch Loss train: 0.05782793089747429\n",
            "i 176\n",
            "epoch 48\n",
            " batch Loss train: 0.05000106617808342\n",
            "i 177\n",
            "epoch 48\n",
            " batch Loss train: 0.053029146045446396\n",
            "i 178\n",
            "epoch 48\n",
            " batch Loss train: 0.048408642411231995\n",
            "i 179\n",
            "epoch 48\n",
            " batch Loss train: 0.054958492517471313\n",
            "i 180\n",
            "epoch 48\n",
            " batch Loss train: 0.04810607060790062\n",
            "i 181\n",
            "epoch 48\n",
            " batch Loss train: 0.04764812812209129\n",
            "i 182\n",
            "epoch 48\n",
            " batch Loss train: 0.04829028993844986\n",
            "i 183\n",
            "epoch 48\n",
            " batch Loss train: 0.04348329082131386\n",
            "i 184\n",
            "epoch 48\n",
            " batch Loss train: 0.035605289041996\n",
            "i 185\n",
            "epoch 48\n",
            " batch Loss train: 0.057908426970243454\n",
            "i 186\n",
            "epoch 48\n",
            " batch Loss train: 0.05867152288556099\n",
            "i 187\n",
            "epoch 48\n",
            " batch Loss train: 0.05937616154551506\n",
            "i 188\n",
            "epoch 48\n",
            " batch Loss train: 0.052943356335163116\n",
            "i 189\n",
            "epoch 48\n",
            " batch Loss train: 0.05758107453584671\n",
            "i 190\n",
            "epoch 48\n",
            " batch Loss train: 0.03796255588531494\n",
            "i 191\n",
            "epoch 48\n",
            " batch Loss train: 0.043502263724803925\n",
            "i 192\n",
            "epoch 48\n",
            " batch Loss train: 0.06340809166431427\n",
            "i 193\n",
            "epoch 48\n",
            " batch Loss train: 0.04772024601697922\n",
            "i 194\n",
            "epoch 48\n",
            " batch Loss train: 0.053665950894355774\n",
            "i 195\n",
            "epoch 48\n",
            " batch Loss train: 0.04988136142492294\n",
            "i 196\n",
            "epoch 48\n",
            " batch Loss train: 0.04811888560652733\n",
            "i 197\n",
            "epoch 48\n",
            " batch Loss train: 0.04874414578080177\n",
            "i 198\n",
            "epoch 48\n",
            " batch Loss train: 0.04240568354725838\n",
            "i 199\n",
            "epoch 48\n",
            " batch Loss train: 0.09077692031860352\n",
            "i 200\n",
            "epoch 48\n",
            " batch Loss train: 0.049650948494672775\n",
            "i 201\n",
            "epoch 48\n",
            " batch Loss train: 0.0412050262093544\n",
            "i 202\n",
            "epoch 48\n",
            " batch Loss train: 0.05292174220085144\n",
            "i 203\n",
            "epoch 48\n",
            " batch Loss train: 0.05145850032567978\n",
            "i 204\n",
            "epoch 48\n",
            " batch Loss train: 0.06776704639196396\n",
            "i 205\n",
            "epoch 48\n",
            " batch Loss train: 0.04025891795754433\n",
            "i 206\n",
            "epoch 48\n",
            " batch Loss train: 0.046186164021492004\n",
            "i 207\n",
            "epoch 48\n",
            " batch Loss train: 0.07185419648885727\n",
            "i 208\n",
            "epoch 48\n",
            " batch Loss train: 0.06684624403715134\n",
            "i 209\n",
            "epoch 48\n",
            " batch Loss train: 0.0536675862967968\n",
            "i 210\n",
            "epoch 48\n",
            " batch Loss train: 0.04844881594181061\n",
            "i 211\n",
            "epoch 48\n",
            " batch Loss train: 0.05547589436173439\n",
            "i 212\n",
            "epoch 48\n",
            " batch Loss train: 0.04799368605017662\n",
            "i 213\n",
            "epoch 48\n",
            " batch Loss train: 0.058312658220529556\n",
            "i 214\n",
            "epoch 48\n",
            " batch Loss train: 0.04764723777770996\n",
            "i 215\n",
            "epoch 48\n",
            " batch Loss train: 0.04155055060982704\n",
            "i 216\n",
            "epoch 48\n",
            " batch Loss train: 0.04471435770392418\n",
            "i 217\n",
            "epoch 48\n",
            " batch Loss train: 0.0643228143453598\n",
            "i 218\n",
            "epoch 48\n",
            " batch Loss train: 0.06298381835222244\n",
            "i 219\n",
            "epoch 48\n",
            " batch Loss train: 0.06719023734331131\n",
            "i 220\n",
            "epoch 48\n",
            " batch Loss train: 0.05046064034104347\n",
            "i 221\n",
            "epoch 48\n",
            " batch Loss train: 0.06585930287837982\n",
            "i 222\n",
            "epoch 48\n",
            " batch Loss train: 0.04172120988368988\n",
            "i 223\n",
            "epoch 48\n",
            " batch Loss train: 0.05086277425289154\n",
            "i 224\n",
            "epoch 48\n",
            " batch Loss train: 0.04347655549645424\n",
            "i 225\n",
            "epoch 48\n",
            " batch Loss train: 0.05787844583392143\n",
            "i 226\n",
            "epoch 48\n",
            " batch Loss train: 0.07470450550317764\n",
            "i 227\n",
            "epoch 48\n",
            " batch Loss train: 0.05196881294250488\n",
            "i 228\n",
            "epoch 48\n",
            " batch Loss train: 0.05448427051305771\n",
            "i 229\n",
            "epoch 48\n",
            " batch Loss train: 0.061509985476732254\n",
            "i 230\n",
            "epoch 48\n",
            " batch Loss train: 0.05801243707537651\n",
            "i 231\n",
            "epoch 48\n",
            " batch Loss train: 0.04615584388375282\n",
            "i 232\n",
            "epoch 48\n",
            " batch Loss train: 0.05199793726205826\n",
            "i 233\n",
            "epoch 48\n",
            " batch Loss train: 0.04311216622591019\n",
            "i 234\n",
            "epoch 48\n",
            " batch Loss train: 0.04322587698698044\n",
            "i 235\n",
            "epoch 48\n",
            " batch Loss train: 0.037216611206531525\n",
            "i 236\n",
            "epoch 48\n",
            " batch Loss train: 0.06824768334627151\n",
            "i 237\n",
            "epoch 48\n",
            " batch Loss train: 0.06711079180240631\n",
            "i 238\n",
            "epoch 48\n",
            " batch Loss train: 0.04769997298717499\n",
            "i 239\n",
            "epoch 48\n",
            " batch Loss train: 0.05539405718445778\n",
            "i 240\n",
            "epoch 48\n",
            " batch Loss train: 0.06162030249834061\n",
            "i 241\n",
            "epoch 48\n",
            " batch Loss train: 0.050328437238931656\n",
            "i 242\n",
            "epoch 48\n",
            " batch Loss train: 0.05608982592821121\n",
            "i 243\n",
            "epoch 48\n",
            " batch Loss train: 0.04646436497569084\n",
            "i 244\n",
            "epoch 48\n",
            " batch Loss train: 0.04329202324151993\n",
            "i 245\n",
            "epoch 48\n",
            " batch Loss train: 0.0562087707221508\n",
            "i 246\n",
            "epoch 48\n",
            " batch Loss train: 0.04449940845370293\n",
            "i 247\n",
            "epoch 48\n",
            " batch Loss train: 0.07051460444927216\n",
            "i 248\n",
            "epoch 48\n",
            " batch Loss train: 0.06215352565050125\n",
            "i 249\n",
            "epoch 48\n",
            " batch Loss train: 0.05702853202819824\n",
            "i 250\n",
            "epoch 48\n",
            " batch Loss train: 0.05434716120362282\n",
            "i 251\n",
            "epoch 48\n",
            " batch Loss train: 0.06880572438240051\n",
            "i 252\n",
            "epoch 48\n",
            " batch Loss train: 0.05436122789978981\n",
            "i 253\n",
            "epoch 48\n",
            " batch Loss train: 0.04136192426085472\n",
            "i 254\n",
            "epoch 48\n",
            " batch Loss train: 0.059087228029966354\n",
            "i 255\n",
            "epoch 48\n",
            " batch Loss train: 0.06975608319044113\n",
            "i 256\n",
            "epoch 48\n",
            " batch Loss train: 0.06136579066514969\n",
            "i 257\n",
            "epoch 48\n",
            " batch Loss train: 0.07105185091495514\n",
            "i 258\n",
            "epoch 48\n",
            " batch Loss train: 0.06418966501951218\n",
            "i 259\n",
            "epoch 48\n",
            " batch Loss train: 0.05119867995381355\n",
            "i 260\n",
            "epoch 48\n",
            " batch Loss train: 0.07058899104595184\n",
            "i 261\n",
            "epoch 48\n",
            " batch Loss train: 0.05154147744178772\n",
            "i 262\n",
            "epoch 48\n",
            " batch Loss train: 0.03923565894365311\n",
            "i 263\n",
            "epoch 48\n",
            " batch Loss train: 0.044379882514476776\n",
            "i 264\n",
            "epoch 48\n",
            " batch Loss train: 0.06833959370851517\n",
            "i 265\n",
            "epoch 48\n",
            " batch Loss train: 0.062352098524570465\n",
            "i 266\n",
            "epoch 48\n",
            " batch Loss train: 0.061959777027368546\n",
            "i 267\n",
            "epoch 48\n",
            " batch Loss train: 0.07373695820569992\n",
            "i 268\n",
            "epoch 48\n",
            " batch Loss train: 0.05901886522769928\n",
            "i 269\n",
            "epoch 48\n",
            " batch Loss train: 0.04715175926685333\n",
            "i 270\n",
            "epoch 48\n",
            " batch Loss train: 0.06794766336679459\n",
            "i 271\n",
            "epoch 48\n",
            " batch Loss train: 0.06978396326303482\n",
            "i 272\n",
            "epoch 48\n",
            " batch Loss train: 0.050131671130657196\n",
            "i 273\n",
            "epoch 48\n",
            " batch Loss train: 0.060277752578258514\n",
            "i 274\n",
            "epoch 48\n",
            " batch Loss train: 0.05439215898513794\n",
            "i 275\n",
            "epoch 48\n",
            " batch Loss train: 0.06190904229879379\n",
            "i 276\n",
            "epoch 48\n",
            " batch Loss train: 0.04652237892150879\n",
            "i 277\n",
            "epoch 48\n",
            " batch Loss train: 0.060783013701438904\n",
            "i 278\n",
            "epoch 48\n",
            " batch Loss train: 0.048359792679548264\n",
            "i 279\n",
            "epoch 48\n",
            " batch Loss train: 0.058354660868644714\n",
            "i 280\n",
            "epoch 48\n",
            " batch Loss train: 0.06354698538780212\n",
            "i 281\n",
            "epoch 48\n",
            " batch Loss train: 0.05219003185629845\n",
            "i 282\n",
            "epoch 48\n",
            " batch Loss train: 0.0827137753367424\n",
            "i 283\n",
            "epoch 48\n",
            " batch Loss train: 0.054765745997428894\n",
            "i 284\n",
            "epoch 48\n",
            " batch Loss train: 0.07041686773300171\n",
            "i 285\n",
            "epoch 48\n",
            " batch Loss train: 0.03825288265943527\n",
            "i 286\n",
            "epoch 48\n",
            " batch Loss train: 0.07168011367321014\n",
            "i 287\n",
            "epoch 48\n",
            " batch Loss train: 0.05686851218342781\n",
            "i 288\n",
            "epoch 48\n",
            " batch Loss train: 0.05477152764797211\n",
            "i 289\n",
            "epoch 48\n",
            " batch Loss train: 0.05714362859725952\n",
            "i 290\n",
            "epoch 48\n",
            " batch Loss train: 0.05465986207127571\n",
            "i 291\n",
            "epoch 48\n",
            " batch Loss train: 0.06603075563907623\n",
            "i 292\n",
            "epoch 48\n",
            " batch Loss train: 0.05190632492303848\n",
            "i 293\n",
            "epoch 48\n",
            " batch Loss train: 0.048700153827667236\n",
            "i 294\n",
            "epoch 48\n",
            " batch Loss train: 0.0415872298181057\n",
            "i 295\n",
            "epoch 48\n",
            " batch Loss train: 0.05626603588461876\n",
            "i 296\n",
            "epoch 48\n",
            " batch Loss train: 0.04234524071216583\n",
            "i 297\n",
            "epoch 48\n",
            " batch Loss train: 0.06009829789400101\n",
            "i 298\n",
            "epoch 48\n",
            " batch Loss train: 0.04764691740274429\n",
            "i 299\n",
            "epoch 48\n",
            " batch Loss train: 0.057180482894182205\n",
            "i 300\n",
            "epoch 48\n",
            " batch Loss train: 0.05929958447813988\n",
            "i 301\n",
            "epoch 48\n",
            " batch Loss train: 0.09164565056562424\n",
            "i 302\n",
            "epoch 48\n",
            " batch Loss train: 0.044011376798152924\n",
            "i 303\n",
            "epoch 48\n",
            " batch Loss train: 0.05634124577045441\n",
            "i 304\n",
            "epoch 48\n",
            " batch Loss train: 0.0775253102183342\n",
            "i 305\n",
            "epoch 48\n",
            " batch Loss train: 0.054482828825712204\n",
            "i 306\n",
            "epoch 48\n",
            " batch Loss train: 0.04624180495738983\n",
            "i 307\n",
            "epoch 48\n",
            " batch Loss train: 0.05297185853123665\n",
            "i 308\n",
            "epoch 48\n",
            " batch Loss train: 0.072662353515625\n",
            "i 309\n",
            "epoch 48\n",
            " batch Loss train: 0.05918968468904495\n",
            "i 310\n",
            "epoch 48\n",
            " batch Loss train: 0.07484995573759079\n",
            "i 311\n",
            "epoch 48\n",
            " batch Loss train: 0.049578744918107986\n",
            "i 312\n",
            "epoch 48\n",
            " batch Loss train: 0.05405186489224434\n",
            "i 313\n",
            "epoch 48\n",
            " batch Loss train: 0.06032618135213852\n",
            "i 314\n",
            "epoch 48\n",
            " batch Loss train: 0.05966757982969284\n",
            "i 315\n",
            "epoch 48\n",
            " batch Loss train: 0.04848688095808029\n",
            "i 316\n",
            "epoch 48\n",
            " batch Loss train: 0.056348759680986404\n",
            "i 317\n",
            "epoch 48\n",
            " batch Loss train: 0.06567644327878952\n",
            "i 318\n",
            "epoch 48\n",
            " batch Loss train: 0.05335836857557297\n",
            "i 319\n",
            "epoch 48\n",
            " batch Loss train: 0.06541413068771362\n",
            "i 320\n",
            "epoch 48\n",
            " batch Loss train: 0.046063560992479324\n",
            "i 321\n",
            "epoch 48\n",
            " batch Loss train: 0.062652587890625\n",
            "i 322\n",
            "epoch 48\n",
            " batch Loss train: 0.05064992606639862\n",
            "i 323\n",
            "epoch 48\n",
            " batch Loss train: 0.04997203126549721\n",
            "i 324\n",
            "epoch 48\n",
            " batch Loss train: 0.05025460571050644\n",
            "i 325\n",
            "epoch 48\n",
            " batch Loss train: 0.05644237622618675\n",
            "i 326\n",
            "epoch 48\n",
            " batch Loss train: 0.042544081807136536\n",
            "i 327\n",
            "epoch 48\n",
            " batch Loss train: 0.043124061077833176\n",
            "i 328\n",
            "epoch 48\n",
            " batch Loss train: 0.055112771689891815\n",
            "i 329\n",
            "epoch 48\n",
            " batch Loss train: 0.05389770492911339\n",
            "i 330\n",
            "epoch 48\n",
            " batch Loss train: 0.05944808945059776\n",
            "i 331\n",
            "epoch 48\n",
            " batch Loss train: 0.05987075716257095\n",
            "i 332\n",
            "epoch 48\n",
            " batch Loss train: 0.05227892845869064\n",
            "i 333\n",
            "epoch 48\n",
            " batch Loss train: 0.03886539861559868\n",
            "i 334\n",
            "epoch 48\n",
            " batch Loss train: 0.05172106623649597\n",
            "i 335\n",
            "epoch 48\n",
            " batch Loss train: 0.03555057570338249\n",
            "i 336\n",
            "epoch 48\n",
            " batch Loss train: 0.046991754323244095\n",
            "i 337\n",
            "epoch 48\n",
            " batch Loss train: 0.059676043689250946\n",
            "i 338\n",
            "epoch 48\n",
            " batch Loss train: 0.0754447653889656\n",
            "i 339\n",
            "epoch 48\n",
            " batch Loss train: 0.06745865195989609\n",
            "i 340\n",
            "epoch 48\n",
            " batch Loss train: 0.08182068914175034\n",
            "i 341\n",
            "epoch 48\n",
            " batch Loss train: 0.04895566403865814\n",
            "i 342\n",
            "epoch 48\n",
            " batch Loss train: 0.05028695985674858\n",
            "i 343\n",
            "epoch 48\n",
            " batch Loss train: 0.045193176716566086\n",
            "i 344\n",
            "epoch 48\n",
            " batch Loss train: 0.05475557595491409\n",
            "i 345\n",
            "epoch 48\n",
            " batch Loss train: 0.05687473341822624\n",
            "i 346\n",
            "epoch 48\n",
            " batch Loss train: 0.06037653237581253\n",
            "i 347\n",
            "epoch 48\n",
            " batch Loss train: 0.05303121358156204\n",
            "i 348\n",
            "epoch 48\n",
            " batch Loss train: 0.0661395713686943\n",
            "i 349\n",
            "epoch 48\n",
            " batch Loss train: 0.04871797561645508\n",
            "i 350\n",
            "epoch 48\n",
            " batch Loss train: 0.06208635866641998\n",
            "i 351\n",
            "epoch 48\n",
            " batch Loss train: 0.052316006273031235\n",
            "i 352\n",
            "epoch 48\n",
            " batch Loss train: 0.06700719147920609\n",
            "i 353\n",
            "epoch 48\n",
            " batch Loss train: 0.06006957218050957\n",
            "i 354\n",
            "epoch 48\n",
            " batch Loss train: 0.04792220890522003\n",
            "i 355\n",
            "epoch 48\n",
            " batch Loss train: 0.0660053938627243\n",
            "i 356\n",
            "epoch 48\n",
            " batch Loss train: 0.04389661177992821\n",
            "i 357\n",
            "epoch 48\n",
            " batch Loss train: 0.053341321647167206\n",
            "i 358\n",
            "epoch 48\n",
            " batch Loss train: 0.05144203454256058\n",
            "i 359\n",
            "epoch 48\n",
            " batch Loss train: 0.061549052596092224\n",
            "i 360\n",
            "epoch 48\n",
            " batch Loss train: 0.06738676130771637\n",
            "i 361\n",
            "epoch 48\n",
            " batch Loss train: 0.058707915246486664\n",
            "i 362\n",
            "epoch 48\n",
            " batch Loss train: 0.0478324294090271\n",
            "i 363\n",
            "epoch 48\n",
            " batch Loss train: 0.09321550279855728\n",
            "i 364\n",
            "epoch 48\n",
            " batch Loss train: 0.05949993431568146\n",
            "i 365\n",
            "epoch 48\n",
            " batch Loss train: 0.04402900114655495\n",
            "i 366\n",
            "epoch 48\n",
            " batch Loss train: 0.06196652352809906\n",
            "i 367\n",
            "epoch 48\n",
            " batch Loss train: 0.09426027536392212\n",
            "i 368\n",
            "epoch 48\n",
            " batch Loss train: 0.06185769662261009\n",
            "i 369\n",
            "epoch 48\n",
            " batch Loss train: 0.042965635657310486\n",
            "i 370\n",
            "epoch 48\n",
            " batch Loss train: 0.04172036051750183\n",
            "i 371\n",
            "epoch 48\n",
            " batch Loss train: 0.06329508870840073\n",
            "i 372\n",
            "epoch 48\n",
            " batch Loss train: 0.06794173270463943\n",
            "i 373\n",
            "epoch 48\n",
            " batch Loss train: 0.04753819853067398\n",
            "i 374\n",
            "epoch 48\n",
            " batch Loss train: 0.05924521014094353\n",
            "i 375\n",
            "epoch 48\n",
            " batch Loss train: 0.042251329869031906\n",
            "i 376\n",
            "epoch 48\n",
            " batch Loss train: 0.049597810953855515\n",
            "i 377\n",
            "epoch 48\n",
            " batch Loss train: 0.04576350003480911\n",
            "i 378\n",
            "epoch 48\n",
            " batch Loss train: 0.04891064763069153\n",
            "i 379\n",
            "epoch 48\n",
            " batch Loss train: 0.06630878150463104\n",
            "i 380\n",
            "epoch 48\n",
            " batch Loss train: 0.04287669435143471\n",
            "i 381\n",
            "epoch 48\n",
            " batch Loss train: 0.04354679957032204\n",
            "i 382\n",
            "epoch 48\n",
            " batch Loss train: 0.04361475259065628\n",
            "i 383\n",
            "epoch 48\n",
            " batch Loss train: 0.04832390695810318\n",
            "i 384\n",
            "epoch 48\n",
            " batch Loss train: 0.044680409133434296\n",
            "i 385\n",
            "epoch 48\n",
            " batch Loss train: 0.057768139988183975\n",
            "i 386\n",
            "epoch 48\n",
            " batch Loss train: 0.05370363965630531\n",
            "i 387\n",
            "epoch 48\n",
            " batch Loss train: 0.07661163806915283\n",
            "i 388\n",
            "epoch 48\n",
            " batch Loss train: 0.06340877711772919\n",
            "i 389\n",
            "epoch 48\n",
            " batch Loss train: 0.06649437546730042\n",
            "i 390\n",
            "epoch 48\n",
            " batch Loss train: 0.04809883236885071\n",
            "i 391\n",
            "epoch 48\n",
            " batch Loss train: 0.06789310276508331\n",
            "i 392\n",
            "epoch 48\n",
            " batch Loss train: 0.06868162006139755\n",
            "i 393\n",
            "epoch 48\n",
            " batch Loss train: 0.05678100138902664\n",
            "i 394\n",
            "epoch 48\n",
            " batch Loss train: 0.041238170117139816\n",
            "i 395\n",
            "epoch 48\n",
            " batch Loss train: 0.07046595215797424\n",
            "i 396\n",
            "epoch 48\n",
            " batch Loss train: 0.0729171484708786\n",
            "i 397\n",
            "epoch 48\n",
            " batch Loss train: 0.054542042315006256\n",
            "i 398\n",
            "epoch 48\n",
            " batch Loss train: 0.05058809369802475\n",
            "i 399\n",
            "epoch 48\n",
            " batch Loss train: 0.05291835963726044\n",
            "i 400\n",
            "epoch 48\n",
            " batch Loss train: 0.051334355026483536\n",
            "i 401\n",
            "epoch 48\n",
            " batch Loss train: 0.060460057109594345\n",
            "i 402\n",
            "epoch 48\n",
            " batch Loss train: 0.047363027930259705\n",
            "i 403\n",
            "epoch 48\n",
            " batch Loss train: 0.05896683782339096\n",
            "i 404\n",
            "epoch 48\n",
            " batch Loss train: 0.05900513380765915\n",
            "i 405\n",
            "epoch 48\n",
            " batch Loss train: 0.04397355392575264\n",
            "i 406\n",
            "epoch 48\n",
            " batch Loss train: 0.060884278267621994\n",
            "i 407\n",
            "epoch 48\n",
            " batch Loss train: 0.06123476102948189\n",
            "i 408\n",
            "epoch 48\n",
            " batch Loss train: 0.08245383203029633\n",
            "i 409\n",
            "epoch 48\n",
            " batch Loss train: 0.06437472254037857\n",
            "i 410\n",
            "epoch 48\n",
            " batch Loss train: 0.05607964098453522\n",
            "i 411\n",
            "epoch 48\n",
            " batch Loss train: 0.0630706399679184\n",
            "i 412\n",
            "epoch 48\n",
            " batch Loss train: 0.0679645761847496\n",
            "i 413\n",
            "epoch 48\n",
            " batch Loss train: 0.04904298484325409\n",
            "i 414\n",
            "epoch 48\n",
            " batch Loss train: 0.05188857391476631\n",
            "i 415\n",
            "epoch 48\n",
            " batch Loss train: 0.07355077564716339\n",
            "i 416\n",
            "epoch 48\n",
            " batch Loss train: 0.06006540358066559\n",
            "i 417\n",
            "epoch 48\n",
            " batch Loss train: 0.06599360704421997\n",
            "i 418\n",
            "epoch 48\n",
            " batch Loss train: 0.05700037628412247\n",
            "i 419\n",
            "epoch 48\n",
            " batch Loss train: 0.05225032940506935\n",
            "i 420\n",
            "epoch 48\n",
            " batch Loss train: 0.07128696888685226\n",
            "i 421\n",
            "epoch 48\n",
            " batch Loss train: 0.06854390352964401\n",
            "i 422\n",
            "epoch 48\n",
            " batch Loss train: 0.055518392473459244\n",
            "i 423\n",
            "epoch 48\n",
            " batch Loss train: 0.06277567148208618\n",
            "i 424\n",
            "epoch 48\n",
            " batch Loss train: 0.06077033281326294\n",
            "i 425\n",
            "epoch 48\n",
            " batch Loss train: 0.044573135673999786\n",
            "i 426\n",
            "epoch 48\n",
            " batch Loss train: 0.04411890357732773\n",
            "i 427\n",
            "epoch 48\n",
            " batch Loss train: 0.053844574838876724\n",
            "i 428\n",
            "epoch 48\n",
            " batch Loss train: 0.04600889980792999\n",
            "i 429\n",
            "epoch 48\n",
            " batch Loss train: 0.07704660296440125\n",
            "i 430\n",
            "epoch 48\n",
            " batch Loss train: 0.056079041212797165\n",
            "i 431\n",
            "epoch 48\n",
            " batch Loss train: 0.045389674603939056\n",
            "i 432\n",
            "epoch 48\n",
            " batch Loss train: 0.0450906939804554\n",
            "i 433\n",
            "epoch 48\n",
            " batch Loss train: 0.06783147156238556\n",
            "i 434\n",
            "epoch 48\n",
            " batch Loss train: 0.06953505426645279\n",
            "i 435\n",
            "epoch 48\n",
            " batch Loss train: 0.05731222778558731\n",
            "i 436\n",
            "epoch 48\n",
            " batch Loss train: 0.06319625675678253\n",
            "i 437\n",
            "epoch 48\n",
            " batch Loss train: 0.07117541134357452\n",
            "i 438\n",
            "epoch 48\n",
            " batch Loss train: 0.07532725483179092\n",
            "i 439\n",
            "epoch 48\n",
            " batch Loss train: 0.04806022718548775\n",
            "i 440\n",
            "epoch 48\n",
            " batch Loss train: 0.0482727475464344\n",
            "i 441\n",
            "epoch 48\n",
            " batch Loss train: 0.0532875657081604\n",
            "i 442\n",
            "epoch 48\n",
            " batch Loss train: 0.04912731051445007\n",
            "i 443\n",
            "epoch 48\n",
            " batch Loss train: 0.04996868968009949\n",
            "i 444\n",
            "epoch 48\n",
            " batch Loss train: 0.07555366307497025\n",
            "i 445\n",
            "epoch 48\n",
            " batch Loss train: 0.05126146972179413\n",
            "total epoch Loss train: tensor(0.0513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 49\n",
            " batch Loss train: 0.04848863556981087\n",
            "i 1\n",
            "epoch 49\n",
            " batch Loss train: 0.04245660454034805\n",
            "i 2\n",
            "epoch 49\n",
            " batch Loss train: 0.05289763957262039\n",
            "i 3\n",
            "epoch 49\n",
            " batch Loss train: 0.0438309982419014\n",
            "i 4\n",
            "epoch 49\n",
            " batch Loss train: 0.05551380664110184\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADPCAYAAADs8oorAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeqklEQVR4nO3df5ScZX338fc3k92ExJBlQxLID0kiERq1BJ6lhAAKiPKjQMCiELVGxPI8PY+KQEXQc+qx1la0Ktj2WChIY0EKjaEEHjGGgDxWMSUEwo+QmLAESJb8IEvCmm3IMnv1j+99s5Nhf8zuzsx93zOf1zlz5vfMxR3me1/7vb7XdVkIARERyZ4RSTdARESGRgFcRCSjFMBFRDJKAVxEJKMUwEVEMkoBXEQko4YVwM3sLDPbYGabzOzacjVKREQGZkOtAzezHPA74EPAFuAxYGEIYV35miciIn0ZOYz3/hGwKYTQCmBm/wYsAPoM4O8wC4cCY4Bc9OX7gdboOv5zIESXarOC7+/tuRw9bUuqjSJSf7rh1RDCxOLHhxPApwIvF9zfApxQ/CIzuxy4HGA88AXgOGAi8J5meL0dTsWDeGP0nn3RdX4YjetPro/Pb4iuu4uez0XPjY7u74ue66pQ+0RECnXCi709XvFBzBDCzSGElhBCSzfwSHRpA/guHPxNPxOA98L3R7crFbzjz+7t87uiS/HzcbDuiC77UPAWkeQNpwe+FZhecH9a9Fif8sAeYDseCJkFjPWUSiPVCd5DlcY2iUh9G04P/DFgtpnNNLNG4BJgWX9v6AZ240nyjQAHA5PhMDylkqMnfZGUXC8XEZE0GnIPPITwppl9DliOx7kfhRCe7fc9eOrhdaIe+G5/YAp+2UVPT1c9XhGR/g0nhUII4WfAz0p9fR4P0qOBzQD3AM3wWWAO8Fd4bnxvweurTScOEcmKqs7EjHvgXXgunBXAGphwDJwDjKt2g0REMqzqAXwfnj5ZBZz6HNy0DHgCctvhGHpK9dLUE1YuXETSqOod3hwenPcB6/H6b2wWTBrDWHoCuIiI9G9YOfDBGoEH6Dw+MWY6njZhZSvsgyZ8MHM7Xo0S98ILJ9BA3xNxKi2p7xUR6U1VA3gOD9gd9KQlOuCtOpaj8EHODbw9WKdNDj8hFZ5oRESqaciLWQ3FBLNwMrAaH8gcAzQDZ+Jz8C+4DGiDUx+AJ+mZ7Vjunm+cxhnM6wu/P55a34y3cWeZ2iUi0ptOeDyE0FL8eNUHMQvXGdmHV6OsBjYBzAdO9sBeTL1cEZEDVTWA/x6fvhmvJ7IPr/nuwhe64jONcCmM5cC1RsodvAf7eX2tjbKHKAUkIpKAqgbwbqCTnmDYHV16Fod6B4zzYN5ET+oijWV8eXztFv1lICJJqeogJvgZozDo7cfXpF0H8Hg75OF0PLD/DA/4cUqlk3QFzKTXbRGR+lb1AF4s7oHvBLgfaIKz8aD9G9K/bGuaTigiUl+qWoWSMwt9TdQZBxwPfBK4ODTCK/v57BSvMOxA5XoiUr9SUYXSnw7gV/hmD7y+H3K+2eaH8Hx4Qz/vFRGpR6kJ4ODpkhXAP4yH9ZPh4iXwo+/CQny1Qk2zFxHpkXgOvNgeYCmeFz+6HRgHF+EbPmymZ4amiEi9S00O/K3X4D3t8fjmx58EzgsHwwuv89FZ3kNP+8CmiEg5pT4HHotnaG4HHgTWALAAZs6hGZ/kIyIiCS0n299z8TolhTvBw6+B9czGF7xSLlxEJIU58EJ5oB3gt62QhzOix1vx4K6yQhGpZ6nLgcOBq/8144OYFwPztgPrYNFpXm7YjoK4iNS+zOTAi3UAD+D5cDqAcXAuvgRtb6sWiojUi1T2wIvFu/dciQfvSc8Aj8GZl/remqpKEZFaltkeOHiAbseXSlkF8J5JcDpMQFUpIlK/MhHAwdcNXwOsBeBNGOd14u9FVSkiUp9SXYVSqBtPgW8D+HU77IMPRM+14j10zdIUkXqSmQAe14Y/AFx3MiwA5j0Kx6+DrZd5amUTvr64Vi4UkXqQmQAe240H8bHAvN3+2AR8rZQ90WvG473xuF5cRKQWZaIKpdho4DB8lcJxeG+7G+99TwUWjYKX3oAPAm1l+D4RkSRlugqlWLyDz3LgIQ7MfzcDXATvPMZ76WnbS1NEpFwyl0KJ7cX30WwAnsHPRGPxnjm3z4cdv2HyZE+jiIjUokz2wGP78MqUnXgvfBdxT/x/waRmDkYlhiJSuwYM4GY23cweNrN1ZvasmV0RPd5sZivMbGN0fUjlm9u7eOXCTnyQE9ZDaOdIYBbajk1EalMpPfA3gatDCHOAecD/NbM5wLXAyhDCbGBldD8xcZnhToDfrYBV8BHgfDRbU0Rq04ABPITwSghhTXS7A3gOL/ZYACyOXrYYuKBSjSxVF7AE+MxR8NiJcHz4Q76yzvfTFBGpNYPKgZvZDOBYfN7M5BDCK9FT24DJfbzncjNbbWarq1GwuAdfM+UxAC6HP5ijPLiI1KSSA7iZvQP4KfDFEMLrhc8FLybvNT6HEG4OIbSEEFpsWE0tTbwlW88EnpGMIBvlhPGORCIipSgpgJtZAx687wghLI0e3m5mh0fPHw7sKOWzqhGg8niZoS99tYmJ+EzNtAfHEWS8LEhEqqqUKhQDbgWeCyF8r+CpZcCi6PYi4N5Svqxa6YwXAe77Z/iXTj6O724/jp5ebjmCeTl7zA34YOvo6HbaTzYikrxSJvKcBPwp8LSZPRk99hXgW8DdZnYZHi8/VpkmDk07wFJgNJzeAKO64HbiTZJFRLKv6muhjKU6KwVOxMtkLgXmdgEPwulnw+oqff9QxPXqWoBLRAqlZi2UagXPTrxUphVg5IUwP/2zMruji4hIKWp2zGwvsJ54NcIT4OB3vpVjTqt4MpKISCkyu5hVKbqIdvDhe7B1B2cDo4Bf4KWGCpYikmU12wOP3QX8te1g8zT4zB64fQm0AGOSbpiIyDDVfABvx4P4QwAHT4G5vhny0WiRKxHJtpoP4PuAl4FHAP53G9wEf32sz0qakGjLRESGp+YDOHgufCd4FN8AnAITjkr3gKaIyEDqIoCD98Jv2gC7lgH/B/hzrxUvDOJai0REsqRuAngn8DQeyDkaOFJT1kUk2+omgO/Cl5ldC2Bfgj8e8bZKFNVhi0iW1E0A78K3W9sFwHr47+64I65qFBHJpJqeyFOsC1gD8Bf3wTi4/g/gpefgNOIJPyIi2VFXARyiVQrvwvdZWwDvbIQxaz0XrvSJiGRJ3aRQYquB+Vvgvl8Af/tR+IXvXK+ZmSKSNXUXwPfiA5lrAPh/0OYzM0/AO+XTUWWKiGRDzaVQcvhZqZv+UyK3AS9aJycDXzsb6IK9D8KvgD8jSrWIiKRY3fXAYzuBJcCD8QNTYOxp8AGgMbFWiYiUruYCeB6vNskzcCokj6dTbnwAXvsxcDcc9MNsbIAsIlJzAXywXgceA54A2AJ0w9l4aaHWShGRNKv6nphpCIpx7zrupY8BZgOfAy4EGp8AlsLx34CNaI9KEUlWavbETFo8yBn/h+fxnepfBpbj66UwtxGOhylAE0qniEg61VUAz+HT5ifw9sDcgZcWPgrw8/2wET4CnIOnUhTERSRtaq6McKjy+FopL4OXpuyGk/GA/wi+hkpHYq0TEXm7ugrg8WqDuwrux7rw2u97gf3fhYuAk5bAu1ZB63e8d/4IvsOPiEga1FUKJdZF7wOTeTyIP4pv3MN0YDYcgefDs55GyXr7ReRAdRnA+7MfT6NsBF84ZTuciU/wGUP2l57N9XFbRLJHAbxIN54m2Q6eC98Ik46BU1BduIikiwJ4kXgm5wrgvHvgtz8GnpzPYY96GiXL0+yLdxzS8rki2aYA3os8sAdYhWdRYD2MgAV4KiXraRQRqQ0K4H3I40vP/ghYbO3kT4ArtsC/fwNmJNs0ERFAAXxA24GlRDM0p54HH1YPXETSoeQAbmY5M3vCzO6P7s80s1VmtsnM7jKzLKeH+xTP0GwDYAHMVQAXkXQYTA/8CuC5gvvXA98PIRwJvAZcNtzGpLWsLd7RHlbCVt+CbQbpba+I1IeSAriZTQP+GLglum/A6fieCACLgQtK+azCoNdAT282XhVwXCkfMgw5vBywt5LAhoJLruC1h+HrhjP/TrgE/hm4AWhGQVxEklNqD/wG4Bq8TBp8PajdIYQ3o/tbgKm9vdHMLjez1Wa2unoL1w5f8YHZBmx+FHgCDvownIEHdw0iiEhSBow/ZnYusCOE8PhQviCEcHMIoSWE0GK8ff2ReEp7HuiMLpUS95a76H1Nk7g93fTsqbkP2IzP6fk4sKILWD4H+4XXhWtyj4gkpZTFrE4CzjezeGXVg4EbgSYzGxn1wqcBW4fbmGpNLBnoewq3Y4snv3QArXgwh23Q4DXhY/GNkLXpg4hU24A98BDCdSGEaSGEGcAlwEMhhE8AD+OL9gEswhfyS7XimYgDvTbOgzfgB6oTuAd4zdrhz+Br34b7WnwPzVqgfL5Itgwnhftl4Coz24TnxG8tT5MqbziBqh1fVpZWYD5wnE+vr5XgVyv/HSL1YFDrgYcQfgn8MrrdCvzRYN5veG82DemGHINLpYCf7fYDm4Bt3XDYWGBM+QYyG+jJvSdF66OIZEeqiijilEWaFAa0HH7y2YXvZk8DMLonxVKOtpfrc0Sk9qUqgBduNlzp7xmMuHYy3nZtDdGGD2OAJs8fTeDAGvJCgz0xjSC52Z46eYhkR9UDePfAL0ml7oLrDqIyxCiFUlwPrjSEiFRDVQN4IB3BrYvBtSN+bXf03j3xE5OmwGyfPTo2el1vJ6hSq1+6ii5JUS9cJBtStalxmnvn8YBmPLlnO8CTbbAZzsVLCbdR2YlIIiKFLITqTXDPmYXBzFwspVKk2nL4ny3T8cD9OWDqG8BNMO8LXl24N8H2lUPhJCYRSV4nPB5CaCl+PFWDmFkQp0m2Aw8A6wAaL4QTvBc+JsG2iUh9SXUAT3MPcD+wE9/BHn4NHXAicBzZXy98MDNWRSQ5qQ7gaRbvXr8N4Jc7YAOcg6+PMo7sB3ERSb9UDWJmRb7g+sdA+2meC5+7HebeDr+52uvEW1FPVkQqRz3wYdoJLAd+A94lb4aPoN3rRaTyUl2FkgXxaoVz8F74xaOAl4AfwNHfjHPkIiJDpyqUCsnjk27agPuA598AJh0HJ/hWbE1JNk5EapoCeBnEa6Ssji789xrYD58Azka79ohIZWgQs0zineufB9/aYoMvFw6wKnpuT9F7NMApIsOhAF4m8Z6etwJrF8JC4IIfwHtWQe4O39X+XjzQj4iuO6L3auajiAyFAniZxVUp04EL2oBOz4W3A7PxQpV4j814Y+V4yQAFcBEZDFWhVEgz8D5gFj7BZzw+1b4DeBGfgr8YnxA0Gg/mO1EQF5G366sKRT3wCmkH/hMPysfggbkJ7203R7fjk9mE6HoyniffXM2GikhmKYBXUB7YCHwLrxUfA7wL+Bi+lso4vFf+AeBo4Jwx8KtO+BOyv6KhiFSeyggrrAtfL+VlfGr95uj2Tno2iNiHB3IWwimjYBTaVEFEBqYAXkVdeI/8B8Dt0e3VwM14j5xbLoWf+gDouKQaKSKZoRRKlXXhve9Ce/GBzZNYDtu9WiUO4O34wlga3BSRYqpCSYkpeC58DnAxMHU8cCGEf4GZ+ESgJPfJFJHkaC2UlGsD7gYeJhrAnAgsAHt/T8VKDuXGRaSHAniK5PFp9x8DHtsEXDAHPuu7/MRpFS1RKyIxBfCU2YtXqzwN8MI6aPOt2k7Ag/gU1AsXEacceEodBrQAnwTOuxrYCDuW+UbKX8bXXdHApkh9UA48Y7YB9wOPAOzirYVTjsCD+mmoJy5S79QDT7km4Cg8990I/CPwznAc/M0apn/VywxFpLapB55Ru/GBzdX4xB93FZzsa6johChSv0oK4GbWZGZLzGy9mT1nZieaWbOZrTCzjdH1IZVubD2LVyv0PTZXQqfXjR+DUiki9arUHviNwM9DCEfjMeM54FpgZQhhNrAyui8V1IXP2OTV22AzXIQHcf0ZJVKfBsyBm9l44ElgVih4sZltAE4NIbxiZocDvwwhHNXfZykHPjw5PB9+MfAX44HdU+C6NmZ8y3PhqkoRqU3DyYHPxP96v83MnjCzW8xsLDA5hPBK9Jpt+HLWb2Nml5vZajNbXb3h0tqUx2ds3gu8tgfgC3CC58HVCxepP6X87kfikwF/GEI4Fp9rckC6JOqZ9xqfQwg3hxBaQggtNtzWCh344lZefTIXpnsAb0yyUSKSiFIC+BZgSwhhVXR/CR7Qt0epE6LrHZVpohTK4wOaPph5I7TBAny2pgYzRerLgAE8hLANeNnM4vz2B/EtHZcBi6LHFuF/2UsV5IG/An5uD8Dn4ettcO9C36pNQVykfpS6HvjngTvMrBFfquNSPPjfbWaX4cURH6tME6U3LwN3AnNfhMMmAEfCGXh6ZS0a0BSpB5qJmVGj8VmaZwJ/D+SOxefYfxOmtnuuXEFcpDZoJmaN6cKD9GZgBXid0LnAiT7tXv+wIrVPv/OMytOz9OwSYP8W4N3z4RTfC2Jsko0TkapQAM+4vXht+GYAfgtdcDLwXrwnrkFNkdqlHHjG5fB8+PnALaOA9+GJ8e/D7E5fN7wL3/Vee2qKZJNy4DUqTqW8CKx4A8Jq/F+1yXfwmY1vlDyDnj01tbemSG1QD7xGjAOmR5cWPGC34JUqkxrgZ11eCxrtC0FXdFuVKiLp11cPvNQ6cEm5eHZmR3S7E182EoARPrDZgq8v3h49vxsFcpEsUwCvEV30BON2fFPk+/Ee+JQ3fLr9Xe8HVsFNb/jA53pgOz6ttpQceQ4FepE0UQCvMYWBvAMvJ3wd39UegAYY84bvbt+MB+/NBe8diIK4SHoogNeYHAeOTHfhvew7gdb/78/l8DXFj78cuA3O6fIe+cbiD+vlc+OqF1W1iCRPVSg1Lo8H2p3AGnqWot0NXr4y3tdQeS+lVaY0AGPwIK5KFpFkqQqlBhUH1rjn3ICnVMbjGyJPB64Bjr4buAombfGY3t/nTsHLErfhJ4P4BCEilaM68DqT58BcdRxo9wFv4AF4DVFP/BjgaK9UaSrxswupJy6SDOXAa1Bx4IaeXjh4/roxur8bvGSlAT6FB/Xl9N6rzuOpmL1ANwcG7hF9vEdEKkc98DpTGMjBK1VoBfbCLOCw6PG+etVdeA35voLH1AMXSYZ64IMQB6q0l9EVlvrFue/4uhEfhIxtALgB2OuDmR3Rawt72IWf1d93Er0v7cdHpFaoBz4EWVhPJNfHbfAg2433pncDr7dBfg8cMsbz4A3RpS/5ous0HweRWqYAPkQj8FK6clbVDPbE0Nvr4p52nCoZUXDZh+evO/DAvRufyHMX8DzAB+B4PJUykbdvDBEH7Dj4xznvUfQf8EWkMhTA60Sc2oiv46qUTnxgsgtgos/OHEffJ6bi6hYRSY5y4INQXN3RXcHPH+rri9Mbxe0srNvuxitSOokC9lHQeATMeNGf21b0Hb3VlxO9v/DkICLVoR74MKS1N1rcpuITTxzE8/QsftUGPpd+BJwNHBe9PtfLe3v7vN6+V0QqSzMx61BhdcloPGUyB69CuaoZL01ZCNMf9Hx5f/XdxQFeRMpPMzHlLYW95rgS5UVgFRDagUP/EN7rm0JMLOGzRCQZCuB1Lk6nbAUeiS688BSMhe8AH2fgqpi0ppJEap0CuAAexDuIJvbcA+yGeTN9h3uVCIqkkwK4HODbwHlXA4uB1i/xoW94GmVsss0SkV6ojLCGDWX3nG14Xfizv4f3sAp2+SDnROAIPFe+przNFJEhUhVKjWqgZzJOXDI4mNUCZwHz6Sk1vBGY0Ar8CTQ9oZUHRapJVSgyKK34Nmz/iQfrCeOBmX8Kp2jtE5G0UACvUfGgZAfegx5KjzmPp1QeAZ9uyTVwbM/qhiKSLAVw6Vd8ItjfBfAq5HzN8PGJtkpEoMQAbmZXmtmzZvaMmd1pZqPNbKaZrTKzTWZ2l5k1VrqxUl45Sl8BcS0AZ0IrXA2cg3riIkkbMICb2VTgC0BLCCHevPwS4Hrg+yGEI4HXgMsq2VBJ1kaAf90PbT65Zz4+SKo/4USSU+rvbyRwkJmNxDd0eQU4HVgSPb8YuKD8zZNKKtwvcyBfAz7yKeAnkAvnseiLvkN9E+qFiyRlwN9uCGEr8HfAS3jg3gM8DuwOIbwZvWwLMLW395vZ5Wa22sxWV69gUUpVap14G/AQsP73ABtgIizCe+IikoxSUiiHAAuAmXinayxwVqlfEEK4OYTQEkJosSE3UyoprhMfSDeePrnSfge3wRXho/zkMk+lqBcuUn2l/PV8BvBCCGFnCKELWAqcBDRFKRWAafh6SFLD8vTUh7MN4Bpo8Zya1ksRqb5SAvhLwDwzG2NmBnwQ30rxYeCi6DWLgHsr00RJk3gbtv1xKgWYjZcWikh1lZIDX4UPVq4Bno7eczPwZeAqM9sETABurWA7JUXyxKH7KngVLsVXLVQaRaS6tBaKDMkMoAVYfCSw8Utw5XeYdIPP+tTa4CLlpbVQpKw243+W7dgEsBumw0LgRHxQUzlxkcpTD1yG5X3AmcDXJwPbRsCCbk5d5mOcbdFr1CMXGZ6+euBaD1yGZSMerD+1Hd4VumGcrxs+Hh/Y3AVsB/ajJWhFyk0BPEPiQcIReE12Gnq28abInwbOGAHTgc/h64lPmAm/fcFncbbhm0HE0tB2kaxTDlyGJY+fTFrpKVWCaP3w8+AYoBHPiet/NpHyUg88Q/JF12mRB/bivfHV0WPH54AbGzlo4352PuBL0opIealTJGXRjee59+J7arq50NTTSxeR8qpqFYqZ7cR/469W7UsH71DUvqFKc9tA7RsutW94htO+I0IIE4sfrGoABzCz1b2Vw6SF2jd0aW4bqH3DpfYNTyXapxSKiEhGKYCLiGRUEgH85gS+czDUvqFLc9tA7RsutW94yt6+qufARUSkPJRCERHJKAVwEZGMqloAN7OzzGyDmW0ys2ur9b39tGe6mT1sZuvM7FkzuyJ6vNnMVpjZxuj6kITbmTOzJ8zs/uj+TDNbFR3Hu8ysMcG2NZnZEjNbb2bPmdmJaTp+ZnZl9G/7jJndaWajkzx+ZvYjM9thZs8UPNbr8TL3g6idT5nZcQm17zvRv+9TZnaPmTUVPHdd1L4NZnZmEu0reO5qMwtmdmh0v6rHr6+2mdnno+P3rJl9u+Dx8hy7EELFL/g6TM/jaxw1AmuBOdX47n7adDhwXHR7HPA7YA7wbeDa6PFrgesTbudVwE+A+6P7dwOXRLf/CfjzBNu2GPhsdLsRaErL8QOmAi8ABxUct08nefyA9wPHAc8UPNbr8QLOAR4ADJgHrEqofR8GRka3ry9o35zodzwK3/D8eSBX7fZFj08HluPrpR2axPHr49idBjwIjIruTyr3savW/7gnAssL7l8HXFeN7x5EG+8FPoTvFnZ49NjhwIYE2zQNWAmcDtwf/c/4asEP6oDjWuW2jY8CpBU9norjFwXwl4FmfM2f+/GlyxM9fvhmRoU/8l6PF3ATsLC311WzfUXPXQjcEd0+4DccBdATk2gfvo7aMfg+I3EAr/rx6+Xf9m7gjF5eV7ZjV60USvxjim2JHksFM5sBHAusAiaHEF6JntoGTE6oWQA3ANfQs5TIBGB3COHN6H6Sx3EmvuzJbVGK5xYzG0tKjl8IYSvwd/im3K8Ae4DHSc/xi/V1vNL4m/kM3quFlLTPzBYAW0MIa4ueSkP73g2cEqXsHjGz48vdtrofxDSzdwA/Bb4YQni98Lngp8dE6izN7FxgRwjh8SS+vwQj8T8ZfxhCOBZf4+aAsY2Ej98hwAL8RDMFGAuclURbSpXk8RqImX0VeBO4I+m2xMxsDPAV4C+TbksfRuJ/Ac4DvgTcbWZWzi+oVgDfiuepYtOixxJlZg148L4jhLA0eni7mR0ePX84sCOh5p0EnG9mm4F/w9MoNwJNZhYvA5zkcdwCbAkhrIruL8EDelqO3xnACyGEnSGELmApfkzTcvxifR2v1PxmzOzTwLnAJ6KTDKSjfe/CT9Bro9/JNGCNmR2WkvZtAZYG91/4X9KHlrNt1QrgjwGzowqARuASYFmVvrtX0ZnwVuC5EML3Cp5aBiyKbi/Cc+NVF0K4LoQwLYQwAz9eD4UQPgE8DFyUgvZtA142s6Oihz4IrCMlxw9PncwzszHRv3XcvlQcvwJ9Ha9lwKeiaop5wJ6CVEvVmNlZeBrv/BBCZ8FTy4BLzGyUmc0EZgP/Vc22hRCeDiFMCiHMiH4nW/DChG2k4/j9Bz6QiZm9Gx/of5VyHrtKDzoUJOrPwSs9nge+Wq3v7ac9J+N/rj4FPBldzsHzzCvx7R4fBJpT0NZT6alCmRX9Y28C/p1ohDuhds3F93B4Kvqf9ZA0HT/g68B64BngX/FR/8SOH3Anno/vwoPNZX0dL3zA+h+j38vTQEtC7duE52vj38g/Fbz+q1H7NgBnJ9G+ouc30zOIWdXj18exawRuj/7/WwOcXu5jp6n0IiIZVfeDmCIiWaUALiKSUQrgIiIZpQAuIpJRCuAiIhmlAC4iklEK4CIiGfU//J3eWwsGFSwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 49\n",
            " batch Loss train: 0.04431162774562836\n",
            "i 6\n",
            "epoch 49\n",
            " batch Loss train: 0.04781636223196983\n",
            "i 7\n",
            "epoch 49\n",
            " batch Loss train: 0.04427238926291466\n",
            "i 8\n",
            "epoch 49\n",
            " batch Loss train: 0.04204448685050011\n",
            "i 9\n",
            "epoch 49\n",
            " batch Loss train: 0.054482318460941315\n",
            "i 10\n",
            "epoch 49\n",
            " batch Loss train: 0.04608890414237976\n",
            "i 11\n",
            "epoch 49\n",
            " batch Loss train: 0.036351241171360016\n",
            "i 12\n",
            "epoch 49\n",
            " batch Loss train: 0.04040446877479553\n",
            "i 13\n",
            "epoch 49\n",
            " batch Loss train: 0.03529305011034012\n",
            "i 14\n",
            "epoch 49\n",
            " batch Loss train: 0.05042695626616478\n",
            "i 15\n",
            "epoch 49\n",
            " batch Loss train: 0.054952774196863174\n",
            "i 16\n",
            "epoch 49\n",
            " batch Loss train: 0.05254345014691353\n",
            "i 17\n",
            "epoch 49\n",
            " batch Loss train: 0.05909129977226257\n",
            "i 18\n",
            "epoch 49\n",
            " batch Loss train: 0.05115346610546112\n",
            "i 19\n",
            "epoch 49\n",
            " batch Loss train: 0.052855394780635834\n",
            "i 20\n",
            "epoch 49\n",
            " batch Loss train: 0.04373086988925934\n",
            "i 21\n",
            "epoch 49\n",
            " batch Loss train: 0.03335459530353546\n",
            "i 22\n",
            "epoch 49\n",
            " batch Loss train: 0.07996252924203873\n",
            "i 23\n",
            "epoch 49\n",
            " batch Loss train: 0.0361596904695034\n",
            "i 24\n",
            "epoch 49\n",
            " batch Loss train: 0.0645379051566124\n",
            "i 25\n",
            "epoch 49\n",
            " batch Loss train: 0.0458294153213501\n",
            "i 26\n",
            "epoch 49\n",
            " batch Loss train: 0.0552317276597023\n",
            "i 27\n",
            "epoch 49\n",
            " batch Loss train: 0.04644260182976723\n",
            "i 28\n",
            "epoch 49\n",
            " batch Loss train: 0.042381152510643005\n",
            "i 29\n",
            "epoch 49\n",
            " batch Loss train: 0.05109023675322533\n",
            "i 30\n",
            "epoch 49\n",
            " batch Loss train: 0.04214022681117058\n",
            "i 31\n",
            "epoch 49\n",
            " batch Loss train: 0.048362426459789276\n",
            "i 32\n",
            "epoch 49\n",
            " batch Loss train: 0.047986578196287155\n",
            "i 33\n",
            "epoch 49\n",
            " batch Loss train: 0.04540788382291794\n",
            "i 34\n",
            "epoch 49\n",
            " batch Loss train: 0.040411438792943954\n",
            "i 35\n",
            "epoch 49\n",
            " batch Loss train: 0.042960572987794876\n",
            "i 36\n",
            "epoch 49\n",
            " batch Loss train: 0.05325834080576897\n",
            "i 37\n",
            "epoch 49\n",
            " batch Loss train: 0.04672100394964218\n",
            "i 38\n",
            "epoch 49\n",
            " batch Loss train: 0.044694650918245316\n",
            "i 39\n",
            "epoch 49\n",
            " batch Loss train: 0.04955476149916649\n",
            "i 40\n",
            "epoch 49\n",
            " batch Loss train: 0.05438533425331116\n",
            "i 41\n",
            "epoch 49\n",
            " batch Loss train: 0.04636679217219353\n",
            "i 42\n",
            "epoch 49\n",
            " batch Loss train: 0.039600566029548645\n",
            "i 43\n",
            "epoch 49\n",
            " batch Loss train: 0.04252899810671806\n",
            "i 44\n",
            "epoch 49\n",
            " batch Loss train: 0.028527066111564636\n",
            "i 45\n",
            "epoch 49\n",
            " batch Loss train: 0.034886252135038376\n",
            "i 46\n",
            "epoch 49\n",
            " batch Loss train: 0.05436551943421364\n",
            "i 47\n",
            "epoch 49\n",
            " batch Loss train: 0.04039807245135307\n",
            "i 48\n",
            "epoch 49\n",
            " batch Loss train: 0.04873695224523544\n",
            "i 49\n",
            "epoch 49\n",
            " batch Loss train: 0.04525664076209068\n",
            "i 50\n",
            "epoch 49\n",
            " batch Loss train: 0.034479763358831406\n",
            "i 51\n",
            "epoch 49\n",
            " batch Loss train: 0.035952404141426086\n",
            "i 52\n",
            "epoch 49\n",
            " batch Loss train: 0.0718485489487648\n",
            "i 53\n",
            "epoch 49\n",
            " batch Loss train: 0.03964519128203392\n",
            "i 54\n",
            "epoch 49\n",
            " batch Loss train: 0.05022582784295082\n",
            "i 55\n",
            "epoch 49\n",
            " batch Loss train: 0.08417469263076782\n",
            "i 56\n",
            "epoch 49\n",
            " batch Loss train: 0.04037958383560181\n",
            "i 57\n",
            "epoch 49\n",
            " batch Loss train: 0.04543912038207054\n",
            "i 58\n",
            "epoch 49\n",
            " batch Loss train: 0.044704142957925797\n",
            "i 59\n",
            "epoch 49\n",
            " batch Loss train: 0.04261310398578644\n",
            "i 60\n",
            "epoch 49\n",
            " batch Loss train: 0.0427468866109848\n",
            "i 61\n",
            "epoch 49\n",
            " batch Loss train: 0.042387284338474274\n",
            "i 62\n",
            "epoch 49\n",
            " batch Loss train: 0.05319109186530113\n",
            "i 63\n",
            "epoch 49\n",
            " batch Loss train: 0.06624964624643326\n",
            "i 64\n",
            "epoch 49\n",
            " batch Loss train: 0.06385398656129837\n",
            "i 65\n",
            "epoch 49\n",
            " batch Loss train: 0.050365377217531204\n",
            "i 66\n",
            "epoch 49\n",
            " batch Loss train: 0.03638734295964241\n",
            "i 67\n",
            "epoch 49\n",
            " batch Loss train: 0.056869134306907654\n",
            "i 68\n",
            "epoch 49\n",
            " batch Loss train: 0.055860958993434906\n",
            "i 69\n",
            "epoch 49\n",
            " batch Loss train: 0.033913347870111465\n",
            "i 70\n",
            "epoch 49\n",
            " batch Loss train: 0.04844743013381958\n",
            "i 71\n",
            "epoch 49\n",
            " batch Loss train: 0.03713443502783775\n",
            "i 72\n",
            "epoch 49\n",
            " batch Loss train: 0.05820072069764137\n",
            "i 73\n",
            "epoch 49\n",
            " batch Loss train: 0.05290870741009712\n",
            "i 74\n",
            "epoch 49\n",
            " batch Loss train: 0.030782686546444893\n",
            "i 75\n",
            "epoch 49\n",
            " batch Loss train: 0.04412061721086502\n",
            "i 76\n",
            "epoch 49\n",
            " batch Loss train: 0.04417816177010536\n",
            "i 77\n",
            "epoch 49\n",
            " batch Loss train: 0.056249745190143585\n",
            "i 78\n",
            "epoch 49\n",
            " batch Loss train: 0.03959433734416962\n",
            "i 79\n",
            "epoch 49\n",
            " batch Loss train: 0.055530793964862823\n",
            "i 80\n",
            "epoch 49\n",
            " batch Loss train: 0.05043550580739975\n",
            "i 81\n",
            "epoch 49\n",
            " batch Loss train: 0.06302128732204437\n",
            "i 82\n",
            "epoch 49\n",
            " batch Loss train: 0.07088608294725418\n",
            "i 83\n",
            "epoch 49\n",
            " batch Loss train: 0.04200485721230507\n",
            "i 84\n",
            "epoch 49\n",
            " batch Loss train: 0.07511124759912491\n",
            "i 85\n",
            "epoch 49\n",
            " batch Loss train: 0.04769757762551308\n",
            "i 86\n",
            "epoch 49\n",
            " batch Loss train: 0.04893506318330765\n",
            "i 87\n",
            "epoch 49\n",
            " batch Loss train: 0.046491675078868866\n",
            "i 88\n",
            "epoch 49\n",
            " batch Loss train: 0.05194888636469841\n",
            "i 89\n",
            "epoch 49\n",
            " batch Loss train: 0.0388159304857254\n",
            "i 90\n",
            "epoch 49\n",
            " batch Loss train: 0.059601739048957825\n",
            "i 91\n",
            "epoch 49\n",
            " batch Loss train: 0.06150127574801445\n",
            "i 92\n",
            "epoch 49\n",
            " batch Loss train: 0.040646832436323166\n",
            "i 93\n",
            "epoch 49\n",
            " batch Loss train: 0.05675855278968811\n",
            "i 94\n",
            "epoch 49\n",
            " batch Loss train: 0.05464130640029907\n",
            "i 95\n",
            "epoch 49\n",
            " batch Loss train: 0.052613891661167145\n",
            "i 96\n",
            "epoch 49\n",
            " batch Loss train: 0.040335964411497116\n",
            "i 97\n",
            "epoch 49\n",
            " batch Loss train: 0.04542981833219528\n",
            "i 98\n",
            "epoch 49\n",
            " batch Loss train: 0.06375721096992493\n",
            "i 99\n",
            "epoch 49\n",
            " batch Loss train: 0.053345099091529846\n",
            "i 100\n",
            "epoch 49\n",
            " batch Loss train: 0.04195820167660713\n",
            "i 101\n",
            "epoch 49\n",
            " batch Loss train: 0.03943266719579697\n",
            "i 102\n",
            "epoch 49\n",
            " batch Loss train: 0.05409783497452736\n",
            "i 103\n",
            "epoch 49\n",
            " batch Loss train: 0.05943092331290245\n",
            "i 104\n",
            "epoch 49\n",
            " batch Loss train: 0.04747646301984787\n",
            "i 105\n",
            "epoch 49\n",
            " batch Loss train: 0.034779712557792664\n",
            "i 106\n",
            "epoch 49\n",
            " batch Loss train: 0.04764183983206749\n",
            "i 107\n",
            "epoch 49\n",
            " batch Loss train: 0.05858774483203888\n",
            "i 108\n",
            "epoch 49\n",
            " batch Loss train: 0.0396583192050457\n",
            "i 109\n",
            "epoch 49\n",
            " batch Loss train: 0.04924316704273224\n",
            "i 110\n",
            "epoch 49\n",
            " batch Loss train: 0.039705149829387665\n",
            "i 111\n",
            "epoch 49\n",
            " batch Loss train: 0.05624227225780487\n",
            "i 112\n",
            "epoch 49\n",
            " batch Loss train: 0.04473654925823212\n",
            "i 113\n",
            "epoch 49\n",
            " batch Loss train: 0.039401907473802567\n",
            "i 114\n",
            "epoch 49\n",
            " batch Loss train: 0.0557069331407547\n",
            "i 115\n",
            "epoch 49\n",
            " batch Loss train: 0.031140167266130447\n",
            "i 116\n",
            "epoch 49\n",
            " batch Loss train: 0.05074172839522362\n",
            "i 117\n",
            "epoch 49\n",
            " batch Loss train: 0.05811377614736557\n",
            "i 118\n",
            "epoch 49\n",
            " batch Loss train: 0.044015225023031235\n",
            "i 119\n",
            "epoch 49\n",
            " batch Loss train: 0.04125631973147392\n",
            "i 120\n",
            "epoch 49\n",
            " batch Loss train: 0.0599757581949234\n",
            "i 121\n",
            "epoch 49\n",
            " batch Loss train: 0.0450412854552269\n",
            "i 122\n",
            "epoch 49\n",
            " batch Loss train: 0.0501570999622345\n",
            "i 123\n",
            "epoch 49\n",
            " batch Loss train: 0.04174921289086342\n",
            "i 124\n",
            "epoch 49\n",
            " batch Loss train: 0.07729561626911163\n",
            "i 125\n",
            "epoch 49\n",
            " batch Loss train: 0.050968412309885025\n",
            "i 126\n",
            "epoch 49\n",
            " batch Loss train: 0.04200528934597969\n",
            "i 127\n",
            "epoch 49\n",
            " batch Loss train: 0.07556189596652985\n",
            "i 128\n",
            "epoch 49\n",
            " batch Loss train: 0.04895896092057228\n",
            "i 129\n",
            "epoch 49\n",
            " batch Loss train: 0.057730987668037415\n",
            "i 130\n",
            "epoch 49\n",
            " batch Loss train: 0.0597030371427536\n",
            "i 131\n",
            "epoch 49\n",
            " batch Loss train: 0.044718511402606964\n",
            "i 132\n",
            "epoch 49\n",
            " batch Loss train: 0.05582611262798309\n",
            "i 133\n",
            "epoch 49\n",
            " batch Loss train: 0.07897085696458817\n",
            "i 134\n",
            "epoch 49\n",
            " batch Loss train: 0.0683973878622055\n",
            "i 135\n",
            "epoch 49\n",
            " batch Loss train: 0.04823034629225731\n",
            "i 136\n",
            "epoch 49\n",
            " batch Loss train: 0.04619823768734932\n",
            "i 137\n",
            "epoch 49\n",
            " batch Loss train: 0.05361324921250343\n",
            "i 138\n",
            "epoch 49\n",
            " batch Loss train: 0.05379785969853401\n",
            "i 139\n",
            "epoch 49\n",
            " batch Loss train: 0.0705127865076065\n",
            "i 140\n",
            "epoch 49\n",
            " batch Loss train: 0.040398482233285904\n",
            "i 141\n",
            "epoch 49\n",
            " batch Loss train: 0.05779355391860008\n",
            "i 142\n",
            "epoch 49\n",
            " batch Loss train: 0.07081393152475357\n",
            "i 143\n",
            "epoch 49\n",
            " batch Loss train: 0.0626327320933342\n",
            "i 144\n",
            "epoch 49\n",
            " batch Loss train: 0.05002830922603607\n",
            "i 145\n",
            "epoch 49\n",
            " batch Loss train: 0.051738012582063675\n",
            "i 146\n",
            "epoch 49\n",
            " batch Loss train: 0.041125498712062836\n",
            "i 147\n",
            "epoch 49\n",
            " batch Loss train: 0.047726135700941086\n",
            "i 148\n",
            "epoch 49\n",
            " batch Loss train: 0.0527208186686039\n",
            "i 149\n",
            "epoch 49\n",
            " batch Loss train: 0.07472948729991913\n",
            "i 150\n",
            "epoch 49\n",
            " batch Loss train: 0.0496247261762619\n",
            "i 151\n",
            "epoch 49\n",
            " batch Loss train: 0.0564434714615345\n",
            "i 152\n",
            "epoch 49\n",
            " batch Loss train: 0.044564388692379\n",
            "i 153\n",
            "epoch 49\n",
            " batch Loss train: 0.0631147176027298\n",
            "i 154\n",
            "epoch 49\n",
            " batch Loss train: 0.07109826803207397\n",
            "i 155\n",
            "epoch 49\n",
            " batch Loss train: 0.06588762253522873\n",
            "i 156\n",
            "epoch 49\n",
            " batch Loss train: 0.046437881886959076\n",
            "i 157\n",
            "epoch 49\n",
            " batch Loss train: 0.051690392196178436\n",
            "i 158\n",
            "epoch 49\n",
            " batch Loss train: 0.03600767254829407\n",
            "i 159\n",
            "epoch 49\n",
            " batch Loss train: 0.06728468090295792\n",
            "i 160\n",
            "epoch 49\n",
            " batch Loss train: 0.051354553550481796\n",
            "i 161\n",
            "epoch 49\n",
            " batch Loss train: 0.052909158170223236\n",
            "i 162\n",
            "epoch 49\n",
            " batch Loss train: 0.040586065500974655\n",
            "i 163\n",
            "epoch 49\n",
            " batch Loss train: 0.05273619666695595\n",
            "i 164\n",
            "epoch 49\n",
            " batch Loss train: 0.04448825120925903\n",
            "i 165\n",
            "epoch 49\n",
            " batch Loss train: 0.07557447254657745\n",
            "i 166\n",
            "epoch 49\n",
            " batch Loss train: 0.05409073829650879\n",
            "i 167\n",
            "epoch 49\n",
            " batch Loss train: 0.06017768010497093\n",
            "i 168\n",
            "epoch 49\n",
            " batch Loss train: 0.05298047512769699\n",
            "i 169\n",
            "epoch 49\n",
            " batch Loss train: 0.05441967025399208\n",
            "i 170\n",
            "epoch 49\n",
            " batch Loss train: 0.054366372525691986\n",
            "i 171\n",
            "epoch 49\n",
            " batch Loss train: 0.05267150700092316\n",
            "i 172\n",
            "epoch 49\n",
            " batch Loss train: 0.053442392498254776\n",
            "i 173\n",
            "epoch 49\n",
            " batch Loss train: 0.06722056120634079\n",
            "i 174\n",
            "epoch 49\n",
            " batch Loss train: 0.04982185363769531\n",
            "i 175\n",
            "epoch 49\n",
            " batch Loss train: 0.05301143601536751\n",
            "i 176\n",
            "epoch 49\n",
            " batch Loss train: 0.05168091878294945\n",
            "i 177\n",
            "epoch 49\n",
            " batch Loss train: 0.045206308364868164\n",
            "i 178\n",
            "epoch 49\n",
            " batch Loss train: 0.060386862605810165\n",
            "i 179\n",
            "epoch 49\n",
            " batch Loss train: 0.0615011565387249\n",
            "i 180\n",
            "epoch 49\n",
            " batch Loss train: 0.03779912739992142\n",
            "i 181\n",
            "epoch 49\n",
            " batch Loss train: 0.04195481538772583\n",
            "i 182\n",
            "epoch 49\n",
            " batch Loss train: 0.05501842126250267\n",
            "i 183\n",
            "epoch 49\n",
            " batch Loss train: 0.05527722090482712\n",
            "i 184\n",
            "epoch 49\n",
            " batch Loss train: 0.05506051704287529\n",
            "i 185\n",
            "epoch 49\n",
            " batch Loss train: 0.047514695674180984\n",
            "i 186\n",
            "epoch 49\n",
            " batch Loss train: 0.04414738342165947\n",
            "i 187\n",
            "epoch 49\n",
            " batch Loss train: 0.04876498505473137\n",
            "i 188\n",
            "epoch 49\n",
            " batch Loss train: 0.0622895322740078\n",
            "i 189\n",
            "epoch 49\n",
            " batch Loss train: 0.04453768581151962\n",
            "i 190\n",
            "epoch 49\n",
            " batch Loss train: 0.042476586997509\n",
            "i 191\n",
            "epoch 49\n",
            " batch Loss train: 0.03480031341314316\n",
            "i 192\n",
            "epoch 49\n",
            " batch Loss train: 0.05981457978487015\n",
            "i 193\n",
            "epoch 49\n",
            " batch Loss train: 0.04872655123472214\n",
            "i 194\n",
            "epoch 49\n",
            " batch Loss train: 0.04281282052397728\n",
            "i 195\n",
            "epoch 49\n",
            " batch Loss train: 0.06373900175094604\n",
            "i 196\n",
            "epoch 49\n",
            " batch Loss train: 0.0519166998565197\n",
            "i 197\n",
            "epoch 49\n",
            " batch Loss train: 0.05958392843604088\n",
            "i 198\n",
            "epoch 49\n",
            " batch Loss train: 0.07404253631830215\n",
            "i 199\n",
            "epoch 49\n",
            " batch Loss train: 0.054169751703739166\n",
            "i 200\n",
            "epoch 49\n",
            " batch Loss train: 0.05564984306693077\n",
            "i 201\n",
            "epoch 49\n",
            " batch Loss train: 0.052316516637802124\n",
            "i 202\n",
            "epoch 49\n",
            " batch Loss train: 0.046558380126953125\n",
            "i 203\n",
            "epoch 49\n",
            " batch Loss train: 0.057600729167461395\n",
            "i 204\n",
            "epoch 49\n",
            " batch Loss train: 0.044891826808452606\n",
            "i 205\n",
            "epoch 49\n",
            " batch Loss train: 0.051580533385276794\n",
            "i 206\n",
            "epoch 49\n",
            " batch Loss train: 0.045738667249679565\n",
            "i 207\n",
            "epoch 49\n",
            " batch Loss train: 0.06587115675210953\n",
            "i 208\n",
            "epoch 49\n",
            " batch Loss train: 0.054323866963386536\n",
            "i 209\n",
            "epoch 49\n",
            " batch Loss train: 0.04028706252574921\n",
            "i 210\n",
            "epoch 49\n",
            " batch Loss train: 0.0396706648170948\n",
            "i 211\n",
            "epoch 49\n",
            " batch Loss train: 0.04594486951828003\n",
            "i 212\n",
            "epoch 49\n",
            " batch Loss train: 0.038914475589990616\n",
            "i 213\n",
            "epoch 49\n",
            " batch Loss train: 0.0576198510825634\n",
            "i 214\n",
            "epoch 49\n",
            " batch Loss train: 0.0313122421503067\n",
            "i 215\n",
            "epoch 49\n",
            " batch Loss train: 0.035217929631471634\n",
            "i 216\n",
            "epoch 49\n",
            " batch Loss train: 0.049860600382089615\n",
            "i 217\n",
            "epoch 49\n",
            " batch Loss train: 0.03473064303398132\n",
            "i 218\n",
            "epoch 49\n",
            " batch Loss train: 0.05236295983195305\n",
            "i 219\n",
            "epoch 49\n",
            " batch Loss train: 0.06534941494464874\n",
            "i 220\n",
            "epoch 49\n",
            " batch Loss train: 0.036921512335538864\n",
            "i 221\n",
            "epoch 49\n",
            " batch Loss train: 0.04937224090099335\n",
            "i 222\n",
            "epoch 49\n",
            " batch Loss train: 0.04901217669248581\n",
            "i 223\n",
            "epoch 49\n",
            " batch Loss train: 0.07573707401752472\n",
            "i 224\n",
            "epoch 49\n",
            " batch Loss train: 0.04434569552540779\n",
            "i 225\n",
            "epoch 49\n",
            " batch Loss train: 0.04363349825143814\n",
            "i 226\n",
            "epoch 49\n",
            " batch Loss train: 0.04944797605276108\n",
            "i 227\n",
            "epoch 49\n",
            " batch Loss train: 0.04531516507267952\n",
            "i 228\n",
            "epoch 49\n",
            " batch Loss train: 0.05023890733718872\n",
            "i 229\n",
            "epoch 49\n",
            " batch Loss train: 0.06664735823869705\n",
            "i 230\n",
            "epoch 49\n",
            " batch Loss train: 0.06159693002700806\n",
            "i 231\n",
            "epoch 49\n",
            " batch Loss train: 0.04256341606378555\n",
            "i 232\n",
            "epoch 49\n",
            " batch Loss train: 0.060885630548000336\n",
            "i 233\n",
            "epoch 49\n",
            " batch Loss train: 0.05392639711499214\n",
            "i 234\n",
            "epoch 49\n",
            " batch Loss train: 0.04037553817033768\n",
            "i 235\n",
            "epoch 49\n",
            " batch Loss train: 0.06306573748588562\n",
            "i 236\n",
            "epoch 49\n",
            " batch Loss train: 0.04779733344912529\n",
            "i 237\n",
            "epoch 49\n",
            " batch Loss train: 0.05072685703635216\n",
            "i 238\n",
            "epoch 49\n",
            " batch Loss train: 0.06085655838251114\n",
            "i 239\n",
            "epoch 49\n",
            " batch Loss train: 0.05101732909679413\n",
            "i 240\n",
            "epoch 49\n",
            " batch Loss train: 0.043708037585020065\n",
            "i 241\n",
            "epoch 49\n",
            " batch Loss train: 0.04963808134198189\n",
            "i 242\n",
            "epoch 49\n",
            " batch Loss train: 0.04836875945329666\n",
            "i 243\n",
            "epoch 49\n",
            " batch Loss train: 0.05366678163409233\n",
            "i 244\n",
            "epoch 49\n",
            " batch Loss train: 0.04704048112034798\n",
            "i 245\n",
            "epoch 49\n",
            " batch Loss train: 0.07407652586698532\n",
            "i 246\n",
            "epoch 49\n",
            " batch Loss train: 0.07125940173864365\n",
            "i 247\n",
            "epoch 49\n",
            " batch Loss train: 0.05754309147596359\n",
            "i 248\n",
            "epoch 49\n",
            " batch Loss train: 0.06197883561253548\n",
            "i 249\n",
            "epoch 49\n",
            " batch Loss train: 0.045104775577783585\n",
            "i 250\n",
            "epoch 49\n",
            " batch Loss train: 0.06388968229293823\n",
            "i 251\n",
            "epoch 49\n",
            " batch Loss train: 0.05269099399447441\n",
            "i 252\n",
            "epoch 49\n",
            " batch Loss train: 0.0638929009437561\n",
            "i 253\n",
            "epoch 49\n",
            " batch Loss train: 0.04629059135913849\n",
            "i 254\n",
            "epoch 49\n",
            " batch Loss train: 0.04628315940499306\n",
            "i 255\n",
            "epoch 49\n",
            " batch Loss train: 0.07190654426813126\n",
            "i 256\n",
            "epoch 49\n",
            " batch Loss train: 0.06635341793298721\n",
            "i 257\n",
            "epoch 49\n",
            " batch Loss train: 0.05588251352310181\n",
            "i 258\n",
            "epoch 49\n",
            " batch Loss train: 0.05584322288632393\n",
            "i 259\n",
            "epoch 49\n",
            " batch Loss train: 0.056049373000860214\n",
            "i 260\n",
            "epoch 49\n",
            " batch Loss train: 0.048559337854385376\n",
            "i 261\n",
            "epoch 49\n",
            " batch Loss train: 0.043957825750112534\n",
            "i 262\n",
            "epoch 49\n",
            " batch Loss train: 0.07832323759794235\n",
            "i 263\n",
            "epoch 49\n",
            " batch Loss train: 0.064194455742836\n",
            "i 264\n",
            "epoch 49\n",
            " batch Loss train: 0.05563574284315109\n",
            "i 265\n",
            "epoch 49\n",
            " batch Loss train: 0.044720981270074844\n",
            "i 266\n",
            "epoch 49\n",
            " batch Loss train: 0.057040415704250336\n",
            "i 267\n",
            "epoch 49\n",
            " batch Loss train: 0.04336085170507431\n",
            "i 268\n",
            "epoch 49\n",
            " batch Loss train: 0.049700066447257996\n",
            "i 269\n",
            "epoch 49\n",
            " batch Loss train: 0.08132001012563705\n",
            "i 270\n",
            "epoch 49\n",
            " batch Loss train: 0.049662668257951736\n",
            "i 271\n",
            "epoch 49\n",
            " batch Loss train: 0.05436019226908684\n",
            "i 272\n",
            "epoch 49\n",
            " batch Loss train: 0.05354165658354759\n",
            "i 273\n",
            "epoch 49\n",
            " batch Loss train: 0.08070545643568039\n",
            "i 274\n",
            "epoch 49\n",
            " batch Loss train: 0.0596834234893322\n",
            "i 275\n",
            "epoch 49\n",
            " batch Loss train: 0.05267167463898659\n",
            "i 276\n",
            "epoch 49\n",
            " batch Loss train: 0.06836553663015366\n",
            "i 277\n",
            "epoch 49\n",
            " batch Loss train: 0.03473348915576935\n",
            "i 278\n",
            "epoch 49\n",
            " batch Loss train: 0.05342038720846176\n",
            "i 279\n",
            "epoch 49\n",
            " batch Loss train: 0.059844791889190674\n",
            "i 280\n",
            "epoch 49\n",
            " batch Loss train: 0.07109108567237854\n",
            "i 281\n",
            "epoch 49\n",
            " batch Loss train: 0.039076611399650574\n",
            "i 282\n",
            "epoch 49\n",
            " batch Loss train: 0.04988599196076393\n",
            "i 283\n",
            "epoch 49\n",
            " batch Loss train: 0.05324384942650795\n",
            "i 284\n",
            "epoch 49\n",
            " batch Loss train: 0.04768064618110657\n",
            "i 285\n",
            "epoch 49\n",
            " batch Loss train: 0.05866732820868492\n",
            "i 286\n",
            "epoch 49\n",
            " batch Loss train: 0.029497431591153145\n",
            "i 287\n",
            "epoch 49\n",
            " batch Loss train: 0.05028311535716057\n",
            "i 288\n",
            "epoch 49\n",
            " batch Loss train: 0.04178618639707565\n",
            "i 289\n",
            "epoch 49\n",
            " batch Loss train: 0.054846882820129395\n",
            "i 290\n",
            "epoch 49\n",
            " batch Loss train: 0.08605238795280457\n",
            "i 291\n",
            "epoch 49\n",
            " batch Loss train: 0.05076902359724045\n",
            "i 292\n",
            "epoch 49\n",
            " batch Loss train: 0.061052024364471436\n",
            "i 293\n",
            "epoch 49\n",
            " batch Loss train: 0.05644791200757027\n",
            "i 294\n",
            "epoch 49\n",
            " batch Loss train: 0.05435684323310852\n",
            "i 295\n",
            "epoch 49\n",
            " batch Loss train: 0.04598167538642883\n",
            "i 296\n",
            "epoch 49\n",
            " batch Loss train: 0.06518843024969101\n",
            "i 297\n",
            "epoch 49\n",
            " batch Loss train: 0.048586566001176834\n",
            "i 298\n",
            "epoch 49\n",
            " batch Loss train: 0.06161864101886749\n",
            "i 299\n",
            "epoch 49\n",
            " batch Loss train: 0.06542281061410904\n",
            "i 300\n",
            "epoch 49\n",
            " batch Loss train: 0.05470304563641548\n",
            "i 301\n",
            "epoch 49\n",
            " batch Loss train: 0.05031807720661163\n",
            "i 302\n",
            "epoch 49\n",
            " batch Loss train: 0.048321232199668884\n",
            "i 303\n",
            "epoch 49\n",
            " batch Loss train: 0.06158401072025299\n",
            "i 304\n",
            "epoch 49\n",
            " batch Loss train: 0.05640549585223198\n",
            "i 305\n",
            "epoch 49\n",
            " batch Loss train: 0.05530460178852081\n",
            "i 306\n",
            "epoch 49\n",
            " batch Loss train: 0.06792625039815903\n",
            "i 307\n",
            "epoch 49\n",
            " batch Loss train: 0.043135520070791245\n",
            "i 308\n",
            "epoch 49\n",
            " batch Loss train: 0.052201494574546814\n",
            "i 309\n",
            "epoch 49\n",
            " batch Loss train: 0.058191873133182526\n",
            "i 310\n",
            "epoch 49\n",
            " batch Loss train: 0.03955576941370964\n",
            "i 311\n",
            "epoch 49\n",
            " batch Loss train: 0.054681599140167236\n",
            "i 312\n",
            "epoch 49\n",
            " batch Loss train: 0.050497956573963165\n",
            "i 313\n",
            "epoch 49\n",
            " batch Loss train: 0.061634354293346405\n",
            "i 314\n",
            "epoch 49\n",
            " batch Loss train: 0.053316984325647354\n",
            "i 315\n",
            "epoch 49\n",
            " batch Loss train: 0.057951439172029495\n",
            "i 316\n",
            "epoch 49\n",
            " batch Loss train: 0.03724546730518341\n",
            "i 317\n",
            "epoch 49\n",
            " batch Loss train: 0.06570836156606674\n",
            "i 318\n",
            "epoch 49\n",
            " batch Loss train: 0.03676198795437813\n",
            "i 319\n",
            "epoch 49\n",
            " batch Loss train: 0.08669888228178024\n",
            "i 320\n",
            "epoch 49\n",
            " batch Loss train: 0.06595827639102936\n",
            "i 321\n",
            "epoch 49\n",
            " batch Loss train: 0.06304290145635605\n",
            "i 322\n",
            "epoch 49\n",
            " batch Loss train: 0.07306112349033356\n",
            "i 323\n",
            "epoch 49\n",
            " batch Loss train: 0.04483876749873161\n",
            "i 324\n",
            "epoch 49\n",
            " batch Loss train: 0.04689255356788635\n",
            "i 325\n",
            "epoch 49\n",
            " batch Loss train: 0.0385708324611187\n",
            "i 326\n",
            "epoch 49\n",
            " batch Loss train: 0.04541241005063057\n",
            "i 327\n",
            "epoch 49\n",
            " batch Loss train: 0.058553021401166916\n",
            "i 328\n",
            "epoch 49\n",
            " batch Loss train: 0.05926254764199257\n",
            "i 329\n",
            "epoch 49\n",
            " batch Loss train: 0.05057818815112114\n",
            "i 330\n",
            "epoch 49\n",
            " batch Loss train: 0.049933794885873795\n",
            "i 331\n",
            "epoch 49\n",
            " batch Loss train: 0.043127913028001785\n",
            "i 332\n",
            "epoch 49\n",
            " batch Loss train: 0.07951471954584122\n",
            "i 333\n",
            "epoch 49\n",
            " batch Loss train: 0.05800341069698334\n",
            "i 334\n",
            "epoch 49\n",
            " batch Loss train: 0.05628548189997673\n",
            "i 335\n",
            "epoch 49\n",
            " batch Loss train: 0.06454760581254959\n",
            "i 336\n",
            "epoch 49\n",
            " batch Loss train: 0.04947301372885704\n",
            "i 337\n",
            "epoch 49\n",
            " batch Loss train: 0.06079376861453056\n",
            "i 338\n",
            "epoch 49\n",
            " batch Loss train: 0.05277835205197334\n",
            "i 339\n",
            "epoch 49\n",
            " batch Loss train: 0.05518530309200287\n",
            "i 340\n",
            "epoch 49\n",
            " batch Loss train: 0.06742978096008301\n",
            "i 341\n",
            "epoch 49\n",
            " batch Loss train: 0.04903791844844818\n",
            "i 342\n",
            "epoch 49\n",
            " batch Loss train: 0.04695020616054535\n",
            "i 343\n",
            "epoch 49\n",
            " batch Loss train: 0.0506829209625721\n",
            "i 344\n",
            "epoch 49\n",
            " batch Loss train: 0.07992684096097946\n",
            "i 345\n",
            "epoch 49\n",
            " batch Loss train: 0.0510636642575264\n",
            "i 346\n",
            "epoch 49\n",
            " batch Loss train: 0.07419826835393906\n",
            "i 347\n",
            "epoch 49\n",
            " batch Loss train: 0.05613860860466957\n",
            "i 348\n",
            "epoch 49\n",
            " batch Loss train: 0.06805290281772614\n",
            "i 349\n",
            "epoch 49\n",
            " batch Loss train: 0.06234036386013031\n",
            "i 350\n",
            "epoch 49\n",
            " batch Loss train: 0.04376933351159096\n",
            "i 351\n",
            "epoch 49\n",
            " batch Loss train: 0.05701829493045807\n",
            "i 352\n",
            "epoch 49\n",
            " batch Loss train: 0.04460974410176277\n",
            "i 353\n",
            "epoch 49\n",
            " batch Loss train: 0.041833508759737015\n",
            "i 354\n",
            "epoch 49\n",
            " batch Loss train: 0.06101379543542862\n",
            "i 355\n",
            "epoch 49\n",
            " batch Loss train: 0.05689491331577301\n",
            "i 356\n",
            "epoch 49\n",
            " batch Loss train: 0.07271989434957504\n",
            "i 357\n",
            "epoch 49\n",
            " batch Loss train: 0.05904502794146538\n",
            "i 358\n",
            "epoch 49\n",
            " batch Loss train: 0.08992914855480194\n",
            "i 359\n",
            "epoch 49\n",
            " batch Loss train: 0.04380258917808533\n",
            "i 360\n",
            "epoch 49\n",
            " batch Loss train: 0.06995956599712372\n",
            "i 361\n",
            "epoch 49\n",
            " batch Loss train: 0.054551027715206146\n",
            "i 362\n",
            "epoch 49\n",
            " batch Loss train: 0.06214084476232529\n",
            "i 363\n",
            "epoch 49\n",
            " batch Loss train: 0.039395999163389206\n",
            "i 364\n",
            "epoch 49\n",
            " batch Loss train: 0.056337691843509674\n",
            "i 365\n",
            "epoch 49\n",
            " batch Loss train: 0.051137059926986694\n",
            "i 366\n",
            "epoch 49\n",
            " batch Loss train: 0.06291020661592484\n",
            "i 367\n",
            "epoch 49\n",
            " batch Loss train: 0.04809808358550072\n",
            "i 368\n",
            "epoch 49\n",
            " batch Loss train: 0.057168472558259964\n",
            "i 369\n",
            "epoch 49\n",
            " batch Loss train: 0.05931132286787033\n",
            "i 370\n",
            "epoch 49\n",
            " batch Loss train: 0.045375432819128036\n",
            "i 371\n",
            "epoch 49\n",
            " batch Loss train: 0.07008036226034164\n",
            "i 372\n",
            "epoch 49\n",
            " batch Loss train: 0.05577513203024864\n",
            "i 373\n",
            "epoch 49\n",
            " batch Loss train: 0.06653387099504471\n",
            "i 374\n",
            "epoch 49\n",
            " batch Loss train: 0.05974594131112099\n",
            "i 375\n",
            "epoch 49\n",
            " batch Loss train: 0.06749585270881653\n",
            "i 376\n",
            "epoch 49\n",
            " batch Loss train: 0.04481177777051926\n",
            "i 377\n",
            "epoch 49\n",
            " batch Loss train: 0.045518625527620316\n",
            "i 378\n",
            "epoch 49\n",
            " batch Loss train: 0.056035928428173065\n",
            "i 379\n",
            "epoch 49\n",
            " batch Loss train: 0.05621710792183876\n",
            "i 380\n",
            "epoch 49\n",
            " batch Loss train: 0.05440785363316536\n",
            "i 381\n",
            "epoch 49\n",
            " batch Loss train: 0.07102800160646439\n",
            "i 382\n",
            "epoch 49\n",
            " batch Loss train: 0.06109248101711273\n",
            "i 383\n",
            "epoch 49\n",
            " batch Loss train: 0.07291743904352188\n",
            "i 384\n",
            "epoch 49\n",
            " batch Loss train: 0.036049555987119675\n",
            "i 385\n",
            "epoch 49\n",
            " batch Loss train: 0.05593330040574074\n",
            "i 386\n",
            "epoch 49\n",
            " batch Loss train: 0.06082994118332863\n",
            "i 387\n",
            "epoch 49\n",
            " batch Loss train: 0.06470721960067749\n",
            "i 388\n",
            "epoch 49\n",
            " batch Loss train: 0.0702948197722435\n",
            "i 389\n",
            "epoch 49\n",
            " batch Loss train: 0.05279925838112831\n",
            "i 390\n",
            "epoch 49\n",
            " batch Loss train: 0.06637058407068253\n",
            "i 391\n",
            "epoch 49\n",
            " batch Loss train: 0.06466639041900635\n",
            "i 392\n",
            "epoch 49\n",
            " batch Loss train: 0.06520868092775345\n",
            "i 393\n",
            "epoch 49\n",
            " batch Loss train: 0.045955099165439606\n",
            "i 394\n",
            "epoch 49\n",
            " batch Loss train: 0.05871818587183952\n",
            "i 395\n",
            "epoch 49\n",
            " batch Loss train: 0.04518595710396767\n",
            "i 396\n",
            "epoch 49\n",
            " batch Loss train: 0.052096571773290634\n",
            "i 397\n",
            "epoch 49\n",
            " batch Loss train: 0.06376433372497559\n",
            "i 398\n",
            "epoch 49\n",
            " batch Loss train: 0.05035894364118576\n",
            "i 399\n",
            "epoch 49\n",
            " batch Loss train: 0.05018683895468712\n",
            "i 400\n",
            "epoch 49\n",
            " batch Loss train: 0.04497179761528969\n",
            "i 401\n",
            "epoch 49\n",
            " batch Loss train: 0.062089502811431885\n",
            "i 402\n",
            "epoch 49\n",
            " batch Loss train: 0.06157665327191353\n",
            "i 403\n",
            "epoch 49\n",
            " batch Loss train: 0.07064584642648697\n",
            "i 404\n",
            "epoch 49\n",
            " batch Loss train: 0.05079919472336769\n",
            "i 405\n",
            "epoch 49\n",
            " batch Loss train: 0.06476660817861557\n",
            "i 406\n",
            "epoch 49\n",
            " batch Loss train: 0.05216538906097412\n",
            "i 407\n",
            "epoch 49\n",
            " batch Loss train: 0.08110769838094711\n",
            "i 408\n",
            "epoch 49\n",
            " batch Loss train: 0.04229682683944702\n",
            "i 409\n",
            "epoch 49\n",
            " batch Loss train: 0.060902997851371765\n",
            "i 410\n",
            "epoch 49\n",
            " batch Loss train: 0.051543328911066055\n",
            "i 411\n",
            "epoch 49\n",
            " batch Loss train: 0.05976494401693344\n",
            "i 412\n",
            "epoch 49\n",
            " batch Loss train: 0.07583355903625488\n",
            "i 413\n",
            "epoch 49\n",
            " batch Loss train: 0.05679859593510628\n",
            "i 414\n",
            "epoch 49\n",
            " batch Loss train: 0.05170697346329689\n",
            "i 415\n",
            "epoch 49\n",
            " batch Loss train: 0.07500933855772018\n",
            "i 416\n",
            "epoch 49\n",
            " batch Loss train: 0.06132296100258827\n",
            "i 417\n",
            "epoch 49\n",
            " batch Loss train: 0.05611921101808548\n",
            "i 418\n",
            "epoch 49\n",
            " batch Loss train: 0.09709151834249496\n",
            "i 419\n",
            "epoch 49\n",
            " batch Loss train: 0.04799199476838112\n",
            "i 420\n",
            "epoch 49\n",
            " batch Loss train: 0.06388068199157715\n",
            "i 421\n",
            "epoch 49\n",
            " batch Loss train: 0.07648221403360367\n",
            "i 422\n",
            "epoch 49\n",
            " batch Loss train: 0.05290135368704796\n",
            "i 423\n",
            "epoch 49\n",
            " batch Loss train: 0.03799011558294296\n",
            "i 424\n",
            "epoch 49\n",
            " batch Loss train: 0.04508577287197113\n",
            "i 425\n",
            "epoch 49\n",
            " batch Loss train: 0.04520706832408905\n",
            "i 426\n",
            "epoch 49\n",
            " batch Loss train: 0.06748268008232117\n",
            "i 427\n",
            "epoch 49\n",
            " batch Loss train: 0.05131705477833748\n",
            "i 428\n",
            "epoch 49\n",
            " batch Loss train: 0.08094404637813568\n",
            "i 429\n",
            "epoch 49\n",
            " batch Loss train: 0.053007811307907104\n",
            "i 430\n",
            "epoch 49\n",
            " batch Loss train: 0.07288520783185959\n",
            "i 431\n",
            "epoch 49\n",
            " batch Loss train: 0.03890318423509598\n",
            "i 432\n",
            "epoch 49\n",
            " batch Loss train: 0.06745018810033798\n",
            "i 433\n",
            "epoch 49\n",
            " batch Loss train: 0.050062403082847595\n",
            "i 434\n",
            "epoch 49\n",
            " batch Loss train: 0.0691513791680336\n",
            "i 435\n",
            "epoch 49\n",
            " batch Loss train: 0.058033231645822525\n",
            "i 436\n",
            "epoch 49\n",
            " batch Loss train: 0.07058794796466827\n",
            "i 437\n",
            "epoch 49\n",
            " batch Loss train: 0.05119837075471878\n",
            "i 438\n",
            "epoch 49\n",
            " batch Loss train: 0.046944983303546906\n",
            "i 439\n",
            "epoch 49\n",
            " batch Loss train: 0.05699877813458443\n",
            "i 440\n",
            "epoch 49\n",
            " batch Loss train: 0.05454558506608009\n",
            "i 441\n",
            "epoch 49\n",
            " batch Loss train: 0.06124553456902504\n",
            "i 442\n",
            "epoch 49\n",
            " batch Loss train: 0.04941016808152199\n",
            "i 443\n",
            "epoch 49\n",
            " batch Loss train: 0.0592973567545414\n",
            "i 444\n",
            "epoch 49\n",
            " batch Loss train: 0.07532773911952972\n",
            "i 445\n",
            "epoch 49\n",
            " batch Loss train: 0.05616964027285576\n",
            "total epoch Loss train: tensor(0.0562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 50\n",
            " batch Loss train: 0.05201032757759094\n",
            "i 1\n",
            "epoch 50\n",
            " batch Loss train: 0.0523538701236248\n",
            "i 2\n",
            "epoch 50\n",
            " batch Loss train: 0.0729815810918808\n",
            "i 3\n",
            "epoch 50\n",
            " batch Loss train: 0.0415915809571743\n",
            "i 4\n",
            "epoch 50\n",
            " batch Loss train: 0.027259904891252518\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 50\n",
            " batch Loss train: 0.039785683155059814\n",
            "i 6\n",
            "epoch 50\n",
            " batch Loss train: 0.05065523087978363\n",
            "i 7\n",
            "epoch 50\n",
            " batch Loss train: 0.047818366438150406\n",
            "i 8\n",
            "epoch 50\n",
            " batch Loss train: 0.03690161928534508\n",
            "i 9\n",
            "epoch 50\n",
            " batch Loss train: 0.043420594185590744\n",
            "i 10\n",
            "epoch 50\n",
            " batch Loss train: 0.04281352460384369\n",
            "i 11\n",
            "epoch 50\n",
            " batch Loss train: 0.05459708720445633\n",
            "i 12\n",
            "epoch 50\n",
            " batch Loss train: 0.03981609269976616\n",
            "i 13\n",
            "epoch 50\n",
            " batch Loss train: 0.043802689760923386\n",
            "i 14\n",
            "epoch 50\n",
            " batch Loss train: 0.03744988888502121\n",
            "i 15\n",
            "epoch 50\n",
            " batch Loss train: 0.05024275556206703\n",
            "i 16\n",
            "epoch 50\n",
            " batch Loss train: 0.037223752588033676\n",
            "i 17\n",
            "epoch 50\n",
            " batch Loss train: 0.04006525129079819\n",
            "i 18\n",
            "epoch 50\n",
            " batch Loss train: 0.05158424749970436\n",
            "i 19\n",
            "epoch 50\n",
            " batch Loss train: 0.05254293978214264\n",
            "i 20\n",
            "epoch 50\n",
            " batch Loss train: 0.03621561825275421\n",
            "i 21\n",
            "epoch 50\n",
            " batch Loss train: 0.03772955387830734\n",
            "i 22\n",
            "epoch 50\n",
            " batch Loss train: 0.037878651171922684\n",
            "i 23\n",
            "epoch 50\n",
            " batch Loss train: 0.06327953934669495\n",
            "i 24\n",
            "epoch 50\n",
            " batch Loss train: 0.0783759355545044\n",
            "i 25\n",
            "epoch 50\n",
            " batch Loss train: 0.06391208618879318\n",
            "i 26\n",
            "epoch 50\n",
            " batch Loss train: 0.04047655314207077\n",
            "i 27\n",
            "epoch 50\n",
            " batch Loss train: 0.0359460823237896\n",
            "i 28\n",
            "epoch 50\n",
            " batch Loss train: 0.043444350361824036\n",
            "i 29\n",
            "epoch 50\n",
            " batch Loss train: 0.03495805338025093\n",
            "i 30\n",
            "epoch 50\n",
            " batch Loss train: 0.03565859794616699\n",
            "i 31\n",
            "epoch 50\n",
            " batch Loss train: 0.05005988851189613\n",
            "i 32\n",
            "epoch 50\n",
            " batch Loss train: 0.07145002484321594\n",
            "i 33\n",
            "epoch 50\n",
            " batch Loss train: 0.05817908048629761\n",
            "i 34\n",
            "epoch 50\n",
            " batch Loss train: 0.04507995396852493\n",
            "i 35\n",
            "epoch 50\n",
            " batch Loss train: 0.05054577812552452\n",
            "i 36\n",
            "epoch 50\n",
            " batch Loss train: 0.047379642724990845\n",
            "i 37\n",
            "epoch 50\n",
            " batch Loss train: 0.04136988893151283\n",
            "i 38\n",
            "epoch 50\n",
            " batch Loss train: 0.05276582017540932\n",
            "i 39\n",
            "epoch 50\n",
            " batch Loss train: 0.04180546849966049\n",
            "i 40\n",
            "epoch 50\n",
            " batch Loss train: 0.040669772773981094\n",
            "i 41\n",
            "epoch 50\n",
            " batch Loss train: 0.026493454352021217\n",
            "i 42\n",
            "epoch 50\n",
            " batch Loss train: 0.047155916690826416\n",
            "i 43\n",
            "epoch 50\n",
            " batch Loss train: 0.033517222851514816\n",
            "i 44\n",
            "epoch 50\n",
            " batch Loss train: 0.029696907848119736\n",
            "i 45\n",
            "epoch 50\n",
            " batch Loss train: 0.03299802169203758\n",
            "i 46\n",
            "epoch 50\n",
            " batch Loss train: 0.04729708656668663\n",
            "i 47\n",
            "epoch 50\n",
            " batch Loss train: 0.050451163202524185\n",
            "i 48\n",
            "epoch 50\n",
            " batch Loss train: 0.041756514459848404\n",
            "i 49\n",
            "epoch 50\n",
            " batch Loss train: 0.06300430744886398\n",
            "i 50\n",
            "epoch 50\n",
            " batch Loss train: 0.042104221880435944\n",
            "i 51\n",
            "epoch 50\n",
            " batch Loss train: 0.048359133303165436\n",
            "i 52\n",
            "epoch 50\n",
            " batch Loss train: 0.03647696599364281\n",
            "i 53\n",
            "epoch 50\n",
            " batch Loss train: 0.04396746680140495\n",
            "i 54\n",
            "epoch 50\n",
            " batch Loss train: 0.030274471268057823\n",
            "i 55\n",
            "epoch 50\n",
            " batch Loss train: 0.04301052913069725\n",
            "i 56\n",
            "epoch 50\n",
            " batch Loss train: 0.05599365755915642\n",
            "i 57\n",
            "epoch 50\n",
            " batch Loss train: 0.05248626694083214\n",
            "i 58\n",
            "epoch 50\n",
            " batch Loss train: 0.03522148355841637\n",
            "i 59\n",
            "epoch 50\n",
            " batch Loss train: 0.05185771360993385\n",
            "i 60\n",
            "epoch 50\n",
            " batch Loss train: 0.06381040811538696\n",
            "i 61\n",
            "epoch 50\n",
            " batch Loss train: 0.03878287971019745\n",
            "i 62\n",
            "epoch 50\n",
            " batch Loss train: 0.05817048251628876\n",
            "i 63\n",
            "epoch 50\n",
            " batch Loss train: 0.0395633801817894\n",
            "i 64\n",
            "epoch 50\n",
            " batch Loss train: 0.04146768897771835\n",
            "i 65\n",
            "epoch 50\n",
            " batch Loss train: 0.06030233949422836\n",
            "i 66\n",
            "epoch 50\n",
            " batch Loss train: 0.030592715367674828\n",
            "i 67\n",
            "epoch 50\n",
            " batch Loss train: 0.061769649386405945\n",
            "i 68\n",
            "epoch 50\n",
            " batch Loss train: 0.07496637105941772\n",
            "i 69\n",
            "epoch 50\n",
            " batch Loss train: 0.03403623774647713\n",
            "i 70\n",
            "epoch 50\n",
            " batch Loss train: 0.055860839784145355\n",
            "i 71\n",
            "epoch 50\n",
            " batch Loss train: 0.04937431961297989\n",
            "i 72\n",
            "epoch 50\n",
            " batch Loss train: 0.04306822270154953\n",
            "i 73\n",
            "epoch 50\n",
            " batch Loss train: 0.050260644406080246\n",
            "i 74\n",
            "epoch 50\n",
            " batch Loss train: 0.04002337530255318\n",
            "i 75\n",
            "epoch 50\n",
            " batch Loss train: 0.04228772968053818\n",
            "i 76\n",
            "epoch 50\n",
            " batch Loss train: 0.048376213759183884\n",
            "i 77\n",
            "epoch 50\n",
            " batch Loss train: 0.06199968606233597\n",
            "i 78\n",
            "epoch 50\n",
            " batch Loss train: 0.057734109461307526\n",
            "i 79\n",
            "epoch 50\n",
            " batch Loss train: 0.05584140494465828\n",
            "i 80\n",
            "epoch 50\n",
            " batch Loss train: 0.03754780814051628\n",
            "i 81\n",
            "epoch 50\n",
            " batch Loss train: 0.05106798931956291\n",
            "i 82\n",
            "epoch 50\n",
            " batch Loss train: 0.04932738095521927\n",
            "i 83\n",
            "epoch 50\n",
            " batch Loss train: 0.06253673136234283\n",
            "i 84\n",
            "epoch 50\n",
            " batch Loss train: 0.04118195176124573\n",
            "i 85\n",
            "epoch 50\n",
            " batch Loss train: 0.05683530122041702\n",
            "i 86\n",
            "epoch 50\n",
            " batch Loss train: 0.0738760307431221\n",
            "i 87\n",
            "epoch 50\n",
            " batch Loss train: 0.03196752443909645\n",
            "i 88\n",
            "epoch 50\n",
            " batch Loss train: 0.0531180202960968\n",
            "i 89\n",
            "epoch 50\n",
            " batch Loss train: 0.04148043319582939\n",
            "i 90\n",
            "epoch 50\n",
            " batch Loss train: 0.04077157378196716\n",
            "i 91\n",
            "epoch 50\n",
            " batch Loss train: 0.04058770835399628\n",
            "i 92\n",
            "epoch 50\n",
            " batch Loss train: 0.04891083389520645\n",
            "i 93\n",
            "epoch 50\n",
            " batch Loss train: 0.053589582443237305\n",
            "i 94\n",
            "epoch 50\n",
            " batch Loss train: 0.0536465086042881\n",
            "i 95\n",
            "epoch 50\n",
            " batch Loss train: 0.04468041658401489\n",
            "i 96\n",
            "epoch 50\n",
            " batch Loss train: 0.04555673152208328\n",
            "i 97\n",
            "epoch 50\n",
            " batch Loss train: 0.05641154199838638\n",
            "i 98\n",
            "epoch 50\n",
            " batch Loss train: 0.043238863348960876\n",
            "i 99\n",
            "epoch 50\n",
            " batch Loss train: 0.04123006761074066\n",
            "i 100\n",
            "epoch 50\n",
            " batch Loss train: 0.04687004163861275\n",
            "i 101\n",
            "epoch 50\n",
            " batch Loss train: 0.06713147461414337\n",
            "i 102\n",
            "epoch 50\n",
            " batch Loss train: 0.058960482478141785\n",
            "i 103\n",
            "epoch 50\n",
            " batch Loss train: 0.038395073264837265\n",
            "i 104\n",
            "epoch 50\n",
            " batch Loss train: 0.049641672521829605\n",
            "i 105\n",
            "epoch 50\n",
            " batch Loss train: 0.04941451549530029\n",
            "i 106\n",
            "epoch 50\n",
            " batch Loss train: 0.05887280032038689\n",
            "i 107\n",
            "epoch 50\n",
            " batch Loss train: 0.04661182314157486\n",
            "i 108\n",
            "epoch 50\n",
            " batch Loss train: 0.0378875769674778\n",
            "i 109\n",
            "epoch 50\n",
            " batch Loss train: 0.04953650012612343\n",
            "i 110\n",
            "epoch 50\n",
            " batch Loss train: 0.04759516566991806\n",
            "i 111\n",
            "epoch 50\n",
            " batch Loss train: 0.046391621232032776\n",
            "i 112\n",
            "epoch 50\n",
            " batch Loss train: 0.05752035975456238\n",
            "i 113\n",
            "epoch 50\n",
            " batch Loss train: 0.047621503472328186\n",
            "i 114\n",
            "epoch 50\n",
            " batch Loss train: 0.04630488157272339\n",
            "i 115\n",
            "epoch 50\n",
            " batch Loss train: 0.04157203435897827\n",
            "i 116\n",
            "epoch 50\n",
            " batch Loss train: 0.05398866534233093\n",
            "i 117\n",
            "epoch 50\n",
            " batch Loss train: 0.0720478743314743\n",
            "i 118\n",
            "epoch 50\n",
            " batch Loss train: 0.03908289596438408\n",
            "i 119\n",
            "epoch 50\n",
            " batch Loss train: 0.053766313940286636\n",
            "i 120\n",
            "epoch 50\n",
            " batch Loss train: 0.04987131804227829\n",
            "i 121\n",
            "epoch 50\n",
            " batch Loss train: 0.049138765782117844\n",
            "i 122\n",
            "epoch 50\n",
            " batch Loss train: 0.05526452139019966\n",
            "i 123\n",
            "epoch 50\n",
            " batch Loss train: 0.04565348103642464\n",
            "i 124\n",
            "epoch 50\n",
            " batch Loss train: 0.04362047463655472\n",
            "i 125\n",
            "epoch 50\n",
            " batch Loss train: 0.05237104743719101\n",
            "i 126\n",
            "epoch 50\n",
            " batch Loss train: 0.057939644902944565\n",
            "i 127\n",
            "epoch 50\n",
            " batch Loss train: 0.05108091980218887\n",
            "i 128\n",
            "epoch 50\n",
            " batch Loss train: 0.04970308765769005\n",
            "i 129\n",
            "epoch 50\n",
            " batch Loss train: 0.05095185711979866\n",
            "i 130\n",
            "epoch 50\n",
            " batch Loss train: 0.04920278117060661\n",
            "i 131\n",
            "epoch 50\n",
            " batch Loss train: 0.04315916821360588\n",
            "i 132\n",
            "epoch 50\n",
            " batch Loss train: 0.05856632813811302\n",
            "i 133\n",
            "epoch 50\n",
            " batch Loss train: 0.05266719311475754\n",
            "i 134\n",
            "epoch 50\n",
            " batch Loss train: 0.05325276777148247\n",
            "i 135\n",
            "epoch 50\n",
            " batch Loss train: 0.05520409345626831\n",
            "i 136\n",
            "epoch 50\n",
            " batch Loss train: 0.05670491233468056\n",
            "i 137\n",
            "epoch 50\n",
            " batch Loss train: 0.033563897013664246\n",
            "i 138\n",
            "epoch 50\n",
            " batch Loss train: 0.05855459347367287\n",
            "i 139\n",
            "epoch 50\n",
            " batch Loss train: 0.04279136285185814\n",
            "i 140\n",
            "epoch 50\n",
            " batch Loss train: 0.04539522901177406\n",
            "i 141\n",
            "epoch 50\n",
            " batch Loss train: 0.05957403406500816\n",
            "i 142\n",
            "epoch 50\n",
            " batch Loss train: 0.04738393425941467\n",
            "i 143\n",
            "epoch 50\n",
            " batch Loss train: 0.04692625254392624\n",
            "i 144\n",
            "epoch 50\n",
            " batch Loss train: 0.040498845279216766\n",
            "i 145\n",
            "epoch 50\n",
            " batch Loss train: 0.032216839492321014\n",
            "i 146\n",
            "epoch 50\n",
            " batch Loss train: 0.04177454859018326\n",
            "i 147\n",
            "epoch 50\n",
            " batch Loss train: 0.0553775392472744\n",
            "i 148\n",
            "epoch 50\n",
            " batch Loss train: 0.05002625659108162\n",
            "i 149\n",
            "epoch 50\n",
            " batch Loss train: 0.03739621490240097\n",
            "i 150\n",
            "epoch 50\n",
            " batch Loss train: 0.042929138988256454\n",
            "i 151\n",
            "epoch 50\n",
            " batch Loss train: 0.07716953754425049\n",
            "i 152\n",
            "epoch 50\n",
            " batch Loss train: 0.039356037974357605\n",
            "i 153\n",
            "epoch 50\n",
            " batch Loss train: 0.0315290242433548\n",
            "i 154\n",
            "epoch 50\n",
            " batch Loss train: 0.05868544429540634\n",
            "i 155\n",
            "epoch 50\n",
            " batch Loss train: 0.05114549770951271\n",
            "i 156\n",
            "epoch 50\n",
            " batch Loss train: 0.0533377043902874\n",
            "i 157\n",
            "epoch 50\n",
            " batch Loss train: 0.05408954992890358\n",
            "i 158\n",
            "epoch 50\n",
            " batch Loss train: 0.05689733102917671\n",
            "i 159\n",
            "epoch 50\n",
            " batch Loss train: 0.057242631912231445\n",
            "i 160\n",
            "epoch 50\n",
            " batch Loss train: 0.059955205768346786\n",
            "i 161\n",
            "epoch 50\n",
            " batch Loss train: 0.06602799892425537\n",
            "i 162\n",
            "epoch 50\n",
            " batch Loss train: 0.049636054784059525\n",
            "i 163\n",
            "epoch 50\n",
            " batch Loss train: 0.06963041424751282\n",
            "i 164\n",
            "epoch 50\n",
            " batch Loss train: 0.04936527460813522\n",
            "i 165\n",
            "epoch 50\n",
            " batch Loss train: 0.085130974650383\n",
            "i 166\n",
            "epoch 50\n",
            " batch Loss train: 0.04877210780978203\n",
            "i 167\n",
            "epoch 50\n",
            " batch Loss train: 0.0483308844268322\n",
            "i 168\n",
            "epoch 50\n",
            " batch Loss train: 0.054753273725509644\n",
            "i 169\n",
            "epoch 50\n",
            " batch Loss train: 0.03877762332558632\n",
            "i 170\n",
            "epoch 50\n",
            " batch Loss train: 0.061077870428562164\n",
            "i 171\n",
            "epoch 50\n",
            " batch Loss train: 0.0458315871655941\n",
            "i 172\n",
            "epoch 50\n",
            " batch Loss train: 0.044113900512456894\n",
            "i 173\n",
            "epoch 50\n",
            " batch Loss train: 0.059690702706575394\n",
            "i 174\n",
            "epoch 50\n",
            " batch Loss train: 0.05214504152536392\n",
            "i 175\n",
            "epoch 50\n",
            " batch Loss train: 0.07218001782894135\n",
            "i 176\n",
            "epoch 50\n",
            " batch Loss train: 0.06502096354961395\n",
            "i 177\n",
            "epoch 50\n",
            " batch Loss train: 0.04379725083708763\n",
            "i 178\n",
            "epoch 50\n",
            " batch Loss train: 0.053409673273563385\n",
            "i 179\n",
            "epoch 50\n",
            " batch Loss train: 0.06065606698393822\n",
            "i 180\n",
            "epoch 50\n",
            " batch Loss train: 0.06393394619226456\n",
            "i 181\n",
            "epoch 50\n",
            " batch Loss train: 0.06065313518047333\n",
            "i 182\n",
            "epoch 50\n",
            " batch Loss train: 0.058220669627189636\n",
            "i 183\n",
            "epoch 50\n",
            " batch Loss train: 0.05606207251548767\n",
            "i 184\n",
            "epoch 50\n",
            " batch Loss train: 0.04225196689367294\n",
            "i 185\n",
            "epoch 50\n",
            " batch Loss train: 0.05713975057005882\n",
            "i 186\n",
            "epoch 50\n",
            " batch Loss train: 0.039128996431827545\n",
            "i 187\n",
            "epoch 50\n",
            " batch Loss train: 0.0639737918972969\n",
            "i 188\n",
            "epoch 50\n",
            " batch Loss train: 0.04620148241519928\n",
            "i 189\n",
            "epoch 50\n",
            " batch Loss train: 0.04830937460064888\n",
            "i 190\n",
            "epoch 50\n",
            " batch Loss train: 0.04914277419447899\n",
            "i 191\n",
            "epoch 50\n",
            " batch Loss train: 0.05935341864824295\n",
            "i 192\n",
            "epoch 50\n",
            " batch Loss train: 0.05886998027563095\n",
            "i 193\n",
            "epoch 50\n",
            " batch Loss train: 0.05147697404026985\n",
            "i 194\n",
            "epoch 50\n",
            " batch Loss train: 0.04400653392076492\n",
            "i 195\n",
            "epoch 50\n",
            " batch Loss train: 0.055096399039030075\n",
            "i 196\n",
            "epoch 50\n",
            " batch Loss train: 0.04756481945514679\n",
            "i 197\n",
            "epoch 50\n",
            " batch Loss train: 0.05870997533202171\n",
            "i 198\n",
            "epoch 50\n",
            " batch Loss train: 0.05697682499885559\n",
            "i 199\n",
            "epoch 50\n",
            " batch Loss train: 0.06057431548833847\n",
            "i 200\n",
            "epoch 50\n",
            " batch Loss train: 0.039048049598932266\n",
            "i 201\n",
            "epoch 50\n",
            " batch Loss train: 0.0640222355723381\n",
            "i 202\n",
            "epoch 50\n",
            " batch Loss train: 0.07501838356256485\n",
            "i 203\n",
            "epoch 50\n",
            " batch Loss train: 0.05546848103404045\n",
            "i 204\n",
            "epoch 50\n",
            " batch Loss train: 0.08151186257600784\n",
            "i 205\n",
            "epoch 50\n",
            " batch Loss train: 0.0469222255051136\n",
            "i 206\n",
            "epoch 50\n",
            " batch Loss train: 0.044905588030815125\n",
            "i 207\n",
            "epoch 50\n",
            " batch Loss train: 0.04737948626279831\n",
            "i 208\n",
            "epoch 50\n",
            " batch Loss train: 0.059374623000621796\n",
            "i 209\n",
            "epoch 50\n",
            " batch Loss train: 0.03148975595831871\n",
            "i 210\n",
            "epoch 50\n",
            " batch Loss train: 0.05369642376899719\n",
            "i 211\n",
            "epoch 50\n",
            " batch Loss train: 0.06414218246936798\n",
            "i 212\n",
            "epoch 50\n",
            " batch Loss train: 0.05895006284117699\n",
            "i 213\n",
            "epoch 50\n",
            " batch Loss train: 0.04417354613542557\n",
            "i 214\n",
            "epoch 50\n",
            " batch Loss train: 0.05326850712299347\n",
            "i 215\n",
            "epoch 50\n",
            " batch Loss train: 0.06700028479099274\n",
            "i 216\n",
            "epoch 50\n",
            " batch Loss train: 0.05445433780550957\n",
            "i 217\n",
            "epoch 50\n",
            " batch Loss train: 0.04075425863265991\n",
            "i 218\n",
            "epoch 50\n",
            " batch Loss train: 0.044405218213796616\n",
            "i 219\n",
            "epoch 50\n",
            " batch Loss train: 0.044291626662015915\n",
            "i 220\n",
            "epoch 50\n",
            " batch Loss train: 0.05751923844218254\n",
            "i 221\n",
            "epoch 50\n",
            " batch Loss train: 0.06505756825208664\n",
            "i 222\n",
            "epoch 50\n",
            " batch Loss train: 0.04822053015232086\n",
            "i 223\n",
            "epoch 50\n",
            " batch Loss train: 0.061829403042793274\n",
            "i 224\n",
            "epoch 50\n",
            " batch Loss train: 0.09113957732915878\n",
            "i 225\n",
            "epoch 50\n",
            " batch Loss train: 0.054297830909490585\n",
            "i 226\n",
            "epoch 50\n",
            " batch Loss train: 0.06698457896709442\n",
            "i 227\n",
            "epoch 50\n",
            " batch Loss train: 0.03820426017045975\n",
            "i 228\n",
            "epoch 50\n",
            " batch Loss train: 0.052226971834897995\n",
            "i 229\n",
            "epoch 50\n",
            " batch Loss train: 0.06340661644935608\n",
            "i 230\n",
            "epoch 50\n",
            " batch Loss train: 0.06911124289035797\n",
            "i 231\n",
            "epoch 50\n",
            " batch Loss train: 0.04893864318728447\n",
            "i 232\n",
            "epoch 50\n",
            " batch Loss train: 0.04428544640541077\n",
            "i 233\n",
            "epoch 50\n",
            " batch Loss train: 0.04285590350627899\n",
            "i 234\n",
            "epoch 50\n",
            " batch Loss train: 0.03970763459801674\n",
            "i 235\n",
            "epoch 50\n",
            " batch Loss train: 0.062003929167985916\n",
            "i 236\n",
            "epoch 50\n",
            " batch Loss train: 0.059203438460826874\n",
            "i 237\n",
            "epoch 50\n",
            " batch Loss train: 0.05187058076262474\n",
            "i 238\n",
            "epoch 50\n",
            " batch Loss train: 0.03098093345761299\n",
            "i 239\n",
            "epoch 50\n",
            " batch Loss train: 0.07009882479906082\n",
            "i 240\n",
            "epoch 50\n",
            " batch Loss train: 0.05249020457267761\n",
            "i 241\n",
            "epoch 50\n",
            " batch Loss train: 0.05179394409060478\n",
            "i 242\n",
            "epoch 50\n",
            " batch Loss train: 0.05545467883348465\n",
            "i 243\n",
            "epoch 50\n",
            " batch Loss train: 0.05376717075705528\n",
            "i 244\n",
            "epoch 50\n",
            " batch Loss train: 0.09007120132446289\n",
            "i 245\n",
            "epoch 50\n",
            " batch Loss train: 0.06054717302322388\n",
            "i 246\n",
            "epoch 50\n",
            " batch Loss train: 0.06345973163843155\n",
            "i 247\n",
            "epoch 50\n",
            " batch Loss train: 0.041406117379665375\n",
            "i 248\n",
            "epoch 50\n",
            " batch Loss train: 0.0494828075170517\n",
            "i 249\n",
            "epoch 50\n",
            " batch Loss train: 0.04992700740695\n",
            "i 250\n",
            "epoch 50\n",
            " batch Loss train: 0.06367940455675125\n",
            "i 251\n",
            "epoch 50\n",
            " batch Loss train: 0.05346251279115677\n",
            "i 252\n",
            "epoch 50\n",
            " batch Loss train: 0.05703764408826828\n",
            "i 253\n",
            "epoch 50\n",
            " batch Loss train: 0.052654244005680084\n",
            "i 254\n",
            "epoch 50\n",
            " batch Loss train: 0.057551901787519455\n",
            "i 255\n",
            "epoch 50\n",
            " batch Loss train: 0.05336485058069229\n",
            "i 256\n",
            "epoch 50\n",
            " batch Loss train: 0.06909286230802536\n",
            "i 257\n",
            "epoch 50\n",
            " batch Loss train: 0.05539003014564514\n",
            "i 258\n",
            "epoch 50\n",
            " batch Loss train: 0.04417805001139641\n",
            "i 259\n",
            "epoch 50\n",
            " batch Loss train: 0.043004341423511505\n",
            "i 260\n",
            "epoch 50\n",
            " batch Loss train: 0.0725063756108284\n",
            "i 261\n",
            "epoch 50\n",
            " batch Loss train: 0.055273640900850296\n",
            "i 262\n",
            "epoch 50\n",
            " batch Loss train: 0.052921611815690994\n",
            "i 263\n",
            "epoch 50\n",
            " batch Loss train: 0.04806875064969063\n",
            "i 264\n",
            "epoch 50\n",
            " batch Loss train: 0.06236933916807175\n",
            "i 265\n",
            "epoch 50\n",
            " batch Loss train: 0.041947007179260254\n",
            "i 266\n",
            "epoch 50\n",
            " batch Loss train: 0.05627332627773285\n",
            "i 267\n",
            "epoch 50\n",
            " batch Loss train: 0.04805902764201164\n",
            "i 268\n",
            "epoch 50\n",
            " batch Loss train: 0.0625016987323761\n",
            "i 269\n",
            "epoch 50\n",
            " batch Loss train: 0.06163550540804863\n",
            "i 270\n",
            "epoch 50\n",
            " batch Loss train: 0.0617905855178833\n",
            "i 271\n",
            "epoch 50\n",
            " batch Loss train: 0.051168739795684814\n",
            "i 272\n",
            "epoch 50\n",
            " batch Loss train: 0.05224641039967537\n",
            "i 273\n",
            "epoch 50\n",
            " batch Loss train: 0.0364840105175972\n",
            "i 274\n",
            "epoch 50\n",
            " batch Loss train: 0.05350351706147194\n",
            "i 275\n",
            "epoch 50\n",
            " batch Loss train: 0.05754988640546799\n",
            "i 276\n",
            "epoch 50\n",
            " batch Loss train: 0.06342706829309464\n",
            "i 277\n",
            "epoch 50\n",
            " batch Loss train: 0.04312052205204964\n",
            "i 278\n",
            "epoch 50\n",
            " batch Loss train: 0.04887198656797409\n",
            "i 279\n",
            "epoch 50\n",
            " batch Loss train: 0.06194637343287468\n",
            "i 280\n",
            "epoch 50\n",
            " batch Loss train: 0.051638469099998474\n",
            "i 281\n",
            "epoch 50\n",
            " batch Loss train: 0.05629623681306839\n",
            "i 282\n",
            "epoch 50\n",
            " batch Loss train: 0.05856442078948021\n",
            "i 283\n",
            "epoch 50\n",
            " batch Loss train: 0.0743388906121254\n",
            "i 284\n",
            "epoch 50\n",
            " batch Loss train: 0.06470120698213577\n",
            "i 285\n",
            "epoch 50\n",
            " batch Loss train: 0.05645574629306793\n",
            "i 286\n",
            "epoch 50\n",
            " batch Loss train: 0.05423981696367264\n",
            "i 287\n",
            "epoch 50\n",
            " batch Loss train: 0.08588260412216187\n",
            "i 288\n",
            "epoch 50\n",
            " batch Loss train: 0.05875289440155029\n",
            "i 289\n",
            "epoch 50\n",
            " batch Loss train: 0.040665559470653534\n",
            "i 290\n",
            "epoch 50\n",
            " batch Loss train: 0.047491658478975296\n",
            "i 291\n",
            "epoch 50\n",
            " batch Loss train: 0.06380902230739594\n",
            "i 292\n",
            "epoch 50\n",
            " batch Loss train: 0.04484424367547035\n",
            "i 293\n",
            "epoch 50\n",
            " batch Loss train: 0.05191722512245178\n",
            "i 294\n",
            "epoch 50\n",
            " batch Loss train: 0.05919881537556648\n",
            "i 295\n",
            "epoch 50\n",
            " batch Loss train: 0.0376870296895504\n",
            "i 296\n",
            "epoch 50\n",
            " batch Loss train: 0.050590213388204575\n",
            "i 297\n",
            "epoch 50\n",
            " batch Loss train: 0.058236800134181976\n",
            "i 298\n",
            "epoch 50\n",
            " batch Loss train: 0.05088411644101143\n",
            "i 299\n",
            "epoch 50\n",
            " batch Loss train: 0.05667465552687645\n",
            "i 300\n",
            "epoch 50\n",
            " batch Loss train: 0.07982006669044495\n",
            "i 301\n",
            "epoch 50\n",
            " batch Loss train: 0.06650375574827194\n",
            "i 302\n",
            "epoch 50\n",
            " batch Loss train: 0.05084315687417984\n",
            "i 303\n",
            "epoch 50\n",
            " batch Loss train: 0.05480847507715225\n",
            "i 304\n",
            "epoch 50\n",
            " batch Loss train: 0.05078921467065811\n",
            "i 305\n",
            "epoch 50\n",
            " batch Loss train: 0.05897406116127968\n",
            "i 306\n",
            "epoch 50\n",
            " batch Loss train: 0.04794083163142204\n",
            "i 307\n",
            "epoch 50\n",
            " batch Loss train: 0.0665731355547905\n",
            "i 308\n",
            "epoch 50\n",
            " batch Loss train: 0.05186569690704346\n",
            "i 309\n",
            "epoch 50\n",
            " batch Loss train: 0.06165163964033127\n",
            "i 310\n",
            "epoch 50\n",
            " batch Loss train: 0.04734748974442482\n",
            "i 311\n",
            "epoch 50\n",
            " batch Loss train: 0.07446987926959991\n",
            "i 312\n",
            "epoch 50\n",
            " batch Loss train: 0.05197831243276596\n",
            "i 313\n",
            "epoch 50\n",
            " batch Loss train: 0.05020085722208023\n",
            "i 314\n",
            "epoch 50\n",
            " batch Loss train: 0.060599252581596375\n",
            "i 315\n",
            "epoch 50\n",
            " batch Loss train: 0.07887722551822662\n",
            "i 316\n",
            "epoch 50\n",
            " batch Loss train: 0.03876001015305519\n",
            "i 317\n",
            "epoch 50\n",
            " batch Loss train: 0.04695691168308258\n",
            "i 318\n",
            "epoch 50\n",
            " batch Loss train: 0.04823858663439751\n",
            "i 319\n",
            "epoch 50\n",
            " batch Loss train: 0.049785714596509933\n",
            "i 320\n",
            "epoch 50\n",
            " batch Loss train: 0.05473797395825386\n",
            "i 321\n",
            "epoch 50\n",
            " batch Loss train: 0.066179558634758\n",
            "i 322\n",
            "epoch 50\n",
            " batch Loss train: 0.04948526248335838\n",
            "i 323\n",
            "epoch 50\n",
            " batch Loss train: 0.08276432007551193\n",
            "i 324\n",
            "epoch 50\n",
            " batch Loss train: 0.04037713259458542\n",
            "i 325\n",
            "epoch 50\n",
            " batch Loss train: 0.05561147630214691\n",
            "i 326\n",
            "epoch 50\n",
            " batch Loss train: 0.04804264381527901\n",
            "i 327\n",
            "epoch 50\n",
            " batch Loss train: 0.05183491110801697\n",
            "i 328\n",
            "epoch 50\n",
            " batch Loss train: 0.06365048885345459\n",
            "i 329\n",
            "epoch 50\n",
            " batch Loss train: 0.057177502661943436\n",
            "i 330\n",
            "epoch 50\n",
            " batch Loss train: 0.05027780309319496\n",
            "i 331\n",
            "epoch 50\n",
            " batch Loss train: 0.07413361221551895\n",
            "i 332\n",
            "epoch 50\n",
            " batch Loss train: 0.05518101155757904\n",
            "i 333\n",
            "epoch 50\n",
            " batch Loss train: 0.044717300683259964\n",
            "i 334\n",
            "epoch 50\n",
            " batch Loss train: 0.06908722966909409\n",
            "i 335\n",
            "epoch 50\n",
            " batch Loss train: 0.0429338775575161\n",
            "i 336\n",
            "epoch 50\n",
            " batch Loss train: 0.04970116540789604\n",
            "i 337\n",
            "epoch 50\n",
            " batch Loss train: 0.06977005302906036\n",
            "i 338\n",
            "epoch 50\n",
            " batch Loss train: 0.0692451149225235\n",
            "i 339\n",
            "epoch 50\n",
            " batch Loss train: 0.07600951194763184\n",
            "i 340\n",
            "epoch 50\n",
            " batch Loss train: 0.0575215145945549\n",
            "i 341\n",
            "epoch 50\n",
            " batch Loss train: 0.05082036927342415\n",
            "i 342\n",
            "epoch 50\n",
            " batch Loss train: 0.046524420380592346\n",
            "i 343\n",
            "epoch 50\n",
            " batch Loss train: 0.06211760640144348\n",
            "i 344\n",
            "epoch 50\n",
            " batch Loss train: 0.049141451716423035\n",
            "i 345\n",
            "epoch 50\n",
            " batch Loss train: 0.06262002885341644\n",
            "i 346\n",
            "epoch 50\n",
            " batch Loss train: 0.041346900165081024\n",
            "i 347\n",
            "epoch 50\n",
            " batch Loss train: 0.0712599828839302\n",
            "i 348\n",
            "epoch 50\n",
            " batch Loss train: 0.07154398411512375\n",
            "i 349\n",
            "epoch 50\n",
            " batch Loss train: 0.05678807199001312\n",
            "i 350\n",
            "epoch 50\n",
            " batch Loss train: 0.06256227195262909\n",
            "i 351\n",
            "epoch 50\n",
            " batch Loss train: 0.04352817311882973\n",
            "i 352\n",
            "epoch 50\n",
            " batch Loss train: 0.0453178696334362\n",
            "i 353\n",
            "epoch 50\n",
            " batch Loss train: 0.04410919174551964\n",
            "i 354\n",
            "epoch 50\n",
            " batch Loss train: 0.10038222372531891\n",
            "i 355\n",
            "epoch 50\n",
            " batch Loss train: 0.0683688297867775\n",
            "i 356\n",
            "epoch 50\n",
            " batch Loss train: 0.06280787289142609\n",
            "i 357\n",
            "epoch 50\n",
            " batch Loss train: 0.05808364972472191\n",
            "i 358\n",
            "epoch 50\n",
            " batch Loss train: 0.06370583921670914\n",
            "i 359\n",
            "epoch 50\n",
            " batch Loss train: 0.08154308050870895\n",
            "i 360\n",
            "epoch 50\n",
            " batch Loss train: 0.058780472725629807\n",
            "i 361\n",
            "epoch 50\n",
            " batch Loss train: 0.05779049172997475\n",
            "i 362\n",
            "epoch 50\n",
            " batch Loss train: 0.041409991681575775\n",
            "i 363\n",
            "epoch 50\n",
            " batch Loss train: 0.059785906225442886\n",
            "i 364\n",
            "epoch 50\n",
            " batch Loss train: 0.05752453953027725\n",
            "i 365\n",
            "epoch 50\n",
            " batch Loss train: 0.05388496816158295\n",
            "i 366\n",
            "epoch 50\n",
            " batch Loss train: 0.059561487287282944\n",
            "i 367\n",
            "epoch 50\n",
            " batch Loss train: 0.05360005423426628\n",
            "i 368\n",
            "epoch 50\n",
            " batch Loss train: 0.058941494673490524\n",
            "i 369\n",
            "epoch 50\n",
            " batch Loss train: 0.04972492530941963\n",
            "i 370\n",
            "epoch 50\n",
            " batch Loss train: 0.05140484869480133\n",
            "i 371\n",
            "epoch 50\n",
            " batch Loss train: 0.048154592514038086\n",
            "i 372\n",
            "epoch 50\n",
            " batch Loss train: 0.07001809030771255\n",
            "i 373\n",
            "epoch 50\n",
            " batch Loss train: 0.07195163518190384\n",
            "i 374\n",
            "epoch 50\n",
            " batch Loss train: 0.07501430064439774\n",
            "i 375\n",
            "epoch 50\n",
            " batch Loss train: 0.04171493276953697\n",
            "i 376\n",
            "epoch 50\n",
            " batch Loss train: 0.047432560473680496\n",
            "i 377\n",
            "epoch 50\n",
            " batch Loss train: 0.05334457382559776\n",
            "i 378\n",
            "epoch 50\n",
            " batch Loss train: 0.054260823875665665\n",
            "i 379\n",
            "epoch 50\n",
            " batch Loss train: 0.058504506945610046\n",
            "i 380\n",
            "epoch 50\n",
            " batch Loss train: 0.04946909472346306\n",
            "i 381\n",
            "epoch 50\n",
            " batch Loss train: 0.059987377375364304\n",
            "i 382\n",
            "epoch 50\n",
            " batch Loss train: 0.05132029205560684\n",
            "i 383\n",
            "epoch 50\n",
            " batch Loss train: 0.0452507846057415\n",
            "i 384\n",
            "epoch 50\n",
            " batch Loss train: 0.0483277253806591\n",
            "i 385\n",
            "epoch 50\n",
            " batch Loss train: 0.053458090871572495\n",
            "i 386\n",
            "epoch 50\n",
            " batch Loss train: 0.04667234793305397\n",
            "i 387\n",
            "epoch 50\n",
            " batch Loss train: 0.05198722332715988\n",
            "i 388\n",
            "epoch 50\n",
            " batch Loss train: 0.061242103576660156\n",
            "i 389\n",
            "epoch 50\n",
            " batch Loss train: 0.041964851319789886\n",
            "i 390\n",
            "epoch 50\n",
            " batch Loss train: 0.05579141899943352\n",
            "i 391\n",
            "epoch 50\n",
            " batch Loss train: 0.05036100000143051\n",
            "i 392\n",
            "epoch 50\n",
            " batch Loss train: 0.06616752594709396\n",
            "i 393\n",
            "epoch 50\n",
            " batch Loss train: 0.063936248421669\n",
            "i 394\n",
            "epoch 50\n",
            " batch Loss train: 0.05397937446832657\n",
            "i 395\n",
            "epoch 50\n",
            " batch Loss train: 0.06208943575620651\n",
            "i 396\n",
            "epoch 50\n",
            " batch Loss train: 0.05556970834732056\n",
            "i 397\n",
            "epoch 50\n",
            " batch Loss train: 0.05526914820075035\n",
            "i 398\n",
            "epoch 50\n",
            " batch Loss train: 0.042007263749837875\n",
            "i 399\n",
            "epoch 50\n",
            " batch Loss train: 0.04335763305425644\n",
            "i 400\n",
            "epoch 50\n",
            " batch Loss train: 0.07248715311288834\n",
            "i 401\n",
            "epoch 50\n",
            " batch Loss train: 0.0632462427020073\n",
            "i 402\n",
            "epoch 50\n",
            " batch Loss train: 0.07531373202800751\n",
            "i 403\n",
            "epoch 50\n",
            " batch Loss train: 0.060492515563964844\n",
            "i 404\n",
            "epoch 50\n",
            " batch Loss train: 0.044927485287189484\n",
            "i 405\n",
            "epoch 50\n",
            " batch Loss train: 0.04254430532455444\n",
            "i 406\n",
            "epoch 50\n",
            " batch Loss train: 0.056132085621356964\n",
            "i 407\n",
            "epoch 50\n",
            " batch Loss train: 0.046890757977962494\n",
            "i 408\n",
            "epoch 50\n",
            " batch Loss train: 0.06939250975847244\n",
            "i 409\n",
            "epoch 50\n",
            " batch Loss train: 0.059717535972595215\n",
            "i 410\n",
            "epoch 50\n",
            " batch Loss train: 0.0970093309879303\n",
            "i 411\n",
            "epoch 50\n",
            " batch Loss train: 0.055718183517456055\n",
            "i 412\n",
            "epoch 50\n",
            " batch Loss train: 0.05236862972378731\n",
            "i 413\n",
            "epoch 50\n",
            " batch Loss train: 0.0882977768778801\n",
            "i 414\n",
            "epoch 50\n",
            " batch Loss train: 0.046171270310878754\n",
            "i 415\n",
            "epoch 50\n",
            " batch Loss train: 0.07612606883049011\n",
            "i 416\n",
            "epoch 50\n",
            " batch Loss train: 0.05400044098496437\n",
            "i 417\n",
            "epoch 50\n",
            " batch Loss train: 0.04533864185214043\n",
            "i 418\n",
            "epoch 50\n",
            " batch Loss train: 0.0467519536614418\n",
            "i 419\n",
            "epoch 50\n",
            " batch Loss train: 0.054267652332782745\n",
            "i 420\n",
            "epoch 50\n",
            " batch Loss train: 0.04303213581442833\n",
            "i 421\n",
            "epoch 50\n",
            " batch Loss train: 0.06181406229734421\n",
            "i 422\n",
            "epoch 50\n",
            " batch Loss train: 0.05101529881358147\n",
            "i 423\n",
            "epoch 50\n",
            " batch Loss train: 0.05696792155504227\n",
            "i 424\n",
            "epoch 50\n",
            " batch Loss train: 0.07046011835336685\n",
            "i 425\n",
            "epoch 50\n",
            " batch Loss train: 0.05403375253081322\n",
            "i 426\n",
            "epoch 50\n",
            " batch Loss train: 0.063459113240242\n",
            "i 427\n",
            "epoch 50\n",
            " batch Loss train: 0.05840367451310158\n",
            "i 428\n",
            "epoch 50\n",
            " batch Loss train: 0.04261482506990433\n",
            "i 429\n",
            "epoch 50\n",
            " batch Loss train: 0.06159038096666336\n",
            "i 430\n",
            "epoch 50\n",
            " batch Loss train: 0.08159827440977097\n",
            "i 431\n",
            "epoch 50\n",
            " batch Loss train: 0.08193767815828323\n",
            "i 432\n",
            "epoch 50\n",
            " batch Loss train: 0.06452678889036179\n",
            "i 433\n",
            "epoch 50\n",
            " batch Loss train: 0.058807116001844406\n",
            "i 434\n",
            "epoch 50\n",
            " batch Loss train: 0.06445202231407166\n",
            "i 435\n",
            "epoch 50\n",
            " batch Loss train: 0.05761442333459854\n",
            "i 436\n",
            "epoch 50\n",
            " batch Loss train: 0.0434105321764946\n",
            "i 437\n",
            "epoch 50\n",
            " batch Loss train: 0.0514909066259861\n",
            "i 438\n",
            "epoch 50\n",
            " batch Loss train: 0.053171150386333466\n",
            "i 439\n",
            "epoch 50\n",
            " batch Loss train: 0.056047748774290085\n",
            "i 440\n",
            "epoch 50\n",
            " batch Loss train: 0.057211488485336304\n",
            "i 441\n",
            "epoch 50\n",
            " batch Loss train: 0.057816579937934875\n",
            "i 442\n",
            "epoch 50\n",
            " batch Loss train: 0.05940259248018265\n",
            "i 443\n",
            "epoch 50\n",
            " batch Loss train: 0.06192707642912865\n",
            "i 444\n",
            "epoch 50\n",
            " batch Loss train: 0.044440820813179016\n",
            "i 445\n",
            "epoch 50\n",
            " batch Loss train: 0.050874751061201096\n",
            "total epoch Loss train: tensor(0.0509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 51\n",
            " batch Loss train: 0.05401424691081047\n",
            "i 1\n",
            "epoch 51\n",
            " batch Loss train: 0.045866481959819794\n",
            "i 2\n",
            "epoch 51\n",
            " batch Loss train: 0.051797907799482346\n",
            "i 3\n",
            "epoch 51\n",
            " batch Loss train: 0.041006900370121\n",
            "i 4\n",
            "epoch 51\n",
            " batch Loss train: 0.06340210884809494\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 51\n",
            " batch Loss train: 0.036767780780792236\n",
            "i 6\n",
            "epoch 51\n",
            " batch Loss train: 0.046348173171281815\n",
            "i 7\n",
            "epoch 51\n",
            " batch Loss train: 0.05300629884004593\n",
            "i 8\n",
            "epoch 51\n",
            " batch Loss train: 0.05060179531574249\n",
            "i 9\n",
            "epoch 51\n",
            " batch Loss train: 0.0428592823445797\n",
            "i 10\n",
            "epoch 51\n",
            " batch Loss train: 0.043823469430208206\n",
            "i 11\n",
            "epoch 51\n",
            " batch Loss train: 0.07333460450172424\n",
            "i 12\n",
            "epoch 51\n",
            " batch Loss train: 0.05013120546936989\n",
            "i 13\n",
            "epoch 51\n",
            " batch Loss train: 0.04251449927687645\n",
            "i 14\n",
            "epoch 51\n",
            " batch Loss train: 0.04288971424102783\n",
            "i 15\n",
            "epoch 51\n",
            " batch Loss train: 0.04667087644338608\n",
            "i 16\n",
            "epoch 51\n",
            " batch Loss train: 0.04920974001288414\n",
            "i 17\n",
            "epoch 51\n",
            " batch Loss train: 0.05920998752117157\n",
            "i 18\n",
            "epoch 51\n",
            " batch Loss train: 0.04883265867829323\n",
            "i 19\n",
            "epoch 51\n",
            " batch Loss train: 0.04045550897717476\n",
            "i 20\n",
            "epoch 51\n",
            " batch Loss train: 0.04987749084830284\n",
            "i 21\n",
            "epoch 51\n",
            " batch Loss train: 0.045501336455345154\n",
            "i 22\n",
            "epoch 51\n",
            " batch Loss train: 0.05553865805268288\n",
            "i 23\n",
            "epoch 51\n",
            " batch Loss train: 0.04191666841506958\n",
            "i 24\n",
            "epoch 51\n",
            " batch Loss train: 0.0473247729241848\n",
            "i 25\n",
            "epoch 51\n",
            " batch Loss train: 0.05252264067530632\n",
            "i 26\n",
            "epoch 51\n",
            " batch Loss train: 0.06511098146438599\n",
            "i 27\n",
            "epoch 51\n",
            " batch Loss train: 0.060352176427841187\n",
            "i 28\n",
            "epoch 51\n",
            " batch Loss train: 0.03662408888339996\n",
            "i 29\n",
            "epoch 51\n",
            " batch Loss train: 0.038931917399168015\n",
            "i 30\n",
            "epoch 51\n",
            " batch Loss train: 0.06623435765504837\n",
            "i 31\n",
            "epoch 51\n",
            " batch Loss train: 0.06244587525725365\n",
            "i 32\n",
            "epoch 51\n",
            " batch Loss train: 0.029069799929857254\n",
            "i 33\n",
            "epoch 51\n",
            " batch Loss train: 0.056703679263591766\n",
            "i 34\n",
            "epoch 51\n",
            " batch Loss train: 0.04347589612007141\n",
            "i 35\n",
            "epoch 51\n",
            " batch Loss train: 0.05617557838559151\n",
            "i 36\n",
            "epoch 51\n",
            " batch Loss train: 0.04173875227570534\n",
            "i 37\n",
            "epoch 51\n",
            " batch Loss train: 0.0702013298869133\n",
            "i 38\n",
            "epoch 51\n",
            " batch Loss train: 0.059715792536735535\n",
            "i 39\n",
            "epoch 51\n",
            " batch Loss train: 0.0508686788380146\n",
            "i 40\n",
            "epoch 51\n",
            " batch Loss train: 0.04854033514857292\n",
            "i 41\n",
            "epoch 51\n",
            " batch Loss train: 0.06545459479093552\n",
            "i 42\n",
            "epoch 51\n",
            " batch Loss train: 0.04831661283969879\n",
            "i 43\n",
            "epoch 51\n",
            " batch Loss train: 0.05591161921620369\n",
            "i 44\n",
            "epoch 51\n",
            " batch Loss train: 0.060229405760765076\n",
            "i 45\n",
            "epoch 51\n",
            " batch Loss train: 0.05725740268826485\n",
            "i 46\n",
            "epoch 51\n",
            " batch Loss train: 0.059311166405677795\n",
            "i 47\n",
            "epoch 51\n",
            " batch Loss train: 0.05020298436284065\n",
            "i 48\n",
            "epoch 51\n",
            " batch Loss train: 0.047620970755815506\n",
            "i 49\n",
            "epoch 51\n",
            " batch Loss train: 0.05044106766581535\n",
            "i 50\n",
            "epoch 51\n",
            " batch Loss train: 0.04300083965063095\n",
            "i 51\n",
            "epoch 51\n",
            " batch Loss train: 0.07760041207075119\n",
            "i 52\n",
            "epoch 51\n",
            " batch Loss train: 0.04776427522301674\n",
            "i 53\n",
            "epoch 51\n",
            " batch Loss train: 0.06641196459531784\n",
            "i 54\n",
            "epoch 51\n",
            " batch Loss train: 0.059043459594249725\n",
            "i 55\n",
            "epoch 51\n",
            " batch Loss train: 0.047178369015455246\n",
            "i 56\n",
            "epoch 51\n",
            " batch Loss train: 0.05630309507250786\n",
            "i 57\n",
            "epoch 51\n",
            " batch Loss train: 0.07656866312026978\n",
            "i 58\n",
            "epoch 51\n",
            " batch Loss train: 0.04361114278435707\n",
            "i 59\n",
            "epoch 51\n",
            " batch Loss train: 0.06152847036719322\n",
            "i 60\n",
            "epoch 51\n",
            " batch Loss train: 0.0419151708483696\n",
            "i 61\n",
            "epoch 51\n",
            " batch Loss train: 0.04676544666290283\n",
            "i 62\n",
            "epoch 51\n",
            " batch Loss train: 0.04608026146888733\n",
            "i 63\n",
            "epoch 51\n",
            " batch Loss train: 0.04087537154555321\n",
            "i 64\n",
            "epoch 51\n",
            " batch Loss train: 0.06347539275884628\n",
            "i 65\n",
            "epoch 51\n",
            " batch Loss train: 0.06983603537082672\n",
            "i 66\n",
            "epoch 51\n",
            " batch Loss train: 0.034836042672395706\n",
            "i 67\n",
            "epoch 51\n",
            " batch Loss train: 0.05235197767615318\n",
            "i 68\n",
            "epoch 51\n",
            " batch Loss train: 0.04075484722852707\n",
            "i 69\n",
            "epoch 51\n",
            " batch Loss train: 0.037950411438941956\n",
            "i 70\n",
            "epoch 51\n",
            " batch Loss train: 0.0583173967897892\n",
            "i 71\n",
            "epoch 51\n",
            " batch Loss train: 0.05431186407804489\n",
            "i 72\n",
            "epoch 51\n",
            " batch Loss train: 0.06073645502328873\n",
            "i 73\n",
            "epoch 51\n",
            " batch Loss train: 0.052115682512521744\n",
            "i 74\n",
            "epoch 51\n",
            " batch Loss train: 0.044858142733573914\n",
            "i 75\n",
            "epoch 51\n",
            " batch Loss train: 0.0492137111723423\n",
            "i 76\n",
            "epoch 51\n",
            " batch Loss train: 0.04738200455904007\n",
            "i 77\n",
            "epoch 51\n",
            " batch Loss train: 0.04009963199496269\n",
            "i 78\n",
            "epoch 51\n",
            " batch Loss train: 0.052749041467905045\n",
            "i 79\n",
            "epoch 51\n",
            " batch Loss train: 0.05128472298383713\n",
            "i 80\n",
            "epoch 51\n",
            " batch Loss train: 0.04831725358963013\n",
            "i 81\n",
            "epoch 51\n",
            " batch Loss train: 0.06183118000626564\n",
            "i 82\n",
            "epoch 51\n",
            " batch Loss train: 0.03872782737016678\n",
            "i 83\n",
            "epoch 51\n",
            " batch Loss train: 0.0521431565284729\n",
            "i 84\n",
            "epoch 51\n",
            " batch Loss train: 0.06047778204083443\n",
            "i 85\n",
            "epoch 51\n",
            " batch Loss train: 0.050316836684942245\n",
            "i 86\n",
            "epoch 51\n",
            " batch Loss train: 0.03589165583252907\n",
            "i 87\n",
            "epoch 51\n",
            " batch Loss train: 0.07540816813707352\n",
            "i 88\n",
            "epoch 51\n",
            " batch Loss train: 0.05132129788398743\n",
            "i 89\n",
            "epoch 51\n",
            " batch Loss train: 0.05012534186244011\n",
            "i 90\n",
            "epoch 51\n",
            " batch Loss train: 0.06538039445877075\n",
            "i 91\n",
            "epoch 51\n",
            " batch Loss train: 0.06510147452354431\n",
            "i 92\n",
            "epoch 51\n",
            " batch Loss train: 0.05925294756889343\n",
            "i 93\n",
            "epoch 51\n",
            " batch Loss train: 0.04407784715294838\n",
            "i 94\n",
            "epoch 51\n",
            " batch Loss train: 0.039760906249284744\n",
            "i 95\n",
            "epoch 51\n",
            " batch Loss train: 0.051118411123752594\n",
            "i 96\n",
            "epoch 51\n",
            " batch Loss train: 0.05743891000747681\n",
            "i 97\n",
            "epoch 51\n",
            " batch Loss train: 0.0422065407037735\n",
            "i 98\n",
            "epoch 51\n",
            " batch Loss train: 0.05074959248304367\n",
            "i 99\n",
            "epoch 51\n",
            " batch Loss train: 0.05216127261519432\n",
            "i 100\n",
            "epoch 51\n",
            " batch Loss train: 0.0478915311396122\n",
            "i 101\n",
            "epoch 51\n",
            " batch Loss train: 0.0496707484126091\n",
            "i 102\n",
            "epoch 51\n",
            " batch Loss train: 0.06631772220134735\n",
            "i 103\n",
            "epoch 51\n",
            " batch Loss train: 0.050270408391952515\n",
            "i 104\n",
            "epoch 51\n",
            " batch Loss train: 0.054297592490911484\n",
            "i 105\n",
            "epoch 51\n",
            " batch Loss train: 0.04856092482805252\n",
            "i 106\n",
            "epoch 51\n",
            " batch Loss train: 0.06254599988460541\n",
            "i 107\n",
            "epoch 51\n",
            " batch Loss train: 0.04855716601014137\n",
            "i 108\n",
            "epoch 51\n",
            " batch Loss train: 0.06261458247900009\n",
            "i 109\n",
            "epoch 51\n",
            " batch Loss train: 0.04474072903394699\n",
            "i 110\n",
            "epoch 51\n",
            " batch Loss train: 0.061850402504205704\n",
            "i 111\n",
            "epoch 51\n",
            " batch Loss train: 0.05366605892777443\n",
            "i 112\n",
            "epoch 51\n",
            " batch Loss train: 0.059014905244112015\n",
            "i 113\n",
            "epoch 51\n",
            " batch Loss train: 0.04605543613433838\n",
            "i 114\n",
            "epoch 51\n",
            " batch Loss train: 0.08950357139110565\n",
            "i 115\n",
            "epoch 51\n",
            " batch Loss train: 0.04119158536195755\n",
            "i 116\n",
            "epoch 51\n",
            " batch Loss train: 0.03843298554420471\n",
            "i 117\n",
            "epoch 51\n",
            " batch Loss train: 0.0652923583984375\n",
            "i 118\n",
            "epoch 51\n",
            " batch Loss train: 0.05937892198562622\n",
            "i 119\n",
            "epoch 51\n",
            " batch Loss train: 0.05208631977438927\n",
            "i 120\n",
            "epoch 51\n",
            " batch Loss train: 0.03562284633517265\n",
            "i 121\n",
            "epoch 51\n",
            " batch Loss train: 0.041261520236730576\n",
            "i 122\n",
            "epoch 51\n",
            " batch Loss train: 0.040795404464006424\n",
            "i 123\n",
            "epoch 51\n",
            " batch Loss train: 0.0408322848379612\n",
            "i 124\n",
            "epoch 51\n",
            " batch Loss train: 0.07863903790712357\n",
            "i 125\n",
            "epoch 51\n",
            " batch Loss train: 0.06879357993602753\n",
            "i 126\n",
            "epoch 51\n",
            " batch Loss train: 0.07324700057506561\n",
            "i 127\n",
            "epoch 51\n",
            " batch Loss train: 0.05316353589296341\n",
            "i 128\n",
            "epoch 51\n",
            " batch Loss train: 0.056865520775318146\n",
            "i 129\n",
            "epoch 51\n",
            " batch Loss train: 0.03676321357488632\n",
            "i 130\n",
            "epoch 51\n",
            " batch Loss train: 0.042661745101213455\n",
            "i 131\n",
            "epoch 51\n",
            " batch Loss train: 0.05453820526599884\n",
            "i 132\n",
            "epoch 51\n",
            " batch Loss train: 0.036550216376781464\n",
            "i 133\n",
            "epoch 51\n",
            " batch Loss train: 0.05632079392671585\n",
            "i 134\n",
            "epoch 51\n",
            " batch Loss train: 0.057816002517938614\n",
            "i 135\n",
            "epoch 51\n",
            " batch Loss train: 0.056233953684568405\n",
            "i 136\n",
            "epoch 51\n",
            " batch Loss train: 0.05500909686088562\n",
            "i 137\n",
            "epoch 51\n",
            " batch Loss train: 0.05874427780508995\n",
            "i 138\n",
            "epoch 51\n",
            " batch Loss train: 0.06332235038280487\n",
            "i 139\n",
            "epoch 51\n",
            " batch Loss train: 0.06712423264980316\n",
            "i 140\n",
            "epoch 51\n",
            " batch Loss train: 0.05803447216749191\n",
            "i 141\n",
            "epoch 51\n",
            " batch Loss train: 0.0418262705206871\n",
            "i 142\n",
            "epoch 51\n",
            " batch Loss train: 0.05633844435214996\n",
            "i 143\n",
            "epoch 51\n",
            " batch Loss train: 0.05449638515710831\n",
            "i 144\n",
            "epoch 51\n",
            " batch Loss train: 0.06599840521812439\n",
            "i 145\n",
            "epoch 51\n",
            " batch Loss train: 0.05582944303750992\n",
            "i 146\n",
            "epoch 51\n",
            " batch Loss train: 0.07149094343185425\n",
            "i 147\n",
            "epoch 51\n",
            " batch Loss train: 0.05861164629459381\n",
            "i 148\n",
            "epoch 51\n",
            " batch Loss train: 0.05738689750432968\n",
            "i 149\n",
            "epoch 51\n",
            " batch Loss train: 0.04428644850850105\n",
            "i 150\n",
            "epoch 51\n",
            " batch Loss train: 0.05259253829717636\n",
            "i 151\n",
            "epoch 51\n",
            " batch Loss train: 0.04820007085800171\n",
            "i 152\n",
            "epoch 51\n",
            " batch Loss train: 0.0606408417224884\n",
            "i 153\n",
            "epoch 51\n",
            " batch Loss train: 0.05482611432671547\n",
            "i 154\n",
            "epoch 51\n",
            " batch Loss train: 0.042800284922122955\n",
            "i 155\n",
            "epoch 51\n",
            " batch Loss train: 0.044687267392873764\n",
            "i 156\n",
            "epoch 51\n",
            " batch Loss train: 0.05330561101436615\n",
            "i 157\n",
            "epoch 51\n",
            " batch Loss train: 0.05050374194979668\n",
            "i 158\n",
            "epoch 51\n",
            " batch Loss train: 0.0322381928563118\n",
            "i 159\n",
            "epoch 51\n",
            " batch Loss train: 0.056909702718257904\n",
            "i 160\n",
            "epoch 51\n",
            " batch Loss train: 0.056708671152591705\n",
            "i 161\n",
            "epoch 51\n",
            " batch Loss train: 0.04716078191995621\n",
            "i 162\n",
            "epoch 51\n",
            " batch Loss train: 0.06421595066785812\n",
            "i 163\n",
            "epoch 51\n",
            " batch Loss train: 0.06856502592563629\n",
            "i 164\n",
            "epoch 51\n",
            " batch Loss train: 0.07732026278972626\n",
            "i 165\n",
            "epoch 51\n",
            " batch Loss train: 0.05067996308207512\n",
            "i 166\n",
            "epoch 51\n",
            " batch Loss train: 0.0736878290772438\n",
            "i 167\n",
            "epoch 51\n",
            " batch Loss train: 0.0518433041870594\n",
            "i 168\n",
            "epoch 51\n",
            " batch Loss train: 0.04788712412118912\n",
            "i 169\n",
            "epoch 51\n",
            " batch Loss train: 0.0594133622944355\n",
            "i 170\n",
            "epoch 51\n",
            " batch Loss train: 0.06181749701499939\n",
            "i 171\n",
            "epoch 51\n",
            " batch Loss train: 0.049460310488939285\n",
            "i 172\n",
            "epoch 51\n",
            " batch Loss train: 0.043351009488105774\n",
            "i 173\n",
            "epoch 51\n",
            " batch Loss train: 0.05552475154399872\n",
            "i 174\n",
            "epoch 51\n",
            " batch Loss train: 0.06149965897202492\n",
            "i 175\n",
            "epoch 51\n",
            " batch Loss train: 0.05199830234050751\n",
            "i 176\n",
            "epoch 51\n",
            " batch Loss train: 0.06800484657287598\n",
            "i 177\n",
            "epoch 51\n",
            " batch Loss train: 0.039370205253362656\n",
            "i 178\n",
            "epoch 51\n",
            " batch Loss train: 0.05017273500561714\n",
            "i 179\n",
            "epoch 51\n",
            " batch Loss train: 0.05505511909723282\n",
            "i 180\n",
            "epoch 51\n",
            " batch Loss train: 0.05687584728002548\n",
            "i 181\n",
            "epoch 51\n",
            " batch Loss train: 0.05570286139845848\n",
            "i 182\n",
            "epoch 51\n",
            " batch Loss train: 0.07622323930263519\n",
            "i 183\n",
            "epoch 51\n",
            " batch Loss train: 0.04444386065006256\n",
            "i 184\n",
            "epoch 51\n",
            " batch Loss train: 0.05969506874680519\n",
            "i 185\n",
            "epoch 51\n",
            " batch Loss train: 0.08955380320549011\n",
            "i 186\n",
            "epoch 51\n",
            " batch Loss train: 0.050299886614084244\n",
            "i 187\n",
            "epoch 51\n",
            " batch Loss train: 0.06616105884313583\n",
            "i 188\n",
            "epoch 51\n",
            " batch Loss train: 0.06432860344648361\n",
            "i 189\n",
            "epoch 51\n",
            " batch Loss train: 0.04750044643878937\n",
            "i 190\n",
            "epoch 51\n",
            " batch Loss train: 0.06262695789337158\n",
            "i 191\n",
            "epoch 51\n",
            " batch Loss train: 0.05054297298192978\n",
            "i 192\n",
            "epoch 51\n",
            " batch Loss train: 0.05650267377495766\n",
            "i 193\n",
            "epoch 51\n",
            " batch Loss train: 0.04883367940783501\n",
            "i 194\n",
            "epoch 51\n",
            " batch Loss train: 0.045636050403118134\n",
            "i 195\n",
            "epoch 51\n",
            " batch Loss train: 0.05869631469249725\n",
            "i 196\n",
            "epoch 51\n",
            " batch Loss train: 0.045312218368053436\n",
            "i 197\n",
            "epoch 51\n",
            " batch Loss train: 0.06672818958759308\n",
            "i 198\n",
            "epoch 51\n",
            " batch Loss train: 0.06437871605157852\n",
            "i 199\n",
            "epoch 51\n",
            " batch Loss train: 0.05432220175862312\n",
            "i 200\n",
            "epoch 51\n",
            " batch Loss train: 0.06527791917324066\n",
            "i 201\n",
            "epoch 51\n",
            " batch Loss train: 0.061386000365018845\n",
            "i 202\n",
            "epoch 51\n",
            " batch Loss train: 0.0644172951579094\n",
            "i 203\n",
            "epoch 51\n",
            " batch Loss train: 0.07154808193445206\n",
            "i 204\n",
            "epoch 51\n",
            " batch Loss train: 0.04136340692639351\n",
            "i 205\n",
            "epoch 51\n",
            " batch Loss train: 0.06956721842288971\n",
            "i 206\n",
            "epoch 51\n",
            " batch Loss train: 0.05207375809550285\n",
            "i 207\n",
            "epoch 51\n",
            " batch Loss train: 0.038394711911678314\n",
            "i 208\n",
            "epoch 51\n",
            " batch Loss train: 0.03555804118514061\n",
            "i 209\n",
            "epoch 51\n",
            " batch Loss train: 0.05964967980980873\n",
            "i 210\n",
            "epoch 51\n",
            " batch Loss train: 0.05254039913415909\n",
            "i 211\n",
            "epoch 51\n",
            " batch Loss train: 0.07691586762666702\n",
            "i 212\n",
            "epoch 51\n",
            " batch Loss train: 0.050937820225954056\n",
            "i 213\n",
            "epoch 51\n",
            " batch Loss train: 0.047845590859651566\n",
            "i 214\n",
            "epoch 51\n",
            " batch Loss train: 0.06625648587942123\n",
            "i 215\n",
            "epoch 51\n",
            " batch Loss train: 0.07670179754495621\n",
            "i 216\n",
            "epoch 51\n",
            " batch Loss train: 0.0453573577105999\n",
            "i 217\n",
            "epoch 51\n",
            " batch Loss train: 0.054390277713537216\n",
            "i 218\n",
            "epoch 51\n",
            " batch Loss train: 0.06195196136832237\n",
            "i 219\n",
            "epoch 51\n",
            " batch Loss train: 0.05059031397104263\n",
            "i 220\n",
            "epoch 51\n",
            " batch Loss train: 0.03004271350800991\n",
            "i 221\n",
            "epoch 51\n",
            " batch Loss train: 0.05035383254289627\n",
            "i 222\n",
            "epoch 51\n",
            " batch Loss train: 0.051808420568704605\n",
            "i 223\n",
            "epoch 51\n",
            " batch Loss train: 0.060727808624506\n",
            "i 224\n",
            "epoch 51\n",
            " batch Loss train: 0.05210110545158386\n",
            "i 225\n",
            "epoch 51\n",
            " batch Loss train: 0.05930997058749199\n",
            "i 226\n",
            "epoch 51\n",
            " batch Loss train: 0.06031342223286629\n",
            "i 227\n",
            "epoch 51\n",
            " batch Loss train: 0.04400688037276268\n",
            "i 228\n",
            "epoch 51\n",
            " batch Loss train: 0.06691800057888031\n",
            "i 229\n",
            "epoch 51\n",
            " batch Loss train: 0.051736071705818176\n",
            "i 230\n",
            "epoch 51\n",
            " batch Loss train: 0.04802593216300011\n",
            "i 231\n",
            "epoch 51\n",
            " batch Loss train: 0.052045345306396484\n",
            "i 232\n",
            "epoch 51\n",
            " batch Loss train: 0.05837095528841019\n",
            "i 233\n",
            "epoch 51\n",
            " batch Loss train: 0.04970397800207138\n",
            "i 234\n",
            "epoch 51\n",
            " batch Loss train: 0.045137129724025726\n",
            "i 235\n",
            "epoch 51\n",
            " batch Loss train: 0.050036750733852386\n",
            "i 236\n",
            "epoch 51\n",
            " batch Loss train: 0.059481099247932434\n",
            "i 237\n",
            "epoch 51\n",
            " batch Loss train: 0.05872402712702751\n",
            "i 238\n",
            "epoch 51\n",
            " batch Loss train: 0.03779195249080658\n",
            "i 239\n",
            "epoch 51\n",
            " batch Loss train: 0.045055970549583435\n",
            "i 240\n",
            "epoch 51\n",
            " batch Loss train: 0.04889632761478424\n",
            "i 241\n",
            "epoch 51\n",
            " batch Loss train: 0.04776008054614067\n",
            "i 242\n",
            "epoch 51\n",
            " batch Loss train: 0.043718114495277405\n",
            "i 243\n",
            "epoch 51\n",
            " batch Loss train: 0.04225489869713783\n",
            "i 244\n",
            "epoch 51\n",
            " batch Loss train: 0.04798193648457527\n",
            "i 245\n",
            "epoch 51\n",
            " batch Loss train: 0.04584473744034767\n",
            "i 246\n",
            "epoch 51\n",
            " batch Loss train: 0.051677897572517395\n",
            "i 247\n",
            "epoch 51\n",
            " batch Loss train: 0.05961406230926514\n",
            "i 248\n",
            "epoch 51\n",
            " batch Loss train: 0.056505829095840454\n",
            "i 249\n",
            "epoch 51\n",
            " batch Loss train: 0.03153236582875252\n",
            "i 250\n",
            "epoch 51\n",
            " batch Loss train: 0.07186459004878998\n",
            "i 251\n",
            "epoch 51\n",
            " batch Loss train: 0.06033288314938545\n",
            "i 252\n",
            "epoch 51\n",
            " batch Loss train: 0.053925495594739914\n",
            "i 253\n",
            "epoch 51\n",
            " batch Loss train: 0.04524761810898781\n",
            "i 254\n",
            "epoch 51\n",
            " batch Loss train: 0.06721600145101547\n",
            "i 255\n",
            "epoch 51\n",
            " batch Loss train: 0.04790610820055008\n",
            "i 256\n",
            "epoch 51\n",
            " batch Loss train: 0.0552942231297493\n",
            "i 257\n",
            "epoch 51\n",
            " batch Loss train: 0.05410126969218254\n",
            "i 258\n",
            "epoch 51\n",
            " batch Loss train: 0.051101844757795334\n",
            "i 259\n",
            "epoch 51\n",
            " batch Loss train: 0.058059390634298325\n",
            "i 260\n",
            "epoch 51\n",
            " batch Loss train: 0.0410405769944191\n",
            "i 261\n",
            "epoch 51\n",
            " batch Loss train: 0.04335152357816696\n",
            "i 262\n",
            "epoch 51\n",
            " batch Loss train: 0.04981879144906998\n",
            "i 263\n",
            "epoch 51\n",
            " batch Loss train: 0.043502453714609146\n",
            "i 264\n",
            "epoch 51\n",
            " batch Loss train: 0.05639776587486267\n",
            "i 265\n",
            "epoch 51\n",
            " batch Loss train: 0.05511920154094696\n",
            "i 266\n",
            "epoch 51\n",
            " batch Loss train: 0.04839637130498886\n",
            "i 267\n",
            "epoch 51\n",
            " batch Loss train: 0.05223700776696205\n",
            "i 268\n",
            "epoch 51\n",
            " batch Loss train: 0.04740608483552933\n",
            "i 269\n",
            "epoch 51\n",
            " batch Loss train: 0.03830626234412193\n",
            "i 270\n",
            "epoch 51\n",
            " batch Loss train: 0.04950277879834175\n",
            "i 271\n",
            "epoch 51\n",
            " batch Loss train: 0.06470568478107452\n",
            "i 272\n",
            "epoch 51\n",
            " batch Loss train: 0.05590328946709633\n",
            "i 273\n",
            "epoch 51\n",
            " batch Loss train: 0.039817288517951965\n",
            "i 274\n",
            "epoch 51\n",
            " batch Loss train: 0.05431558936834335\n",
            "i 275\n",
            "epoch 51\n",
            " batch Loss train: 0.05666782334446907\n",
            "i 276\n",
            "epoch 51\n",
            " batch Loss train: 0.059932708740234375\n",
            "i 277\n",
            "epoch 51\n",
            " batch Loss train: 0.06267993897199631\n",
            "i 278\n",
            "epoch 51\n",
            " batch Loss train: 0.06320296227931976\n",
            "i 279\n",
            "epoch 51\n",
            " batch Loss train: 0.038450632244348526\n",
            "i 280\n",
            "epoch 51\n",
            " batch Loss train: 0.03766223415732384\n",
            "i 281\n",
            "epoch 51\n",
            " batch Loss train: 0.0605289600789547\n",
            "i 282\n",
            "epoch 51\n",
            " batch Loss train: 0.05905167758464813\n",
            "i 283\n",
            "epoch 51\n",
            " batch Loss train: 0.06407971680164337\n",
            "i 284\n",
            "epoch 51\n",
            " batch Loss train: 0.04616966471076012\n",
            "i 285\n",
            "epoch 51\n",
            " batch Loss train: 0.0638454481959343\n",
            "i 286\n",
            "epoch 51\n",
            " batch Loss train: 0.07473395019769669\n",
            "i 287\n",
            "epoch 51\n",
            " batch Loss train: 0.05137990787625313\n",
            "i 288\n",
            "epoch 51\n",
            " batch Loss train: 0.046566370874643326\n",
            "i 289\n",
            "epoch 51\n",
            " batch Loss train: 0.05718199536204338\n",
            "i 290\n",
            "epoch 51\n",
            " batch Loss train: 0.04348290339112282\n",
            "i 291\n",
            "epoch 51\n",
            " batch Loss train: 0.047826461493968964\n",
            "i 292\n",
            "epoch 51\n",
            " batch Loss train: 0.048790354281663895\n",
            "i 293\n",
            "epoch 51\n",
            " batch Loss train: 0.04107421264052391\n",
            "i 294\n",
            "epoch 51\n",
            " batch Loss train: 0.0384170301258564\n",
            "i 295\n",
            "epoch 51\n",
            " batch Loss train: 0.06469962000846863\n",
            "i 296\n",
            "epoch 51\n",
            " batch Loss train: 0.056160420179367065\n",
            "i 297\n",
            "epoch 51\n",
            " batch Loss train: 0.05074820667505264\n",
            "i 298\n",
            "epoch 51\n",
            " batch Loss train: 0.04300743341445923\n",
            "i 299\n",
            "epoch 51\n",
            " batch Loss train: 0.03934821859002113\n",
            "i 300\n",
            "epoch 51\n",
            " batch Loss train: 0.04763149097561836\n",
            "i 301\n",
            "epoch 51\n",
            " batch Loss train: 0.06049678474664688\n",
            "i 302\n",
            "epoch 51\n",
            " batch Loss train: 0.05622236058115959\n",
            "i 303\n",
            "epoch 51\n",
            " batch Loss train: 0.06400680541992188\n",
            "i 304\n",
            "epoch 51\n",
            " batch Loss train: 0.05390625819563866\n",
            "i 305\n",
            "epoch 51\n",
            " batch Loss train: 0.053593818098306656\n",
            "i 306\n",
            "epoch 51\n",
            " batch Loss train: 0.06967966258525848\n",
            "i 307\n",
            "epoch 51\n",
            " batch Loss train: 0.04644545912742615\n",
            "i 308\n",
            "epoch 51\n",
            " batch Loss train: 0.06472617387771606\n",
            "i 309\n",
            "epoch 51\n",
            " batch Loss train: 0.0315420962870121\n",
            "i 310\n",
            "epoch 51\n",
            " batch Loss train: 0.05016075074672699\n",
            "i 311\n",
            "epoch 51\n",
            " batch Loss train: 0.051816172897815704\n",
            "i 312\n",
            "epoch 51\n",
            " batch Loss train: 0.06257320195436478\n",
            "i 313\n",
            "epoch 51\n",
            " batch Loss train: 0.06314212083816528\n",
            "i 314\n",
            "epoch 51\n",
            " batch Loss train: 0.04680916294455528\n",
            "i 315\n",
            "epoch 51\n",
            " batch Loss train: 0.06170710176229477\n",
            "i 316\n",
            "epoch 51\n",
            " batch Loss train: 0.05407862365245819\n",
            "i 317\n",
            "epoch 51\n",
            " batch Loss train: 0.06055812910199165\n",
            "i 318\n",
            "epoch 51\n",
            " batch Loss train: 0.07735885679721832\n",
            "i 319\n",
            "epoch 51\n",
            " batch Loss train: 0.061126045882701874\n",
            "i 320\n",
            "epoch 51\n",
            " batch Loss train: 0.059304747730493546\n",
            "i 321\n",
            "epoch 51\n",
            " batch Loss train: 0.05122775584459305\n",
            "i 322\n",
            "epoch 51\n",
            " batch Loss train: 0.052687544375658035\n",
            "i 323\n",
            "epoch 51\n",
            " batch Loss train: 0.04635534808039665\n",
            "i 324\n",
            "epoch 51\n",
            " batch Loss train: 0.04787728190422058\n",
            "i 325\n",
            "epoch 51\n",
            " batch Loss train: 0.0441167987883091\n",
            "i 326\n",
            "epoch 51\n",
            " batch Loss train: 0.0477120466530323\n",
            "i 327\n",
            "epoch 51\n",
            " batch Loss train: 0.052354324609041214\n",
            "i 328\n",
            "epoch 51\n",
            " batch Loss train: 0.05701510235667229\n",
            "i 329\n",
            "epoch 51\n",
            " batch Loss train: 0.0497668981552124\n",
            "i 330\n",
            "epoch 51\n",
            " batch Loss train: 0.0632820650935173\n",
            "i 331\n",
            "epoch 51\n",
            " batch Loss train: 0.06438116729259491\n",
            "i 332\n",
            "epoch 51\n",
            " batch Loss train: 0.050908301025629044\n",
            "i 333\n",
            "epoch 51\n",
            " batch Loss train: 0.06195129081606865\n",
            "i 334\n",
            "epoch 51\n",
            " batch Loss train: 0.052611179649829865\n",
            "i 335\n",
            "epoch 51\n",
            " batch Loss train: 0.05984964221715927\n",
            "i 336\n",
            "epoch 51\n",
            " batch Loss train: 0.05257914587855339\n",
            "i 337\n",
            "epoch 51\n",
            " batch Loss train: 0.03936676308512688\n",
            "i 338\n",
            "epoch 51\n",
            " batch Loss train: 0.05002715066075325\n",
            "i 339\n",
            "epoch 51\n",
            " batch Loss train: 0.04994022846221924\n",
            "i 340\n",
            "epoch 51\n",
            " batch Loss train: 0.052623897790908813\n",
            "i 341\n",
            "epoch 51\n",
            " batch Loss train: 0.06436475366353989\n",
            "i 342\n",
            "epoch 51\n",
            " batch Loss train: 0.049769897013902664\n",
            "i 343\n",
            "epoch 51\n",
            " batch Loss train: 0.0595904216170311\n",
            "i 344\n",
            "epoch 51\n",
            " batch Loss train: 0.09526443481445312\n",
            "i 345\n",
            "epoch 51\n",
            " batch Loss train: 0.052095334976911545\n",
            "i 346\n",
            "epoch 51\n",
            " batch Loss train: 0.046869754791259766\n",
            "i 347\n",
            "epoch 51\n",
            " batch Loss train: 0.04793599247932434\n",
            "i 348\n",
            "epoch 51\n",
            " batch Loss train: 0.04520183801651001\n",
            "i 349\n",
            "epoch 51\n",
            " batch Loss train: 0.061064623296260834\n",
            "i 350\n",
            "epoch 51\n",
            " batch Loss train: 0.055143531411886215\n",
            "i 351\n",
            "epoch 51\n",
            " batch Loss train: 0.06411644071340561\n",
            "i 352\n",
            "epoch 51\n",
            " batch Loss train: 0.05787676200270653\n",
            "i 353\n",
            "epoch 51\n",
            " batch Loss train: 0.04091423749923706\n",
            "i 354\n",
            "epoch 51\n",
            " batch Loss train: 0.05577249452471733\n",
            "i 355\n",
            "epoch 51\n",
            " batch Loss train: 0.046329066157341\n",
            "i 356\n",
            "epoch 51\n",
            " batch Loss train: 0.05293840169906616\n",
            "i 357\n",
            "epoch 51\n",
            " batch Loss train: 0.052540767937898636\n",
            "i 358\n",
            "epoch 51\n",
            " batch Loss train: 0.055810634046792984\n",
            "i 359\n",
            "epoch 51\n",
            " batch Loss train: 0.07526689767837524\n",
            "i 360\n",
            "epoch 51\n",
            " batch Loss train: 0.08095081895589828\n",
            "i 361\n",
            "epoch 51\n",
            " batch Loss train: 0.05983450636267662\n",
            "i 362\n",
            "epoch 51\n",
            " batch Loss train: 0.05743446201086044\n",
            "i 363\n",
            "epoch 51\n",
            " batch Loss train: 0.043880246579647064\n",
            "i 364\n",
            "epoch 51\n",
            " batch Loss train: 0.05252159759402275\n",
            "i 365\n",
            "epoch 51\n",
            " batch Loss train: 0.06198772415518761\n",
            "i 366\n",
            "epoch 51\n",
            " batch Loss train: 0.0427512563765049\n",
            "i 367\n",
            "epoch 51\n",
            " batch Loss train: 0.0544888898730278\n",
            "i 368\n",
            "epoch 51\n",
            " batch Loss train: 0.056080739945173264\n",
            "i 369\n",
            "epoch 51\n",
            " batch Loss train: 0.06995046883821487\n",
            "i 370\n",
            "epoch 51\n",
            " batch Loss train: 0.052429087460041046\n",
            "i 371\n",
            "epoch 51\n",
            " batch Loss train: 0.06935810297727585\n",
            "i 372\n",
            "epoch 51\n",
            " batch Loss train: 0.05153679475188255\n",
            "i 373\n",
            "epoch 51\n",
            " batch Loss train: 0.04320353642106056\n",
            "i 374\n",
            "epoch 51\n",
            " batch Loss train: 0.03734850138425827\n",
            "i 375\n",
            "epoch 51\n",
            " batch Loss train: 0.08536013215780258\n",
            "i 376\n",
            "epoch 51\n",
            " batch Loss train: 0.06301988661289215\n",
            "i 377\n",
            "epoch 51\n",
            " batch Loss train: 0.060545921325683594\n",
            "i 378\n",
            "epoch 51\n",
            " batch Loss train: 0.05655433237552643\n",
            "i 379\n",
            "epoch 51\n",
            " batch Loss train: 0.047986529767513275\n",
            "i 380\n",
            "epoch 51\n",
            " batch Loss train: 0.06295066326856613\n",
            "i 381\n",
            "epoch 51\n",
            " batch Loss train: 0.05754087120294571\n",
            "i 382\n",
            "epoch 51\n",
            " batch Loss train: 0.06363065540790558\n",
            "i 383\n",
            "epoch 51\n",
            " batch Loss train: 0.04765498638153076\n",
            "i 384\n",
            "epoch 51\n",
            " batch Loss train: 0.057150375097990036\n",
            "i 385\n",
            "epoch 51\n",
            " batch Loss train: 0.05399791896343231\n",
            "i 386\n",
            "epoch 51\n",
            " batch Loss train: 0.05022905021905899\n",
            "i 387\n",
            "epoch 51\n",
            " batch Loss train: 0.043385449796915054\n",
            "i 388\n",
            "epoch 51\n",
            " batch Loss train: 0.07393591850996017\n",
            "i 389\n",
            "epoch 51\n",
            " batch Loss train: 0.04597118869423866\n",
            "i 390\n",
            "epoch 51\n",
            " batch Loss train: 0.057165324687957764\n",
            "i 391\n",
            "epoch 51\n",
            " batch Loss train: 0.059214890003204346\n",
            "i 392\n",
            "epoch 51\n",
            " batch Loss train: 0.053298499435186386\n",
            "i 393\n",
            "epoch 51\n",
            " batch Loss train: 0.05333048850297928\n",
            "i 394\n",
            "epoch 51\n",
            " batch Loss train: 0.04759686812758446\n",
            "i 395\n",
            "epoch 51\n",
            " batch Loss train: 0.050367940217256546\n",
            "i 396\n",
            "epoch 51\n",
            " batch Loss train: 0.04586711525917053\n",
            "i 397\n",
            "epoch 51\n",
            " batch Loss train: 0.05377528443932533\n",
            "i 398\n",
            "epoch 51\n",
            " batch Loss train: 0.049567777663469315\n",
            "i 399\n",
            "epoch 51\n",
            " batch Loss train: 0.06306662410497665\n",
            "i 400\n",
            "epoch 51\n",
            " batch Loss train: 0.05731270834803581\n",
            "i 401\n",
            "epoch 51\n",
            " batch Loss train: 0.04742738604545593\n",
            "i 402\n",
            "epoch 51\n",
            " batch Loss train: 0.03366890922188759\n",
            "i 403\n",
            "epoch 51\n",
            " batch Loss train: 0.04547096788883209\n",
            "i 404\n",
            "epoch 51\n",
            " batch Loss train: 0.04388617351651192\n",
            "i 405\n",
            "epoch 51\n",
            " batch Loss train: 0.07000322639942169\n",
            "i 406\n",
            "epoch 51\n",
            " batch Loss train: 0.05440819263458252\n",
            "i 407\n",
            "epoch 51\n",
            " batch Loss train: 0.04145264998078346\n",
            "i 408\n",
            "epoch 51\n",
            " batch Loss train: 0.04579000920057297\n",
            "i 409\n",
            "epoch 51\n",
            " batch Loss train: 0.03894701600074768\n",
            "i 410\n",
            "epoch 51\n",
            " batch Loss train: 0.06502608209848404\n",
            "i 411\n",
            "epoch 51\n",
            " batch Loss train: 0.05138738825917244\n",
            "i 412\n",
            "epoch 51\n",
            " batch Loss train: 0.058916762471199036\n",
            "i 413\n",
            "epoch 51\n",
            " batch Loss train: 0.057225972414016724\n",
            "i 414\n",
            "epoch 51\n",
            " batch Loss train: 0.05070444568991661\n",
            "i 415\n",
            "epoch 51\n",
            " batch Loss train: 0.04861907660961151\n",
            "i 416\n",
            "epoch 51\n",
            " batch Loss train: 0.05143677443265915\n",
            "i 417\n",
            "epoch 51\n",
            " batch Loss train: 0.05912044271826744\n",
            "i 418\n",
            "epoch 51\n",
            " batch Loss train: 0.05506763607263565\n",
            "i 419\n",
            "epoch 51\n",
            " batch Loss train: 0.055458180606365204\n",
            "i 420\n",
            "epoch 51\n",
            " batch Loss train: 0.06345968693494797\n",
            "i 421\n",
            "epoch 51\n",
            " batch Loss train: 0.03852478414773941\n",
            "i 422\n",
            "epoch 51\n",
            " batch Loss train: 0.05852910503745079\n",
            "i 423\n",
            "epoch 51\n",
            " batch Loss train: 0.06952161341905594\n",
            "i 424\n",
            "epoch 51\n",
            " batch Loss train: 0.05373129993677139\n",
            "i 425\n",
            "epoch 51\n",
            " batch Loss train: 0.05662074312567711\n",
            "i 426\n",
            "epoch 51\n",
            " batch Loss train: 0.06691931188106537\n",
            "i 427\n",
            "epoch 51\n",
            " batch Loss train: 0.05892343074083328\n",
            "i 428\n",
            "epoch 51\n",
            " batch Loss train: 0.05660860240459442\n",
            "i 429\n",
            "epoch 51\n",
            " batch Loss train: 0.05407147854566574\n",
            "i 430\n",
            "epoch 51\n",
            " batch Loss train: 0.05176776275038719\n",
            "i 431\n",
            "epoch 51\n",
            " batch Loss train: 0.062357716262340546\n",
            "i 432\n",
            "epoch 51\n",
            " batch Loss train: 0.058206845074892044\n",
            "i 433\n",
            "epoch 51\n",
            " batch Loss train: 0.05051814392209053\n",
            "i 434\n",
            "epoch 51\n",
            " batch Loss train: 0.0622098334133625\n",
            "i 435\n",
            "epoch 51\n",
            " batch Loss train: 0.04444148391485214\n",
            "i 436\n",
            "epoch 51\n",
            " batch Loss train: 0.04619742929935455\n",
            "i 437\n",
            "epoch 51\n",
            " batch Loss train: 0.07303858548402786\n",
            "i 438\n",
            "epoch 51\n",
            " batch Loss train: 0.0761202797293663\n",
            "i 439\n",
            "epoch 51\n",
            " batch Loss train: 0.04039337858557701\n",
            "i 440\n",
            "epoch 51\n",
            " batch Loss train: 0.062410324811935425\n",
            "i 441\n",
            "epoch 51\n",
            " batch Loss train: 0.046051036566495895\n",
            "i 442\n",
            "epoch 51\n",
            " batch Loss train: 0.06774942576885223\n",
            "i 443\n",
            "epoch 51\n",
            " batch Loss train: 0.0429568812251091\n",
            "i 444\n",
            "epoch 51\n",
            " batch Loss train: 0.044916246086359024\n",
            "i 445\n",
            "epoch 51\n",
            " batch Loss train: 0.04873126000165939\n",
            "total epoch Loss train: tensor(0.0487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 52\n",
            " batch Loss train: 0.04246515780687332\n",
            "i 1\n",
            "epoch 52\n",
            " batch Loss train: 0.04271915555000305\n",
            "i 2\n",
            "epoch 52\n",
            " batch Loss train: 0.04125748574733734\n",
            "i 3\n",
            "epoch 52\n",
            " batch Loss train: 0.04819205775856972\n",
            "i 4\n",
            "epoch 52\n",
            " batch Loss train: 0.04633648693561554\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 52\n",
            " batch Loss train: 0.05510023608803749\n",
            "i 6\n",
            "epoch 52\n",
            " batch Loss train: 0.0327686071395874\n",
            "i 7\n",
            "epoch 52\n",
            " batch Loss train: 0.0523262619972229\n",
            "i 8\n",
            "epoch 52\n",
            " batch Loss train: 0.045597631484270096\n",
            "i 9\n",
            "epoch 52\n",
            " batch Loss train: 0.05221695452928543\n",
            "i 10\n",
            "epoch 52\n",
            " batch Loss train: 0.041054416447877884\n",
            "i 11\n",
            "epoch 52\n",
            " batch Loss train: 0.045557744801044464\n",
            "i 12\n",
            "epoch 52\n",
            " batch Loss train: 0.03750133886933327\n",
            "i 13\n",
            "epoch 52\n",
            " batch Loss train: 0.06051325798034668\n",
            "i 14\n",
            "epoch 52\n",
            " batch Loss train: 0.047225821763277054\n",
            "i 15\n",
            "epoch 52\n",
            " batch Loss train: 0.05431489646434784\n",
            "i 16\n",
            "epoch 52\n",
            " batch Loss train: 0.037449173629283905\n",
            "i 17\n",
            "epoch 52\n",
            " batch Loss train: 0.058054253458976746\n",
            "i 18\n",
            "epoch 52\n",
            " batch Loss train: 0.06774483621120453\n",
            "i 19\n",
            "epoch 52\n",
            " batch Loss train: 0.05310089886188507\n",
            "i 20\n",
            "epoch 52\n",
            " batch Loss train: 0.04713006317615509\n",
            "i 21\n",
            "epoch 52\n",
            " batch Loss train: 0.051172565668821335\n",
            "i 22\n",
            "epoch 52\n",
            " batch Loss train: 0.059825897216796875\n",
            "i 23\n",
            "epoch 52\n",
            " batch Loss train: 0.04744568094611168\n",
            "i 24\n",
            "epoch 52\n",
            " batch Loss train: 0.04000915586948395\n",
            "i 25\n",
            "epoch 52\n",
            " batch Loss train: 0.053160592913627625\n",
            "i 26\n",
            "epoch 52\n",
            " batch Loss train: 0.04651612415909767\n",
            "i 27\n",
            "epoch 52\n",
            " batch Loss train: 0.05273277685046196\n",
            "i 28\n",
            "epoch 52\n",
            " batch Loss train: 0.036203764379024506\n",
            "i 29\n",
            "epoch 52\n",
            " batch Loss train: 0.04558604583144188\n",
            "i 30\n",
            "epoch 52\n",
            " batch Loss train: 0.05214378610253334\n",
            "i 31\n",
            "epoch 52\n",
            " batch Loss train: 0.03636780008673668\n",
            "i 32\n",
            "epoch 52\n",
            " batch Loss train: 0.03670772910118103\n",
            "i 33\n",
            "epoch 52\n",
            " batch Loss train: 0.03900093957781792\n",
            "i 34\n",
            "epoch 52\n",
            " batch Loss train: 0.04585679620504379\n",
            "i 35\n",
            "epoch 52\n",
            " batch Loss train: 0.0652388259768486\n",
            "i 36\n",
            "epoch 52\n",
            " batch Loss train: 0.05336127430200577\n",
            "i 37\n",
            "epoch 52\n",
            " batch Loss train: 0.05809877812862396\n",
            "i 38\n",
            "epoch 52\n",
            " batch Loss train: 0.03379758074879646\n",
            "i 39\n",
            "epoch 52\n",
            " batch Loss train: 0.05464489385485649\n",
            "i 40\n",
            "epoch 52\n",
            " batch Loss train: 0.0572013296186924\n",
            "i 41\n",
            "epoch 52\n",
            " batch Loss train: 0.05865618586540222\n",
            "i 42\n",
            "epoch 52\n",
            " batch Loss train: 0.030604418367147446\n",
            "i 43\n",
            "epoch 52\n",
            " batch Loss train: 0.04597647115588188\n",
            "i 44\n",
            "epoch 52\n",
            " batch Loss train: 0.050645433366298676\n",
            "i 45\n",
            "epoch 52\n",
            " batch Loss train: 0.039880961179733276\n",
            "i 46\n",
            "epoch 52\n",
            " batch Loss train: 0.04457669332623482\n",
            "i 47\n",
            "epoch 52\n",
            " batch Loss train: 0.049149248749017715\n",
            "i 48\n",
            "epoch 52\n",
            " batch Loss train: 0.04929431527853012\n",
            "i 49\n",
            "epoch 52\n",
            " batch Loss train: 0.049312423914670944\n",
            "i 50\n",
            "epoch 52\n",
            " batch Loss train: 0.04197019338607788\n",
            "i 51\n",
            "epoch 52\n",
            " batch Loss train: 0.05446433648467064\n",
            "i 52\n",
            "epoch 52\n",
            " batch Loss train: 0.05629774555563927\n",
            "i 53\n",
            "epoch 52\n",
            " batch Loss train: 0.04399758577346802\n",
            "i 54\n",
            "epoch 52\n",
            " batch Loss train: 0.0375387966632843\n",
            "i 55\n",
            "epoch 52\n",
            " batch Loss train: 0.06966739892959595\n",
            "i 56\n",
            "epoch 52\n",
            " batch Loss train: 0.057996124029159546\n",
            "i 57\n",
            "epoch 52\n",
            " batch Loss train: 0.0604628287255764\n",
            "i 58\n",
            "epoch 52\n",
            " batch Loss train: 0.05625321716070175\n",
            "i 59\n",
            "epoch 52\n",
            " batch Loss train: 0.04172082617878914\n",
            "i 60\n",
            "epoch 52\n",
            " batch Loss train: 0.03600053861737251\n",
            "i 61\n",
            "epoch 52\n",
            " batch Loss train: 0.036842040717601776\n",
            "i 62\n",
            "epoch 52\n",
            " batch Loss train: 0.06607543677091599\n",
            "i 63\n",
            "epoch 52\n",
            " batch Loss train: 0.04553196579217911\n",
            "i 64\n",
            "epoch 52\n",
            " batch Loss train: 0.04744367301464081\n",
            "i 65\n",
            "epoch 52\n",
            " batch Loss train: 0.04608038440346718\n",
            "i 66\n",
            "epoch 52\n",
            " batch Loss train: 0.050824545323848724\n",
            "i 67\n",
            "epoch 52\n",
            " batch Loss train: 0.05482266843318939\n",
            "i 68\n",
            "epoch 52\n",
            " batch Loss train: 0.05134079232811928\n",
            "i 69\n",
            "epoch 52\n",
            " batch Loss train: 0.059427861124277115\n",
            "i 70\n",
            "epoch 52\n",
            " batch Loss train: 0.04948120564222336\n",
            "i 71\n",
            "epoch 52\n",
            " batch Loss train: 0.03510338440537453\n",
            "i 72\n",
            "epoch 52\n",
            " batch Loss train: 0.0583120733499527\n",
            "i 73\n",
            "epoch 52\n",
            " batch Loss train: 0.034826911985874176\n",
            "i 74\n",
            "epoch 52\n",
            " batch Loss train: 0.04237452521920204\n",
            "i 75\n",
            "epoch 52\n",
            " batch Loss train: 0.036930251866579056\n",
            "i 76\n",
            "epoch 52\n",
            " batch Loss train: 0.059040337800979614\n",
            "i 77\n",
            "epoch 52\n",
            " batch Loss train: 0.049306247383356094\n",
            "i 78\n",
            "epoch 52\n",
            " batch Loss train: 0.044774867594242096\n",
            "i 79\n",
            "epoch 52\n",
            " batch Loss train: 0.06642064452171326\n",
            "i 80\n",
            "epoch 52\n",
            " batch Loss train: 0.05459411442279816\n",
            "i 81\n",
            "epoch 52\n",
            " batch Loss train: 0.03959687054157257\n",
            "i 82\n",
            "epoch 52\n",
            " batch Loss train: 0.03822655975818634\n",
            "i 83\n",
            "epoch 52\n",
            " batch Loss train: 0.04348927363753319\n",
            "i 84\n",
            "epoch 52\n",
            " batch Loss train: 0.042617253959178925\n",
            "i 85\n",
            "epoch 52\n",
            " batch Loss train: 0.06306876987218857\n",
            "i 86\n",
            "epoch 52\n",
            " batch Loss train: 0.03330766409635544\n",
            "i 87\n",
            "epoch 52\n",
            " batch Loss train: 0.06231467053294182\n",
            "i 88\n",
            "epoch 52\n",
            " batch Loss train: 0.05307649448513985\n",
            "i 89\n",
            "epoch 52\n",
            " batch Loss train: 0.0558442622423172\n",
            "i 90\n",
            "epoch 52\n",
            " batch Loss train: 0.05919360741972923\n",
            "i 91\n",
            "epoch 52\n",
            " batch Loss train: 0.059725258499383926\n",
            "i 92\n",
            "epoch 52\n",
            " batch Loss train: 0.038414981216192245\n",
            "i 93\n",
            "epoch 52\n",
            " batch Loss train: 0.036418765783309937\n",
            "i 94\n",
            "epoch 52\n",
            " batch Loss train: 0.03704331815242767\n",
            "i 95\n",
            "epoch 52\n",
            " batch Loss train: 0.04897995665669441\n",
            "i 96\n",
            "epoch 52\n",
            " batch Loss train: 0.040755197405815125\n",
            "i 97\n",
            "epoch 52\n",
            " batch Loss train: 0.06555227190256119\n",
            "i 98\n",
            "epoch 52\n",
            " batch Loss train: 0.05851384624838829\n",
            "i 99\n",
            "epoch 52\n",
            " batch Loss train: 0.036400262266397476\n",
            "i 100\n",
            "epoch 52\n",
            " batch Loss train: 0.05306348204612732\n",
            "i 101\n",
            "epoch 52\n",
            " batch Loss train: 0.05673805996775627\n",
            "i 102\n",
            "epoch 52\n",
            " batch Loss train: 0.042453985661268234\n",
            "i 103\n",
            "epoch 52\n",
            " batch Loss train: 0.052936263382434845\n",
            "i 104\n",
            "epoch 52\n",
            " batch Loss train: 0.03952501714229584\n",
            "i 105\n",
            "epoch 52\n",
            " batch Loss train: 0.05802203342318535\n",
            "i 106\n",
            "epoch 52\n",
            " batch Loss train: 0.05414610356092453\n",
            "i 107\n",
            "epoch 52\n",
            " batch Loss train: 0.0421803779900074\n",
            "i 108\n",
            "epoch 52\n",
            " batch Loss train: 0.05311267450451851\n",
            "i 109\n",
            "epoch 52\n",
            " batch Loss train: 0.0425802581012249\n",
            "i 110\n",
            "epoch 52\n",
            " batch Loss train: 0.043492428958415985\n",
            "i 111\n",
            "epoch 52\n",
            " batch Loss train: 0.066266730427742\n",
            "i 112\n",
            "epoch 52\n",
            " batch Loss train: 0.03828710690140724\n",
            "i 113\n",
            "epoch 52\n",
            " batch Loss train: 0.041106633841991425\n",
            "i 114\n",
            "epoch 52\n",
            " batch Loss train: 0.053537413477897644\n",
            "i 115\n",
            "epoch 52\n",
            " batch Loss train: 0.04518589749932289\n",
            "i 116\n",
            "epoch 52\n",
            " batch Loss train: 0.055337246507406235\n",
            "i 117\n",
            "epoch 52\n",
            " batch Loss train: 0.05788207799196243\n",
            "i 118\n",
            "epoch 52\n",
            " batch Loss train: 0.0415647029876709\n",
            "i 119\n",
            "epoch 52\n",
            " batch Loss train: 0.034535106271505356\n",
            "i 120\n",
            "epoch 52\n",
            " batch Loss train: 0.058566782623529434\n",
            "i 121\n",
            "epoch 52\n",
            " batch Loss train: 0.049703214317560196\n",
            "i 122\n",
            "epoch 52\n",
            " batch Loss train: 0.07212179899215698\n",
            "i 123\n",
            "epoch 52\n",
            " batch Loss train: 0.0521131306886673\n",
            "i 124\n",
            "epoch 52\n",
            " batch Loss train: 0.050676945596933365\n",
            "i 125\n",
            "epoch 52\n",
            " batch Loss train: 0.03642192855477333\n",
            "i 126\n",
            "epoch 52\n",
            " batch Loss train: 0.03286962956190109\n",
            "i 127\n",
            "epoch 52\n",
            " batch Loss train: 0.037364594638347626\n",
            "i 128\n",
            "epoch 52\n",
            " batch Loss train: 0.041213732212781906\n",
            "i 129\n",
            "epoch 52\n",
            " batch Loss train: 0.053187333047389984\n",
            "i 130\n",
            "epoch 52\n",
            " batch Loss train: 0.049992457032203674\n",
            "i 131\n",
            "epoch 52\n",
            " batch Loss train: 0.046240635216236115\n",
            "i 132\n",
            "epoch 52\n",
            " batch Loss train: 0.05144994705915451\n",
            "i 133\n",
            "epoch 52\n",
            " batch Loss train: 0.04911210387945175\n",
            "i 134\n",
            "epoch 52\n",
            " batch Loss train: 0.0565408430993557\n",
            "i 135\n",
            "epoch 52\n",
            " batch Loss train: 0.03391724079847336\n",
            "i 136\n",
            "epoch 52\n",
            " batch Loss train: 0.04376831278204918\n",
            "i 137\n",
            "epoch 52\n",
            " batch Loss train: 0.03984209895133972\n",
            "i 138\n",
            "epoch 52\n",
            " batch Loss train: 0.044409461319446564\n",
            "i 139\n",
            "epoch 52\n",
            " batch Loss train: 0.05515670031309128\n",
            "i 140\n",
            "epoch 52\n",
            " batch Loss train: 0.042422693222761154\n",
            "i 141\n",
            "epoch 52\n",
            " batch Loss train: 0.037351563572883606\n",
            "i 142\n",
            "epoch 52\n",
            " batch Loss train: 0.0580083392560482\n",
            "i 143\n",
            "epoch 52\n",
            " batch Loss train: 0.05710569769144058\n",
            "i 144\n",
            "epoch 52\n",
            " batch Loss train: 0.05176694691181183\n",
            "i 145\n",
            "epoch 52\n",
            " batch Loss train: 0.041486624628305435\n",
            "i 146\n",
            "epoch 52\n",
            " batch Loss train: 0.050409622490406036\n",
            "i 147\n",
            "epoch 52\n",
            " batch Loss train: 0.05552659183740616\n",
            "i 148\n",
            "epoch 52\n",
            " batch Loss train: 0.045565471053123474\n",
            "i 149\n",
            "epoch 52\n",
            " batch Loss train: 0.04768151417374611\n",
            "i 150\n",
            "epoch 52\n",
            " batch Loss train: 0.04936295747756958\n",
            "i 151\n",
            "epoch 52\n",
            " batch Loss train: 0.041555121541023254\n",
            "i 152\n",
            "epoch 52\n",
            " batch Loss train: 0.04281618073582649\n",
            "i 153\n",
            "epoch 52\n",
            " batch Loss train: 0.044828347861766815\n",
            "i 154\n",
            "epoch 52\n",
            " batch Loss train: 0.053127486258745193\n",
            "i 155\n",
            "epoch 52\n",
            " batch Loss train: 0.052970994263887405\n",
            "i 156\n",
            "epoch 52\n",
            " batch Loss train: 0.04899807646870613\n",
            "i 157\n",
            "epoch 52\n",
            " batch Loss train: 0.038188185542821884\n",
            "i 158\n",
            "epoch 52\n",
            " batch Loss train: 0.05207716301083565\n",
            "i 159\n",
            "epoch 52\n",
            " batch Loss train: 0.04323914274573326\n",
            "i 160\n",
            "epoch 52\n",
            " batch Loss train: 0.055356238037347794\n",
            "i 161\n",
            "epoch 52\n",
            " batch Loss train: 0.046795982867479324\n",
            "i 162\n",
            "epoch 52\n",
            " batch Loss train: 0.05102010443806648\n",
            "i 163\n",
            "epoch 52\n",
            " batch Loss train: 0.044204600155353546\n",
            "i 164\n",
            "epoch 52\n",
            " batch Loss train: 0.03805387020111084\n",
            "i 165\n",
            "epoch 52\n",
            " batch Loss train: 0.054226141422986984\n",
            "i 166\n",
            "epoch 52\n",
            " batch Loss train: 0.028852874413132668\n",
            "i 167\n",
            "epoch 52\n",
            " batch Loss train: 0.06530364602804184\n",
            "i 168\n",
            "epoch 52\n",
            " batch Loss train: 0.03827714920043945\n",
            "i 169\n",
            "epoch 52\n",
            " batch Loss train: 0.035953838378190994\n",
            "i 170\n",
            "epoch 52\n",
            " batch Loss train: 0.043201643973588943\n",
            "i 171\n",
            "epoch 52\n",
            " batch Loss train: 0.04427817836403847\n",
            "i 172\n",
            "epoch 52\n",
            " batch Loss train: 0.04728665575385094\n",
            "i 173\n",
            "epoch 52\n",
            " batch Loss train: 0.06378047168254852\n",
            "i 174\n",
            "epoch 52\n",
            " batch Loss train: 0.03604551777243614\n",
            "i 175\n",
            "epoch 52\n",
            " batch Loss train: 0.047682732343673706\n",
            "i 176\n",
            "epoch 52\n",
            " batch Loss train: 0.03638019412755966\n",
            "i 177\n",
            "epoch 52\n",
            " batch Loss train: 0.04899405688047409\n",
            "i 178\n",
            "epoch 52\n",
            " batch Loss train: 0.041664812713861465\n",
            "i 179\n",
            "epoch 52\n",
            " batch Loss train: 0.04954487085342407\n",
            "i 180\n",
            "epoch 52\n",
            " batch Loss train: 0.03832576423883438\n",
            "i 181\n",
            "epoch 52\n",
            " batch Loss train: 0.04463673382997513\n",
            "i 182\n",
            "epoch 52\n",
            " batch Loss train: 0.03790636733174324\n",
            "i 183\n",
            "epoch 52\n",
            " batch Loss train: 0.057130325585603714\n",
            "i 184\n",
            "epoch 52\n",
            " batch Loss train: 0.04308504983782768\n",
            "i 185\n",
            "epoch 52\n",
            " batch Loss train: 0.04121538996696472\n",
            "i 186\n",
            "epoch 52\n",
            " batch Loss train: 0.06018473207950592\n",
            "i 187\n",
            "epoch 52\n",
            " batch Loss train: 0.04456896707415581\n",
            "i 188\n",
            "epoch 52\n",
            " batch Loss train: 0.055065035820007324\n",
            "i 189\n",
            "epoch 52\n",
            " batch Loss train: 0.05230189487338066\n",
            "i 190\n",
            "epoch 52\n",
            " batch Loss train: 0.062391266226768494\n",
            "i 191\n",
            "epoch 52\n",
            " batch Loss train: 0.05344153940677643\n",
            "i 192\n",
            "epoch 52\n",
            " batch Loss train: 0.04695792496204376\n",
            "i 193\n",
            "epoch 52\n",
            " batch Loss train: 0.052922144532203674\n",
            "i 194\n",
            "epoch 52\n",
            " batch Loss train: 0.03415971249341965\n",
            "i 195\n",
            "epoch 52\n",
            " batch Loss train: 0.055479250848293304\n",
            "i 196\n",
            "epoch 52\n",
            " batch Loss train: 0.06131415814161301\n",
            "i 197\n",
            "epoch 52\n",
            " batch Loss train: 0.05191511660814285\n",
            "i 198\n",
            "epoch 52\n",
            " batch Loss train: 0.04478985071182251\n",
            "i 199\n",
            "epoch 52\n",
            " batch Loss train: 0.038991451263427734\n",
            "i 200\n",
            "epoch 52\n",
            " batch Loss train: 0.0401558093726635\n",
            "i 201\n",
            "epoch 52\n",
            " batch Loss train: 0.047047581523656845\n",
            "i 202\n",
            "epoch 52\n",
            " batch Loss train: 0.05992640182375908\n",
            "i 203\n",
            "epoch 52\n",
            " batch Loss train: 0.05881611630320549\n",
            "i 204\n",
            "epoch 52\n",
            " batch Loss train: 0.052447736263275146\n",
            "i 205\n",
            "epoch 52\n",
            " batch Loss train: 0.05348685011267662\n",
            "i 206\n",
            "epoch 52\n",
            " batch Loss train: 0.048633940517902374\n",
            "i 207\n",
            "epoch 52\n",
            " batch Loss train: 0.04867471754550934\n",
            "i 208\n",
            "epoch 52\n",
            " batch Loss train: 0.05645839869976044\n",
            "i 209\n",
            "epoch 52\n",
            " batch Loss train: 0.038006193935871124\n",
            "i 210\n",
            "epoch 52\n",
            " batch Loss train: 0.03796740993857384\n",
            "i 211\n",
            "epoch 52\n",
            " batch Loss train: 0.049033038318157196\n",
            "i 212\n",
            "epoch 52\n",
            " batch Loss train: 0.041006818413734436\n",
            "i 213\n",
            "epoch 52\n",
            " batch Loss train: 0.054809123277664185\n",
            "i 214\n",
            "epoch 52\n",
            " batch Loss train: 0.04890250042080879\n",
            "i 215\n",
            "epoch 52\n",
            " batch Loss train: 0.0458676777780056\n",
            "i 216\n",
            "epoch 52\n",
            " batch Loss train: 0.047339193522930145\n",
            "i 217\n",
            "epoch 52\n",
            " batch Loss train: 0.04436029866337776\n",
            "i 218\n",
            "epoch 52\n",
            " batch Loss train: 0.048386674374341965\n",
            "i 219\n",
            "epoch 52\n",
            " batch Loss train: 0.0454682931303978\n",
            "i 220\n",
            "epoch 52\n",
            " batch Loss train: 0.0785381942987442\n",
            "i 221\n",
            "epoch 52\n",
            " batch Loss train: 0.06463329493999481\n",
            "i 222\n",
            "epoch 52\n",
            " batch Loss train: 0.03712330013513565\n",
            "i 223\n",
            "epoch 52\n",
            " batch Loss train: 0.04603461176156998\n",
            "i 224\n",
            "epoch 52\n",
            " batch Loss train: 0.035813137888908386\n",
            "i 225\n",
            "epoch 52\n",
            " batch Loss train: 0.06866008788347244\n",
            "i 226\n",
            "epoch 52\n",
            " batch Loss train: 0.053401168435811996\n",
            "i 227\n",
            "epoch 52\n",
            " batch Loss train: 0.0363331139087677\n",
            "i 228\n",
            "epoch 52\n",
            " batch Loss train: 0.05753685161471367\n",
            "i 229\n",
            "epoch 52\n",
            " batch Loss train: 0.0491488091647625\n",
            "i 230\n",
            "epoch 52\n",
            " batch Loss train: 0.042598627507686615\n",
            "i 231\n",
            "epoch 52\n",
            " batch Loss train: 0.058199841529130936\n",
            "i 232\n",
            "epoch 52\n",
            " batch Loss train: 0.05168895795941353\n",
            "i 233\n",
            "epoch 52\n",
            " batch Loss train: 0.057233281433582306\n",
            "i 234\n",
            "epoch 52\n",
            " batch Loss train: 0.060866907238960266\n",
            "i 235\n",
            "epoch 52\n",
            " batch Loss train: 0.048847801983356476\n",
            "i 236\n",
            "epoch 52\n",
            " batch Loss train: 0.04313028231263161\n",
            "i 237\n",
            "epoch 52\n",
            " batch Loss train: 0.04590941593050957\n",
            "i 238\n",
            "epoch 52\n",
            " batch Loss train: 0.06046219915151596\n",
            "i 239\n",
            "epoch 52\n",
            " batch Loss train: 0.03937337175011635\n",
            "i 240\n",
            "epoch 52\n",
            " batch Loss train: 0.05134938284754753\n",
            "i 241\n",
            "epoch 52\n",
            " batch Loss train: 0.036559149622917175\n",
            "i 242\n",
            "epoch 52\n",
            " batch Loss train: 0.060066744685173035\n",
            "i 243\n",
            "epoch 52\n",
            " batch Loss train: 0.037136469036340714\n",
            "i 244\n",
            "epoch 52\n",
            " batch Loss train: 0.07593393325805664\n",
            "i 245\n",
            "epoch 52\n",
            " batch Loss train: 0.05312587693333626\n",
            "i 246\n",
            "epoch 52\n",
            " batch Loss train: 0.044991496950387955\n",
            "i 247\n",
            "epoch 52\n",
            " batch Loss train: 0.046483226120471954\n",
            "i 248\n",
            "epoch 52\n",
            " batch Loss train: 0.05070369318127632\n",
            "i 249\n",
            "epoch 52\n",
            " batch Loss train: 0.06318207085132599\n",
            "i 250\n",
            "epoch 52\n",
            " batch Loss train: 0.056602843105793\n",
            "i 251\n",
            "epoch 52\n",
            " batch Loss train: 0.0407177172601223\n",
            "i 252\n",
            "epoch 52\n",
            " batch Loss train: 0.05122177675366402\n",
            "i 253\n",
            "epoch 52\n",
            " batch Loss train: 0.05930328741669655\n",
            "i 254\n",
            "epoch 52\n",
            " batch Loss train: 0.04791067913174629\n",
            "i 255\n",
            "epoch 52\n",
            " batch Loss train: 0.06494621932506561\n",
            "i 256\n",
            "epoch 52\n",
            " batch Loss train: 0.03272916376590729\n",
            "i 257\n",
            "epoch 52\n",
            " batch Loss train: 0.05007075518369675\n",
            "i 258\n",
            "epoch 52\n",
            " batch Loss train: 0.0593205988407135\n",
            "i 259\n",
            "epoch 52\n",
            " batch Loss train: 0.04836862534284592\n",
            "i 260\n",
            "epoch 52\n",
            " batch Loss train: 0.04389152675867081\n",
            "i 261\n",
            "epoch 52\n",
            " batch Loss train: 0.04585529491305351\n",
            "i 262\n",
            "epoch 52\n",
            " batch Loss train: 0.04191404581069946\n",
            "i 263\n",
            "epoch 52\n",
            " batch Loss train: 0.055326253175735474\n",
            "i 264\n",
            "epoch 52\n",
            " batch Loss train: 0.04433783143758774\n",
            "i 265\n",
            "epoch 52\n",
            " batch Loss train: 0.060010287910699844\n",
            "i 266\n",
            "epoch 52\n",
            " batch Loss train: 0.05394073948264122\n",
            "i 267\n",
            "epoch 52\n",
            " batch Loss train: 0.06116810441017151\n",
            "i 268\n",
            "epoch 52\n",
            " batch Loss train: 0.046235281974077225\n",
            "i 269\n",
            "epoch 52\n",
            " batch Loss train: 0.04531245678663254\n",
            "i 270\n",
            "epoch 52\n",
            " batch Loss train: 0.04269332066178322\n",
            "i 271\n",
            "epoch 52\n",
            " batch Loss train: 0.05595983937382698\n",
            "i 272\n",
            "epoch 52\n",
            " batch Loss train: 0.06203511357307434\n",
            "i 273\n",
            "epoch 52\n",
            " batch Loss train: 0.06070243567228317\n",
            "i 274\n",
            "epoch 52\n",
            " batch Loss train: 0.04460586979985237\n",
            "i 275\n",
            "epoch 52\n",
            " batch Loss train: 0.04742087423801422\n",
            "i 276\n",
            "epoch 52\n",
            " batch Loss train: 0.057011768221855164\n",
            "i 277\n",
            "epoch 52\n",
            " batch Loss train: 0.07172199338674545\n",
            "i 278\n",
            "epoch 52\n",
            " batch Loss train: 0.05851206183433533\n",
            "i 279\n",
            "epoch 52\n",
            " batch Loss train: 0.03180134296417236\n",
            "i 280\n",
            "epoch 52\n",
            " batch Loss train: 0.037580858916044235\n",
            "i 281\n",
            "epoch 52\n",
            " batch Loss train: 0.048193302005529404\n",
            "i 282\n",
            "epoch 52\n",
            " batch Loss train: 0.04268248751759529\n",
            "i 283\n",
            "epoch 52\n",
            " batch Loss train: 0.04560722038149834\n",
            "i 284\n",
            "epoch 52\n",
            " batch Loss train: 0.05002365633845329\n",
            "i 285\n",
            "epoch 52\n",
            " batch Loss train: 0.05340461805462837\n",
            "i 286\n",
            "epoch 52\n",
            " batch Loss train: 0.052703943103551865\n",
            "i 287\n",
            "epoch 52\n",
            " batch Loss train: 0.05578680709004402\n",
            "i 288\n",
            "epoch 52\n",
            " batch Loss train: 0.05181702598929405\n",
            "i 289\n",
            "epoch 52\n",
            " batch Loss train: 0.052707087248563766\n",
            "i 290\n",
            "epoch 52\n",
            " batch Loss train: 0.045262232422828674\n",
            "i 291\n",
            "epoch 52\n",
            " batch Loss train: 0.04148012399673462\n",
            "i 292\n",
            "epoch 52\n",
            " batch Loss train: 0.04490812495350838\n",
            "i 293\n",
            "epoch 52\n",
            " batch Loss train: 0.05267128720879555\n",
            "i 294\n",
            "epoch 52\n",
            " batch Loss train: 0.06205174699425697\n",
            "i 295\n",
            "epoch 52\n",
            " batch Loss train: 0.047152552753686905\n",
            "i 296\n",
            "epoch 52\n",
            " batch Loss train: 0.05970030277967453\n",
            "i 297\n",
            "epoch 52\n",
            " batch Loss train: 0.06598198413848877\n",
            "i 298\n",
            "epoch 52\n",
            " batch Loss train: 0.052183713763952255\n",
            "i 299\n",
            "epoch 52\n",
            " batch Loss train: 0.05298283323645592\n",
            "i 300\n",
            "epoch 52\n",
            " batch Loss train: 0.0769110843539238\n",
            "i 301\n",
            "epoch 52\n",
            " batch Loss train: 0.06554552912712097\n",
            "i 302\n",
            "epoch 52\n",
            " batch Loss train: 0.046123210340738297\n",
            "i 303\n",
            "epoch 52\n",
            " batch Loss train: 0.06503934413194656\n",
            "i 304\n",
            "epoch 52\n",
            " batch Loss train: 0.061610352247953415\n",
            "i 305\n",
            "epoch 52\n",
            " batch Loss train: 0.061030853539705276\n",
            "i 306\n",
            "epoch 52\n",
            " batch Loss train: 0.05363230034708977\n",
            "i 307\n",
            "epoch 52\n",
            " batch Loss train: 0.05887170135974884\n",
            "i 308\n",
            "epoch 52\n",
            " batch Loss train: 0.058504387736320496\n",
            "i 309\n",
            "epoch 52\n",
            " batch Loss train: 0.055018503218889236\n",
            "i 310\n",
            "epoch 52\n",
            " batch Loss train: 0.06057242304086685\n",
            "i 311\n",
            "epoch 52\n",
            " batch Loss train: 0.042686957865953445\n",
            "i 312\n",
            "epoch 52\n",
            " batch Loss train: 0.05217079073190689\n",
            "i 313\n",
            "epoch 52\n",
            " batch Loss train: 0.04383040592074394\n",
            "i 314\n",
            "epoch 52\n",
            " batch Loss train: 0.04899708554148674\n",
            "i 315\n",
            "epoch 52\n",
            " batch Loss train: 0.07574283331632614\n",
            "i 316\n",
            "epoch 52\n",
            " batch Loss train: 0.04780929908156395\n",
            "i 317\n",
            "epoch 52\n",
            " batch Loss train: 0.07418350130319595\n",
            "i 318\n",
            "epoch 52\n",
            " batch Loss train: 0.056097544729709625\n",
            "i 319\n",
            "epoch 52\n",
            " batch Loss train: 0.0519803948700428\n",
            "i 320\n",
            "epoch 52\n",
            " batch Loss train: 0.06001745164394379\n",
            "i 321\n",
            "epoch 52\n",
            " batch Loss train: 0.06413937360048294\n",
            "i 322\n",
            "epoch 52\n",
            " batch Loss train: 0.057013269513845444\n",
            "i 323\n",
            "epoch 52\n",
            " batch Loss train: 0.05633129924535751\n",
            "i 324\n",
            "epoch 52\n",
            " batch Loss train: 0.04861569032073021\n",
            "i 325\n",
            "epoch 52\n",
            " batch Loss train: 0.07019022107124329\n",
            "i 326\n",
            "epoch 52\n",
            " batch Loss train: 0.057313911616802216\n",
            "i 327\n",
            "epoch 52\n",
            " batch Loss train: 0.042321059852838516\n",
            "i 328\n",
            "epoch 52\n",
            " batch Loss train: 0.06574826687574387\n",
            "i 329\n",
            "epoch 52\n",
            " batch Loss train: 0.05670516565442085\n",
            "i 330\n",
            "epoch 52\n",
            " batch Loss train: 0.05941319465637207\n",
            "i 331\n",
            "epoch 52\n",
            " batch Loss train: 0.04672487452626228\n",
            "i 332\n",
            "epoch 52\n",
            " batch Loss train: 0.04371461644768715\n",
            "i 333\n",
            "epoch 52\n",
            " batch Loss train: 0.05560903251171112\n",
            "i 334\n",
            "epoch 52\n",
            " batch Loss train: 0.04621240496635437\n",
            "i 335\n",
            "epoch 52\n",
            " batch Loss train: 0.05725402012467384\n",
            "i 336\n",
            "epoch 52\n",
            " batch Loss train: 0.049980539828538895\n",
            "i 337\n",
            "epoch 52\n",
            " batch Loss train: 0.04370710998773575\n",
            "i 338\n",
            "epoch 52\n",
            " batch Loss train: 0.04765438660979271\n",
            "i 339\n",
            "epoch 52\n",
            " batch Loss train: 0.046591904014348984\n",
            "i 340\n",
            "epoch 52\n",
            " batch Loss train: 0.0684380978345871\n",
            "i 341\n",
            "epoch 52\n",
            " batch Loss train: 0.05905339494347572\n",
            "i 342\n",
            "epoch 52\n",
            " batch Loss train: 0.04065924137830734\n",
            "i 343\n",
            "epoch 52\n",
            " batch Loss train: 0.09238961338996887\n",
            "i 344\n",
            "epoch 52\n",
            " batch Loss train: 0.035290561616420746\n",
            "i 345\n",
            "epoch 52\n",
            " batch Loss train: 0.05666056647896767\n",
            "i 346\n",
            "epoch 52\n",
            " batch Loss train: 0.03947751596570015\n",
            "i 347\n",
            "epoch 52\n",
            " batch Loss train: 0.05470441281795502\n",
            "i 348\n",
            "epoch 52\n",
            " batch Loss train: 0.06623950600624084\n",
            "i 349\n",
            "epoch 52\n",
            " batch Loss train: 0.0489162914454937\n",
            "i 350\n",
            "epoch 52\n",
            " batch Loss train: 0.047380611300468445\n",
            "i 351\n",
            "epoch 52\n",
            " batch Loss train: 0.05742952972650528\n",
            "i 352\n",
            "epoch 52\n",
            " batch Loss train: 0.0931239202618599\n",
            "i 353\n",
            "epoch 52\n",
            " batch Loss train: 0.057507339864969254\n",
            "i 354\n",
            "epoch 52\n",
            " batch Loss train: 0.04781372845172882\n",
            "i 355\n",
            "epoch 52\n",
            " batch Loss train: 0.06426449120044708\n",
            "i 356\n",
            "epoch 52\n",
            " batch Loss train: 0.05873913690447807\n",
            "i 357\n",
            "epoch 52\n",
            " batch Loss train: 0.06274695694446564\n",
            "i 358\n",
            "epoch 52\n",
            " batch Loss train: 0.05384480208158493\n",
            "i 359\n",
            "epoch 52\n",
            " batch Loss train: 0.04903099685907364\n",
            "i 360\n",
            "epoch 52\n",
            " batch Loss train: 0.04698576405644417\n",
            "i 361\n",
            "epoch 52\n",
            " batch Loss train: 0.039319027215242386\n",
            "i 362\n",
            "epoch 52\n",
            " batch Loss train: 0.04340047016739845\n",
            "i 363\n",
            "epoch 52\n",
            " batch Loss train: 0.0622837133705616\n",
            "i 364\n",
            "epoch 52\n",
            " batch Loss train: 0.06925678998231888\n",
            "i 365\n",
            "epoch 52\n",
            " batch Loss train: 0.04975000396370888\n",
            "i 366\n",
            "epoch 52\n",
            " batch Loss train: 0.05168875679373741\n",
            "i 367\n",
            "epoch 52\n",
            " batch Loss train: 0.05573693662881851\n",
            "i 368\n",
            "epoch 52\n",
            " batch Loss train: 0.054734908044338226\n",
            "i 369\n",
            "epoch 52\n",
            " batch Loss train: 0.05180750787258148\n",
            "i 370\n",
            "epoch 52\n",
            " batch Loss train: 0.053709566593170166\n",
            "i 371\n",
            "epoch 52\n",
            " batch Loss train: 0.04541764035820961\n",
            "i 372\n",
            "epoch 52\n",
            " batch Loss train: 0.05035728961229324\n",
            "i 373\n",
            "epoch 52\n",
            " batch Loss train: 0.07060883194208145\n",
            "i 374\n",
            "epoch 52\n",
            " batch Loss train: 0.06265384703874588\n",
            "i 375\n",
            "epoch 52\n",
            " batch Loss train: 0.05287326127290726\n",
            "i 376\n",
            "epoch 52\n",
            " batch Loss train: 0.09567882865667343\n",
            "i 377\n",
            "epoch 52\n",
            " batch Loss train: 0.04313025623559952\n",
            "i 378\n",
            "epoch 52\n",
            " batch Loss train: 0.0508204810321331\n",
            "i 379\n",
            "epoch 52\n",
            " batch Loss train: 0.05351034551858902\n",
            "i 380\n",
            "epoch 52\n",
            " batch Loss train: 0.05833958834409714\n",
            "i 381\n",
            "epoch 52\n",
            " batch Loss train: 0.05393174663186073\n",
            "i 382\n",
            "epoch 52\n",
            " batch Loss train: 0.046373993158340454\n",
            "i 383\n",
            "epoch 52\n",
            " batch Loss train: 0.04570241644978523\n",
            "i 384\n",
            "epoch 52\n",
            " batch Loss train: 0.050322242081165314\n",
            "i 385\n",
            "epoch 52\n",
            " batch Loss train: 0.06340264528989792\n",
            "i 386\n",
            "epoch 52\n",
            " batch Loss train: 0.06206497922539711\n",
            "i 387\n",
            "epoch 52\n",
            " batch Loss train: 0.04401459917426109\n",
            "i 388\n",
            "epoch 52\n",
            " batch Loss train: 0.047885630279779434\n",
            "i 389\n",
            "epoch 52\n",
            " batch Loss train: 0.0685066506266594\n",
            "i 390\n",
            "epoch 52\n",
            " batch Loss train: 0.11239660531282425\n",
            "i 391\n",
            "epoch 52\n",
            " batch Loss train: 0.08620790392160416\n",
            "i 392\n",
            "epoch 52\n",
            " batch Loss train: 0.06084104999899864\n",
            "i 393\n",
            "epoch 52\n",
            " batch Loss train: 0.0707123652100563\n",
            "i 394\n",
            "epoch 52\n",
            " batch Loss train: 0.05646732077002525\n",
            "i 395\n",
            "epoch 52\n",
            " batch Loss train: 0.08285507559776306\n",
            "i 396\n",
            "epoch 52\n",
            " batch Loss train: 0.0544317401945591\n",
            "i 397\n",
            "epoch 52\n",
            " batch Loss train: 0.05459262803196907\n",
            "i 398\n",
            "epoch 52\n",
            " batch Loss train: 0.05963168293237686\n",
            "i 399\n",
            "epoch 52\n",
            " batch Loss train: 0.06186067685484886\n",
            "i 400\n",
            "epoch 52\n",
            " batch Loss train: 0.05479941889643669\n",
            "i 401\n",
            "epoch 52\n",
            " batch Loss train: 0.05009501427412033\n",
            "i 402\n",
            "epoch 52\n",
            " batch Loss train: 0.046511780470609665\n",
            "i 403\n",
            "epoch 52\n",
            " batch Loss train: 0.05256455019116402\n",
            "i 404\n",
            "epoch 52\n",
            " batch Loss train: 0.06154312938451767\n",
            "i 405\n",
            "epoch 52\n",
            " batch Loss train: 0.04337252676486969\n",
            "i 406\n",
            "epoch 52\n",
            " batch Loss train: 0.06907687336206436\n",
            "i 407\n",
            "epoch 52\n",
            " batch Loss train: 0.050756972283124924\n",
            "i 408\n",
            "epoch 52\n",
            " batch Loss train: 0.056187085807323456\n",
            "i 409\n",
            "epoch 52\n",
            " batch Loss train: 0.06135845184326172\n",
            "i 410\n",
            "epoch 52\n",
            " batch Loss train: 0.05681614577770233\n",
            "i 411\n",
            "epoch 52\n",
            " batch Loss train: 0.05580445006489754\n",
            "i 412\n",
            "epoch 52\n",
            " batch Loss train: 0.057154253125190735\n",
            "i 413\n",
            "epoch 52\n",
            " batch Loss train: 0.062282416969537735\n",
            "i 414\n",
            "epoch 52\n",
            " batch Loss train: 0.04989858716726303\n",
            "i 415\n",
            "epoch 52\n",
            " batch Loss train: 0.05303729325532913\n",
            "i 416\n",
            "epoch 52\n",
            " batch Loss train: 0.06589413434267044\n",
            "i 417\n",
            "epoch 52\n",
            " batch Loss train: 0.06947066634893417\n",
            "i 418\n",
            "epoch 52\n",
            " batch Loss train: 0.060767292976379395\n",
            "i 419\n",
            "epoch 52\n",
            " batch Loss train: 0.06078317388892174\n",
            "i 420\n",
            "epoch 52\n",
            " batch Loss train: 0.0409114696085453\n",
            "i 421\n",
            "epoch 52\n",
            " batch Loss train: 0.04908594489097595\n",
            "i 422\n",
            "epoch 52\n",
            " batch Loss train: 0.058962684124708176\n",
            "i 423\n",
            "epoch 52\n",
            " batch Loss train: 0.04513733088970184\n",
            "i 424\n",
            "epoch 52\n",
            " batch Loss train: 0.055453866720199585\n",
            "i 425\n",
            "epoch 52\n",
            " batch Loss train: 0.06466002017259598\n",
            "i 426\n",
            "epoch 52\n",
            " batch Loss train: 0.07925479859113693\n",
            "i 427\n",
            "epoch 52\n",
            " batch Loss train: 0.047468576580286026\n",
            "i 428\n",
            "epoch 52\n",
            " batch Loss train: 0.05236499384045601\n",
            "i 429\n",
            "epoch 52\n",
            " batch Loss train: 0.06700393557548523\n",
            "i 430\n",
            "epoch 52\n",
            " batch Loss train: 0.05849574878811836\n",
            "i 431\n",
            "epoch 52\n",
            " batch Loss train: 0.05479063466191292\n",
            "i 432\n",
            "epoch 52\n",
            " batch Loss train: 0.053679656237363815\n",
            "i 433\n",
            "epoch 52\n",
            " batch Loss train: 0.05644349008798599\n",
            "i 434\n",
            "epoch 52\n",
            " batch Loss train: 0.06060510128736496\n",
            "i 435\n",
            "epoch 52\n",
            " batch Loss train: 0.07013854384422302\n",
            "i 436\n",
            "epoch 52\n",
            " batch Loss train: 0.05700290948152542\n",
            "i 437\n",
            "epoch 52\n",
            " batch Loss train: 0.08513307571411133\n",
            "i 438\n",
            "epoch 52\n",
            " batch Loss train: 0.03318227082490921\n",
            "i 439\n",
            "epoch 52\n",
            " batch Loss train: 0.059926461428403854\n",
            "i 440\n",
            "epoch 52\n",
            " batch Loss train: 0.0850721001625061\n",
            "i 441\n",
            "epoch 52\n",
            " batch Loss train: 0.050970129668712616\n",
            "i 442\n",
            "epoch 52\n",
            " batch Loss train: 0.0513920858502388\n",
            "i 443\n",
            "epoch 52\n",
            " batch Loss train: 0.05256664752960205\n",
            "i 444\n",
            "epoch 52\n",
            " batch Loss train: 0.06274045258760452\n",
            "i 445\n",
            "epoch 52\n",
            " batch Loss train: 0.04791950061917305\n",
            "total epoch Loss train: tensor(0.0479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 53\n",
            " batch Loss train: 0.04927492514252663\n",
            "i 1\n",
            "epoch 53\n",
            " batch Loss train: 0.05184713751077652\n",
            "i 2\n",
            "epoch 53\n",
            " batch Loss train: 0.04416070133447647\n",
            "i 3\n",
            "epoch 53\n",
            " batch Loss train: 0.04378875717520714\n",
            "i 4\n",
            "epoch 53\n",
            " batch Loss train: 0.035780563950538635\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 53\n",
            " batch Loss train: 0.05048660933971405\n",
            "i 6\n",
            "epoch 53\n",
            " batch Loss train: 0.045689500868320465\n",
            "i 7\n",
            "epoch 53\n",
            " batch Loss train: 0.051332734525203705\n",
            "i 8\n",
            "epoch 53\n",
            " batch Loss train: 0.03832632303237915\n",
            "i 9\n",
            "epoch 53\n",
            " batch Loss train: 0.04896659404039383\n",
            "i 10\n",
            "epoch 53\n",
            " batch Loss train: 0.049854397773742676\n",
            "i 11\n",
            "epoch 53\n",
            " batch Loss train: 0.049099795520305634\n",
            "i 12\n",
            "epoch 53\n",
            " batch Loss train: 0.05552205443382263\n",
            "i 13\n",
            "epoch 53\n",
            " batch Loss train: 0.05026334151625633\n",
            "i 14\n",
            "epoch 53\n",
            " batch Loss train: 0.06092418357729912\n",
            "i 15\n",
            "epoch 53\n",
            " batch Loss train: 0.03458515182137489\n",
            "i 16\n",
            "epoch 53\n",
            " batch Loss train: 0.0615377239882946\n",
            "i 17\n",
            "epoch 53\n",
            " batch Loss train: 0.050698958337306976\n",
            "i 18\n",
            "epoch 53\n",
            " batch Loss train: 0.07358664274215698\n",
            "i 19\n",
            "epoch 53\n",
            " batch Loss train: 0.035215962678194046\n",
            "i 20\n",
            "epoch 53\n",
            " batch Loss train: 0.049183908849954605\n",
            "i 21\n",
            "epoch 53\n",
            " batch Loss train: 0.043117981404066086\n",
            "i 22\n",
            "epoch 53\n",
            " batch Loss train: 0.03327842801809311\n",
            "i 23\n",
            "epoch 53\n",
            " batch Loss train: 0.03627963364124298\n",
            "i 24\n",
            "epoch 53\n",
            " batch Loss train: 0.07103163748979568\n",
            "i 25\n",
            "epoch 53\n",
            " batch Loss train: 0.026663098484277725\n",
            "i 26\n",
            "epoch 53\n",
            " batch Loss train: 0.03860330954194069\n",
            "i 27\n",
            "epoch 53\n",
            " batch Loss train: 0.039270054548978806\n",
            "i 28\n",
            "epoch 53\n",
            " batch Loss train: 0.03824218735098839\n",
            "i 29\n",
            "epoch 53\n",
            " batch Loss train: 0.05203644558787346\n",
            "i 30\n",
            "epoch 53\n",
            " batch Loss train: 0.04766004905104637\n",
            "i 31\n",
            "epoch 53\n",
            " batch Loss train: 0.05397636443376541\n",
            "i 32\n",
            "epoch 53\n",
            " batch Loss train: 0.04464186728000641\n",
            "i 33\n",
            "epoch 53\n",
            " batch Loss train: 0.04526924341917038\n",
            "i 34\n",
            "epoch 53\n",
            " batch Loss train: 0.06691648811101913\n",
            "i 35\n",
            "epoch 53\n",
            " batch Loss train: 0.041300009936094284\n",
            "i 36\n",
            "epoch 53\n",
            " batch Loss train: 0.03146073967218399\n",
            "i 37\n",
            "epoch 53\n",
            " batch Loss train: 0.04515378922224045\n",
            "i 38\n",
            "epoch 53\n",
            " batch Loss train: 0.04747866094112396\n",
            "i 39\n",
            "epoch 53\n",
            " batch Loss train: 0.04476480185985565\n",
            "i 40\n",
            "epoch 53\n",
            " batch Loss train: 0.051172491163015366\n",
            "i 41\n",
            "epoch 53\n",
            " batch Loss train: 0.051847878843545914\n",
            "i 42\n",
            "epoch 53\n",
            " batch Loss train: 0.05692823603749275\n",
            "i 43\n",
            "epoch 53\n",
            " batch Loss train: 0.04569963738322258\n",
            "i 44\n",
            "epoch 53\n",
            " batch Loss train: 0.04153654724359512\n",
            "i 45\n",
            "epoch 53\n",
            " batch Loss train: 0.05019679293036461\n",
            "i 46\n",
            "epoch 53\n",
            " batch Loss train: 0.044406991451978683\n",
            "i 47\n",
            "epoch 53\n",
            " batch Loss train: 0.04902295395731926\n",
            "i 48\n",
            "epoch 53\n",
            " batch Loss train: 0.0470537394285202\n",
            "i 49\n",
            "epoch 53\n",
            " batch Loss train: 0.031917959451675415\n",
            "i 50\n",
            "epoch 53\n",
            " batch Loss train: 0.03422119468450546\n",
            "i 51\n",
            "epoch 53\n",
            " batch Loss train: 0.03744306415319443\n",
            "i 52\n",
            "epoch 53\n",
            " batch Loss train: 0.07048721611499786\n",
            "i 53\n",
            "epoch 53\n",
            " batch Loss train: 0.06434691697359085\n",
            "i 54\n",
            "epoch 53\n",
            " batch Loss train: 0.04513254016637802\n",
            "i 55\n",
            "epoch 53\n",
            " batch Loss train: 0.042613301426172256\n",
            "i 56\n",
            "epoch 53\n",
            " batch Loss train: 0.03500745818018913\n",
            "i 57\n",
            "epoch 53\n",
            " batch Loss train: 0.04725873842835426\n",
            "i 58\n",
            "epoch 53\n",
            " batch Loss train: 0.031919170171022415\n",
            "i 59\n",
            "epoch 53\n",
            " batch Loss train: 0.06336088478565216\n",
            "i 60\n",
            "epoch 53\n",
            " batch Loss train: 0.0416073352098465\n",
            "i 61\n",
            "epoch 53\n",
            " batch Loss train: 0.04832461103796959\n",
            "i 62\n",
            "epoch 53\n",
            " batch Loss train: 0.04329431802034378\n",
            "i 63\n",
            "epoch 53\n",
            " batch Loss train: 0.034347519278526306\n",
            "i 64\n",
            "epoch 53\n",
            " batch Loss train: 0.0535164438188076\n",
            "i 65\n",
            "epoch 53\n",
            " batch Loss train: 0.04474792629480362\n",
            "i 66\n",
            "epoch 53\n",
            " batch Loss train: 0.04448991268873215\n",
            "i 67\n",
            "epoch 53\n",
            " batch Loss train: 0.057740405201911926\n",
            "i 68\n",
            "epoch 53\n",
            " batch Loss train: 0.06407178193330765\n",
            "i 69\n",
            "epoch 53\n",
            " batch Loss train: 0.05271388217806816\n",
            "i 70\n",
            "epoch 53\n",
            " batch Loss train: 0.050156548619270325\n",
            "i 71\n",
            "epoch 53\n",
            " batch Loss train: 0.057505082339048386\n",
            "i 72\n",
            "epoch 53\n",
            " batch Loss train: 0.07736838608980179\n",
            "i 73\n",
            "epoch 53\n",
            " batch Loss train: 0.033868223428726196\n",
            "i 74\n",
            "epoch 53\n",
            " batch Loss train: 0.04834320768713951\n",
            "i 75\n",
            "epoch 53\n",
            " batch Loss train: 0.05877186357975006\n",
            "i 76\n",
            "epoch 53\n",
            " batch Loss train: 0.0406760647892952\n",
            "i 77\n",
            "epoch 53\n",
            " batch Loss train: 0.06300026923418045\n",
            "i 78\n",
            "epoch 53\n",
            " batch Loss train: 0.044152166694402695\n",
            "i 79\n",
            "epoch 53\n",
            " batch Loss train: 0.03628071770071983\n",
            "i 80\n",
            "epoch 53\n",
            " batch Loss train: 0.05801552161574364\n",
            "i 81\n",
            "epoch 53\n",
            " batch Loss train: 0.03567591682076454\n",
            "i 82\n",
            "epoch 53\n",
            " batch Loss train: 0.08127143234014511\n",
            "i 83\n",
            "epoch 53\n",
            " batch Loss train: 0.054391250014305115\n",
            "i 84\n",
            "epoch 53\n",
            " batch Loss train: 0.03559528663754463\n",
            "i 85\n",
            "epoch 53\n",
            " batch Loss train: 0.04354539141058922\n",
            "i 86\n",
            "epoch 53\n",
            " batch Loss train: 0.0468265675008297\n",
            "i 87\n",
            "epoch 53\n",
            " batch Loss train: 0.05446156859397888\n",
            "i 88\n",
            "epoch 53\n",
            " batch Loss train: 0.03655416518449783\n",
            "i 89\n",
            "epoch 53\n",
            " batch Loss train: 0.059476613998413086\n",
            "i 90\n",
            "epoch 53\n",
            " batch Loss train: 0.05408982187509537\n",
            "i 91\n",
            "epoch 53\n",
            " batch Loss train: 0.04627273976802826\n",
            "i 92\n",
            "epoch 53\n",
            " batch Loss train: 0.04306071251630783\n",
            "i 93\n",
            "epoch 53\n",
            " batch Loss train: 0.03635140135884285\n",
            "i 94\n",
            "epoch 53\n",
            " batch Loss train: 0.029745014384388924\n",
            "i 95\n",
            "epoch 53\n",
            " batch Loss train: 0.0642474815249443\n",
            "i 96\n",
            "epoch 53\n",
            " batch Loss train: 0.05003742501139641\n",
            "i 97\n",
            "epoch 53\n",
            " batch Loss train: 0.045041415840387344\n",
            "i 98\n",
            "epoch 53\n",
            " batch Loss train: 0.05853084847331047\n",
            "i 99\n",
            "epoch 53\n",
            " batch Loss train: 0.04322100058197975\n",
            "i 100\n",
            "epoch 53\n",
            " batch Loss train: 0.04653378203511238\n",
            "i 101\n",
            "epoch 53\n",
            " batch Loss train: 0.0579521507024765\n",
            "i 102\n",
            "epoch 53\n",
            " batch Loss train: 0.04755471274256706\n",
            "i 103\n",
            "epoch 53\n",
            " batch Loss train: 0.04676676541566849\n",
            "i 104\n",
            "epoch 53\n",
            " batch Loss train: 0.04238740727305412\n",
            "i 105\n",
            "epoch 53\n",
            " batch Loss train: 0.05243817716836929\n",
            "i 106\n",
            "epoch 53\n",
            " batch Loss train: 0.04894845560193062\n",
            "i 107\n",
            "epoch 53\n",
            " batch Loss train: 0.050754059106111526\n",
            "i 108\n",
            "epoch 53\n",
            " batch Loss train: 0.041744623333215714\n",
            "i 109\n",
            "epoch 53\n",
            " batch Loss train: 0.038271673023700714\n",
            "i 110\n",
            "epoch 53\n",
            " batch Loss train: 0.06879796087741852\n",
            "i 111\n",
            "epoch 53\n",
            " batch Loss train: 0.05111537501215935\n",
            "i 112\n",
            "epoch 53\n",
            " batch Loss train: 0.043141286820173264\n",
            "i 113\n",
            "epoch 53\n",
            " batch Loss train: 0.04424574226140976\n",
            "i 114\n",
            "epoch 53\n",
            " batch Loss train: 0.05204443261027336\n",
            "i 115\n",
            "epoch 53\n",
            " batch Loss train: 0.04951722174882889\n",
            "i 116\n",
            "epoch 53\n",
            " batch Loss train: 0.052392080426216125\n",
            "i 117\n",
            "epoch 53\n",
            " batch Loss train: 0.04845792427659035\n",
            "i 118\n",
            "epoch 53\n",
            " batch Loss train: 0.04462530091404915\n",
            "i 119\n",
            "epoch 53\n",
            " batch Loss train: 0.06257504224777222\n",
            "i 120\n",
            "epoch 53\n",
            " batch Loss train: 0.05991451069712639\n",
            "i 121\n",
            "epoch 53\n",
            " batch Loss train: 0.04389512538909912\n",
            "i 122\n",
            "epoch 53\n",
            " batch Loss train: 0.049359407275915146\n",
            "i 123\n",
            "epoch 53\n",
            " batch Loss train: 0.05108283460140228\n",
            "i 124\n",
            "epoch 53\n",
            " batch Loss train: 0.05465744435787201\n",
            "i 125\n",
            "epoch 53\n",
            " batch Loss train: 0.05061384662985802\n",
            "i 126\n",
            "epoch 53\n",
            " batch Loss train: 0.05311337485909462\n",
            "i 127\n",
            "epoch 53\n",
            " batch Loss train: 0.051832158118486404\n",
            "i 128\n",
            "epoch 53\n",
            " batch Loss train: 0.06044703722000122\n",
            "i 129\n",
            "epoch 53\n",
            " batch Loss train: 0.056844182312488556\n",
            "i 130\n",
            "epoch 53\n",
            " batch Loss train: 0.07934224605560303\n",
            "i 131\n",
            "epoch 53\n",
            " batch Loss train: 0.06023796647787094\n",
            "i 132\n",
            "epoch 53\n",
            " batch Loss train: 0.04317576810717583\n",
            "i 133\n",
            "epoch 53\n",
            " batch Loss train: 0.057270120829343796\n",
            "i 134\n",
            "epoch 53\n",
            " batch Loss train: 0.03446381539106369\n",
            "i 135\n",
            "epoch 53\n",
            " batch Loss train: 0.05456498637795448\n",
            "i 136\n",
            "epoch 53\n",
            " batch Loss train: 0.05102017521858215\n",
            "i 137\n",
            "epoch 53\n",
            " batch Loss train: 0.03651703894138336\n",
            "i 138\n",
            "epoch 53\n",
            " batch Loss train: 0.04419665038585663\n",
            "i 139\n",
            "epoch 53\n",
            " batch Loss train: 0.052048128098249435\n",
            "i 140\n",
            "epoch 53\n",
            " batch Loss train: 0.04438822343945503\n",
            "i 141\n",
            "epoch 53\n",
            " batch Loss train: 0.047222163528203964\n",
            "i 142\n",
            "epoch 53\n",
            " batch Loss train: 0.03627009317278862\n",
            "i 143\n",
            "epoch 53\n",
            " batch Loss train: 0.042219050228595734\n",
            "i 144\n",
            "epoch 53\n",
            " batch Loss train: 0.04736832529306412\n",
            "i 145\n",
            "epoch 53\n",
            " batch Loss train: 0.036329545080661774\n",
            "i 146\n",
            "epoch 53\n",
            " batch Loss train: 0.046452414244413376\n",
            "i 147\n",
            "epoch 53\n",
            " batch Loss train: 0.05352157726883888\n",
            "i 148\n",
            "epoch 53\n",
            " batch Loss train: 0.056679219007492065\n",
            "i 149\n",
            "epoch 53\n",
            " batch Loss train: 0.04442921653389931\n",
            "i 150\n",
            "epoch 53\n",
            " batch Loss train: 0.04769273102283478\n",
            "i 151\n",
            "epoch 53\n",
            " batch Loss train: 0.04962440952658653\n",
            "i 152\n",
            "epoch 53\n",
            " batch Loss train: 0.05474543571472168\n",
            "i 153\n",
            "epoch 53\n",
            " batch Loss train: 0.04428951069712639\n",
            "i 154\n",
            "epoch 53\n",
            " batch Loss train: 0.046595312654972076\n",
            "i 155\n",
            "epoch 53\n",
            " batch Loss train: 0.047154221683740616\n",
            "i 156\n",
            "epoch 53\n",
            " batch Loss train: 0.03653891384601593\n",
            "i 157\n",
            "epoch 53\n",
            " batch Loss train: 0.055190280079841614\n",
            "i 158\n",
            "epoch 53\n",
            " batch Loss train: 0.04289209097623825\n",
            "i 159\n",
            "epoch 53\n",
            " batch Loss train: 0.04906988516449928\n",
            "i 160\n",
            "epoch 53\n",
            " batch Loss train: 0.06738607585430145\n",
            "i 161\n",
            "epoch 53\n",
            " batch Loss train: 0.03745512291789055\n",
            "i 162\n",
            "epoch 53\n",
            " batch Loss train: 0.054107263684272766\n",
            "i 163\n",
            "epoch 53\n",
            " batch Loss train: 0.05232971906661987\n",
            "i 164\n",
            "epoch 53\n",
            " batch Loss train: 0.04440620541572571\n",
            "i 165\n",
            "epoch 53\n",
            " batch Loss train: 0.06798804551362991\n",
            "i 166\n",
            "epoch 53\n",
            " batch Loss train: 0.04377656802535057\n",
            "i 167\n",
            "epoch 53\n",
            " batch Loss train: 0.051864612847566605\n",
            "i 168\n",
            "epoch 53\n",
            " batch Loss train: 0.03922826796770096\n",
            "i 169\n",
            "epoch 53\n",
            " batch Loss train: 0.05621729791164398\n",
            "i 170\n",
            "epoch 53\n",
            " batch Loss train: 0.05274708569049835\n",
            "i 171\n",
            "epoch 53\n",
            " batch Loss train: 0.047944456338882446\n",
            "i 172\n",
            "epoch 53\n",
            " batch Loss train: 0.05768868327140808\n",
            "i 173\n",
            "epoch 53\n",
            " batch Loss train: 0.06255198270082474\n",
            "i 174\n",
            "epoch 53\n",
            " batch Loss train: 0.035661280155181885\n",
            "i 175\n",
            "epoch 53\n",
            " batch Loss train: 0.08520909398794174\n",
            "i 176\n",
            "epoch 53\n",
            " batch Loss train: 0.053177185356616974\n",
            "i 177\n",
            "epoch 53\n",
            " batch Loss train: 0.046446140855550766\n",
            "i 178\n",
            "epoch 53\n",
            " batch Loss train: 0.036823026835918427\n",
            "i 179\n",
            "epoch 53\n",
            " batch Loss train: 0.039323195815086365\n",
            "i 180\n",
            "epoch 53\n",
            " batch Loss train: 0.040011659264564514\n",
            "i 181\n",
            "epoch 53\n",
            " batch Loss train: 0.061653982847929\n",
            "i 182\n",
            "epoch 53\n",
            " batch Loss train: 0.06594464927911758\n",
            "i 183\n",
            "epoch 53\n",
            " batch Loss train: 0.04987592250108719\n",
            "i 184\n",
            "epoch 53\n",
            " batch Loss train: 0.043408021330833435\n",
            "i 185\n",
            "epoch 53\n",
            " batch Loss train: 0.04906953498721123\n",
            "i 186\n",
            "epoch 53\n",
            " batch Loss train: 0.06761521846055984\n",
            "i 187\n",
            "epoch 53\n",
            " batch Loss train: 0.06178091838955879\n",
            "i 188\n",
            "epoch 53\n",
            " batch Loss train: 0.06993229687213898\n",
            "i 189\n",
            "epoch 53\n",
            " batch Loss train: 0.0379638634622097\n",
            "i 190\n",
            "epoch 53\n",
            " batch Loss train: 0.0406867191195488\n",
            "i 191\n",
            "epoch 53\n",
            " batch Loss train: 0.03904278203845024\n",
            "i 192\n",
            "epoch 53\n",
            " batch Loss train: 0.04364588484168053\n",
            "i 193\n",
            "epoch 53\n",
            " batch Loss train: 0.048445530235767365\n",
            "i 194\n",
            "epoch 53\n",
            " batch Loss train: 0.04812771454453468\n",
            "i 195\n",
            "epoch 53\n",
            " batch Loss train: 0.051975999027490616\n",
            "i 196\n",
            "epoch 53\n",
            " batch Loss train: 0.03465935215353966\n",
            "i 197\n",
            "epoch 53\n",
            " batch Loss train: 0.04740486666560173\n",
            "i 198\n",
            "epoch 53\n",
            " batch Loss train: 0.057522762566804886\n",
            "i 199\n",
            "epoch 53\n",
            " batch Loss train: 0.04916533827781677\n",
            "i 200\n",
            "epoch 53\n",
            " batch Loss train: 0.04342534393072128\n",
            "i 201\n",
            "epoch 53\n",
            " batch Loss train: 0.04476446285843849\n",
            "i 202\n",
            "epoch 53\n",
            " batch Loss train: 0.04774586483836174\n",
            "i 203\n",
            "epoch 53\n",
            " batch Loss train: 0.046138979494571686\n",
            "i 204\n",
            "epoch 53\n",
            " batch Loss train: 0.04121262952685356\n",
            "i 205\n",
            "epoch 53\n",
            " batch Loss train: 0.033929452300071716\n",
            "i 206\n",
            "epoch 53\n",
            " batch Loss train: 0.0840781033039093\n",
            "i 207\n",
            "epoch 53\n",
            " batch Loss train: 0.03221668675541878\n",
            "i 208\n",
            "epoch 53\n",
            " batch Loss train: 0.053376682102680206\n",
            "i 209\n",
            "epoch 53\n",
            " batch Loss train: 0.04676170274615288\n",
            "i 210\n",
            "epoch 53\n",
            " batch Loss train: 0.050571244210004807\n",
            "i 211\n",
            "epoch 53\n",
            " batch Loss train: 0.05120709910988808\n",
            "i 212\n",
            "epoch 53\n",
            " batch Loss train: 0.06043907627463341\n",
            "i 213\n",
            "epoch 53\n",
            " batch Loss train: 0.05118684843182564\n",
            "i 214\n",
            "epoch 53\n",
            " batch Loss train: 0.0452093742787838\n",
            "i 215\n",
            "epoch 53\n",
            " batch Loss train: 0.05959576368331909\n",
            "i 216\n",
            "epoch 53\n",
            " batch Loss train: 0.048825953155756\n",
            "i 217\n",
            "epoch 53\n",
            " batch Loss train: 0.039620909839868546\n",
            "i 218\n",
            "epoch 53\n",
            " batch Loss train: 0.03898533061146736\n",
            "i 219\n",
            "epoch 53\n",
            " batch Loss train: 0.05045230686664581\n",
            "i 220\n",
            "epoch 53\n",
            " batch Loss train: 0.04250242933630943\n",
            "i 221\n",
            "epoch 53\n",
            " batch Loss train: 0.05572683736681938\n",
            "i 222\n",
            "epoch 53\n",
            " batch Loss train: 0.04856928437948227\n",
            "i 223\n",
            "epoch 53\n",
            " batch Loss train: 0.04248836636543274\n",
            "i 224\n",
            "epoch 53\n",
            " batch Loss train: 0.05050613358616829\n",
            "i 225\n",
            "epoch 53\n",
            " batch Loss train: 0.05412531644105911\n",
            "i 226\n",
            "epoch 53\n",
            " batch Loss train: 0.04983289912343025\n",
            "i 227\n",
            "epoch 53\n",
            " batch Loss train: 0.05982746183872223\n",
            "i 228\n",
            "epoch 53\n",
            " batch Loss train: 0.05177208408713341\n",
            "i 229\n",
            "epoch 53\n",
            " batch Loss train: 0.05150846391916275\n",
            "i 230\n",
            "epoch 53\n",
            " batch Loss train: 0.03716672211885452\n",
            "i 231\n",
            "epoch 53\n",
            " batch Loss train: 0.04321509972214699\n",
            "i 232\n",
            "epoch 53\n",
            " batch Loss train: 0.06890513747930527\n",
            "i 233\n",
            "epoch 53\n",
            " batch Loss train: 0.047451917082071304\n",
            "i 234\n",
            "epoch 53\n",
            " batch Loss train: 0.05279864743351936\n",
            "i 235\n",
            "epoch 53\n",
            " batch Loss train: 0.03499273955821991\n",
            "i 236\n",
            "epoch 53\n",
            " batch Loss train: 0.05089207738637924\n",
            "i 237\n",
            "epoch 53\n",
            " batch Loss train: 0.04311208799481392\n",
            "i 238\n",
            "epoch 53\n",
            " batch Loss train: 0.04063433036208153\n",
            "i 239\n",
            "epoch 53\n",
            " batch Loss train: 0.049188561737537384\n",
            "i 240\n",
            "epoch 53\n",
            " batch Loss train: 0.03834846615791321\n",
            "i 241\n",
            "epoch 53\n",
            " batch Loss train: 0.04991820454597473\n",
            "i 242\n",
            "epoch 53\n",
            " batch Loss train: 0.05758370831608772\n",
            "i 243\n",
            "epoch 53\n",
            " batch Loss train: 0.046127498149871826\n",
            "i 244\n",
            "epoch 53\n",
            " batch Loss train: 0.06052446737885475\n",
            "i 245\n",
            "epoch 53\n",
            " batch Loss train: 0.040402475744485855\n",
            "i 246\n",
            "epoch 53\n",
            " batch Loss train: 0.047887805849313736\n",
            "i 247\n",
            "epoch 53\n",
            " batch Loss train: 0.04970857873558998\n",
            "i 248\n",
            "epoch 53\n",
            " batch Loss train: 0.07608602941036224\n",
            "i 249\n",
            "epoch 53\n",
            " batch Loss train: 0.03716278076171875\n",
            "i 250\n",
            "epoch 53\n",
            " batch Loss train: 0.054962288588285446\n",
            "i 251\n",
            "epoch 53\n",
            " batch Loss train: 0.05258716642856598\n",
            "i 252\n",
            "epoch 53\n",
            " batch Loss train: 0.06085454300045967\n",
            "i 253\n",
            "epoch 53\n",
            " batch Loss train: 0.03823623061180115\n",
            "i 254\n",
            "epoch 53\n",
            " batch Loss train: 0.03753399848937988\n",
            "i 255\n",
            "epoch 53\n",
            " batch Loss train: 0.05084117874503136\n",
            "i 256\n",
            "epoch 53\n",
            " batch Loss train: 0.05843765288591385\n",
            "i 257\n",
            "epoch 53\n",
            " batch Loss train: 0.051217783242464066\n",
            "i 258\n",
            "epoch 53\n",
            " batch Loss train: 0.04628688469529152\n",
            "i 259\n",
            "epoch 53\n",
            " batch Loss train: 0.05490458756685257\n",
            "i 260\n",
            "epoch 53\n",
            " batch Loss train: 0.06434693187475204\n",
            "i 261\n",
            "epoch 53\n",
            " batch Loss train: 0.05020967498421669\n",
            "i 262\n",
            "epoch 53\n",
            " batch Loss train: 0.052308499813079834\n",
            "i 263\n",
            "epoch 53\n",
            " batch Loss train: 0.07090029865503311\n",
            "i 264\n",
            "epoch 53\n",
            " batch Loss train: 0.04896722361445427\n",
            "i 265\n",
            "epoch 53\n",
            " batch Loss train: 0.051968786865472794\n",
            "i 266\n",
            "epoch 53\n",
            " batch Loss train: 0.05644489452242851\n",
            "i 267\n",
            "epoch 53\n",
            " batch Loss train: 0.050771404057741165\n",
            "i 268\n",
            "epoch 53\n",
            " batch Loss train: 0.050855617970228195\n",
            "i 269\n",
            "epoch 53\n",
            " batch Loss train: 0.05116504430770874\n",
            "i 270\n",
            "epoch 53\n",
            " batch Loss train: 0.06700325012207031\n",
            "i 271\n",
            "epoch 53\n",
            " batch Loss train: 0.03409958630800247\n",
            "i 272\n",
            "epoch 53\n",
            " batch Loss train: 0.05120999366044998\n",
            "i 273\n",
            "epoch 53\n",
            " batch Loss train: 0.041723206639289856\n",
            "i 274\n",
            "epoch 53\n",
            " batch Loss train: 0.05262918770313263\n",
            "i 275\n",
            "epoch 53\n",
            " batch Loss train: 0.037351276725530624\n",
            "i 276\n",
            "epoch 53\n",
            " batch Loss train: 0.04432764649391174\n",
            "i 277\n",
            "epoch 53\n",
            " batch Loss train: 0.05319732800126076\n",
            "i 278\n",
            "epoch 53\n",
            " batch Loss train: 0.05248311534523964\n",
            "i 279\n",
            "epoch 53\n",
            " batch Loss train: 0.056965164840221405\n",
            "i 280\n",
            "epoch 53\n",
            " batch Loss train: 0.04631480574607849\n",
            "i 281\n",
            "epoch 53\n",
            " batch Loss train: 0.06026510149240494\n",
            "i 282\n",
            "epoch 53\n",
            " batch Loss train: 0.05657843500375748\n",
            "i 283\n",
            "epoch 53\n",
            " batch Loss train: 0.03578170761466026\n",
            "i 284\n",
            "epoch 53\n",
            " batch Loss train: 0.0551372766494751\n",
            "i 285\n",
            "epoch 53\n",
            " batch Loss train: 0.03331534564495087\n",
            "i 286\n",
            "epoch 53\n",
            " batch Loss train: 0.05350685864686966\n",
            "i 287\n",
            "epoch 53\n",
            " batch Loss train: 0.04444780573248863\n",
            "i 288\n",
            "epoch 53\n",
            " batch Loss train: 0.05742739886045456\n",
            "i 289\n",
            "epoch 53\n",
            " batch Loss train: 0.06277825683355331\n",
            "i 290\n",
            "epoch 53\n",
            " batch Loss train: 0.0515754334628582\n",
            "i 291\n",
            "epoch 53\n",
            " batch Loss train: 0.07250803709030151\n",
            "i 292\n",
            "epoch 53\n",
            " batch Loss train: 0.05679909884929657\n",
            "i 293\n",
            "epoch 53\n",
            " batch Loss train: 0.05125591903924942\n",
            "i 294\n",
            "epoch 53\n",
            " batch Loss train: 0.0629468783736229\n",
            "i 295\n",
            "epoch 53\n",
            " batch Loss train: 0.045669108629226685\n",
            "i 296\n",
            "epoch 53\n",
            " batch Loss train: 0.03446485474705696\n",
            "i 297\n",
            "epoch 53\n",
            " batch Loss train: 0.07441014051437378\n",
            "i 298\n",
            "epoch 53\n",
            " batch Loss train: 0.07303749024868011\n",
            "i 299\n",
            "epoch 53\n",
            " batch Loss train: 0.05515637621283531\n",
            "i 300\n",
            "epoch 53\n",
            " batch Loss train: 0.041074372828006744\n",
            "i 301\n",
            "epoch 53\n",
            " batch Loss train: 0.049435749650001526\n",
            "i 302\n",
            "epoch 53\n",
            " batch Loss train: 0.04741306230425835\n",
            "i 303\n",
            "epoch 53\n",
            " batch Loss train: 0.048179104924201965\n",
            "i 304\n",
            "epoch 53\n",
            " batch Loss train: 0.04153931885957718\n",
            "i 305\n",
            "epoch 53\n",
            " batch Loss train: 0.046786826103925705\n",
            "i 306\n",
            "epoch 53\n",
            " batch Loss train: 0.04203943535685539\n",
            "i 307\n",
            "epoch 53\n",
            " batch Loss train: 0.050202298909425735\n",
            "i 308\n",
            "epoch 53\n",
            " batch Loss train: 0.05444222316145897\n",
            "i 309\n",
            "epoch 53\n",
            " batch Loss train: 0.04883897304534912\n",
            "i 310\n",
            "epoch 53\n",
            " batch Loss train: 0.0642237439751625\n",
            "i 311\n",
            "epoch 53\n",
            " batch Loss train: 0.04047311097383499\n",
            "i 312\n",
            "epoch 53\n",
            " batch Loss train: 0.047572117298841476\n",
            "i 313\n",
            "epoch 53\n",
            " batch Loss train: 0.055343713611364365\n",
            "i 314\n",
            "epoch 53\n",
            " batch Loss train: 0.050316184759140015\n",
            "i 315\n",
            "epoch 53\n",
            " batch Loss train: 0.05314777418971062\n",
            "i 316\n",
            "epoch 53\n",
            " batch Loss train: 0.04702888801693916\n",
            "i 317\n",
            "epoch 53\n",
            " batch Loss train: 0.0485936664044857\n",
            "i 318\n",
            "epoch 53\n",
            " batch Loss train: 0.060609035193920135\n",
            "i 319\n",
            "epoch 53\n",
            " batch Loss train: 0.05981377884745598\n",
            "i 320\n",
            "epoch 53\n",
            " batch Loss train: 0.05607232823967934\n",
            "i 321\n",
            "epoch 53\n",
            " batch Loss train: 0.05093168839812279\n",
            "i 322\n",
            "epoch 53\n",
            " batch Loss train: 0.05477739870548248\n",
            "i 323\n",
            "epoch 53\n",
            " batch Loss train: 0.04997405782341957\n",
            "i 324\n",
            "epoch 53\n",
            " batch Loss train: 0.05357201024889946\n",
            "i 325\n",
            "epoch 53\n",
            " batch Loss train: 0.04942861944437027\n",
            "i 326\n",
            "epoch 53\n",
            " batch Loss train: 0.0626087337732315\n",
            "i 327\n",
            "epoch 53\n",
            " batch Loss train: 0.04484117031097412\n",
            "i 328\n",
            "epoch 53\n",
            " batch Loss train: 0.05107303708791733\n",
            "i 329\n",
            "epoch 53\n",
            " batch Loss train: 0.05000891536474228\n",
            "i 330\n",
            "epoch 53\n",
            " batch Loss train: 0.055526044219732285\n",
            "i 331\n",
            "epoch 53\n",
            " batch Loss train: 0.05017925053834915\n",
            "i 332\n",
            "epoch 53\n",
            " batch Loss train: 0.07198292016983032\n",
            "i 333\n",
            "epoch 53\n",
            " batch Loss train: 0.06424025446176529\n",
            "i 334\n",
            "epoch 53\n",
            " batch Loss train: 0.057430390268564224\n",
            "i 335\n",
            "epoch 53\n",
            " batch Loss train: 0.05812877044081688\n",
            "i 336\n",
            "epoch 53\n",
            " batch Loss train: 0.06448770314455032\n",
            "i 337\n",
            "epoch 53\n",
            " batch Loss train: 0.05338247865438461\n",
            "i 338\n",
            "epoch 53\n",
            " batch Loss train: 0.05775108188390732\n",
            "i 339\n",
            "epoch 53\n",
            " batch Loss train: 0.0528537780046463\n",
            "i 340\n",
            "epoch 53\n",
            " batch Loss train: 0.06115889921784401\n",
            "i 341\n",
            "epoch 53\n",
            " batch Loss train: 0.0676790252327919\n",
            "i 342\n",
            "epoch 53\n",
            " batch Loss train: 0.05012040212750435\n",
            "i 343\n",
            "epoch 53\n",
            " batch Loss train: 0.07612641155719757\n",
            "i 344\n",
            "epoch 53\n",
            " batch Loss train: 0.05886412039399147\n",
            "i 345\n",
            "epoch 53\n",
            " batch Loss train: 0.05760541930794716\n",
            "i 346\n",
            "epoch 53\n",
            " batch Loss train: 0.05138729512691498\n",
            "i 347\n",
            "epoch 53\n",
            " batch Loss train: 0.05609753727912903\n",
            "i 348\n",
            "epoch 53\n",
            " batch Loss train: 0.0406092032790184\n",
            "i 349\n",
            "epoch 53\n",
            " batch Loss train: 0.05832426995038986\n",
            "i 350\n",
            "epoch 53\n",
            " batch Loss train: 0.053554944694042206\n",
            "i 351\n",
            "epoch 53\n",
            " batch Loss train: 0.05934443324804306\n",
            "i 352\n",
            "epoch 53\n",
            " batch Loss train: 0.05237361788749695\n",
            "i 353\n",
            "epoch 53\n",
            " batch Loss train: 0.051022373139858246\n",
            "i 354\n",
            "epoch 53\n",
            " batch Loss train: 0.041455283761024475\n",
            "i 355\n",
            "epoch 53\n",
            " batch Loss train: 0.06462934613227844\n",
            "i 356\n",
            "epoch 53\n",
            " batch Loss train: 0.05557285249233246\n",
            "i 357\n",
            "epoch 53\n",
            " batch Loss train: 0.07314988970756531\n",
            "i 358\n",
            "epoch 53\n",
            " batch Loss train: 0.04635436832904816\n",
            "i 359\n",
            "epoch 53\n",
            " batch Loss train: 0.04354920983314514\n",
            "i 360\n",
            "epoch 53\n",
            " batch Loss train: 0.05636458843946457\n",
            "i 361\n",
            "epoch 53\n",
            " batch Loss train: 0.05784941092133522\n",
            "i 362\n",
            "epoch 53\n",
            " batch Loss train: 0.04057208448648453\n",
            "i 363\n",
            "epoch 53\n",
            " batch Loss train: 0.042110543698072433\n",
            "i 364\n",
            "epoch 53\n",
            " batch Loss train: 0.04645933583378792\n",
            "i 365\n",
            "epoch 53\n",
            " batch Loss train: 0.06114135682582855\n",
            "i 366\n",
            "epoch 53\n",
            " batch Loss train: 0.06314919143915176\n",
            "i 367\n",
            "epoch 53\n",
            " batch Loss train: 0.05063242092728615\n",
            "i 368\n",
            "epoch 53\n",
            " batch Loss train: 0.03926892578601837\n",
            "i 369\n",
            "epoch 53\n",
            " batch Loss train: 0.05958357825875282\n",
            "i 370\n",
            "epoch 53\n",
            " batch Loss train: 0.05787255987524986\n",
            "i 371\n",
            "epoch 53\n",
            " batch Loss train: 0.03882942348718643\n",
            "i 372\n",
            "epoch 53\n",
            " batch Loss train: 0.046113044023513794\n",
            "i 373\n",
            "epoch 53\n",
            " batch Loss train: 0.060392215847969055\n",
            "i 374\n",
            "epoch 53\n",
            " batch Loss train: 0.049290064722299576\n",
            "i 375\n",
            "epoch 53\n",
            " batch Loss train: 0.041583020240068436\n",
            "i 376\n",
            "epoch 53\n",
            " batch Loss train: 0.04795019328594208\n",
            "i 377\n",
            "epoch 53\n",
            " batch Loss train: 0.07306139916181564\n",
            "i 378\n",
            "epoch 53\n",
            " batch Loss train: 0.06119656190276146\n",
            "i 379\n",
            "epoch 53\n",
            " batch Loss train: 0.07142498344182968\n",
            "i 380\n",
            "epoch 53\n",
            " batch Loss train: 0.059147968888282776\n",
            "i 381\n",
            "epoch 53\n",
            " batch Loss train: 0.054104942828416824\n",
            "i 382\n",
            "epoch 53\n",
            " batch Loss train: 0.07424750179052353\n",
            "i 383\n",
            "epoch 53\n",
            " batch Loss train: 0.06570546329021454\n",
            "i 384\n",
            "epoch 53\n",
            " batch Loss train: 0.05215686932206154\n",
            "i 385\n",
            "epoch 53\n",
            " batch Loss train: 0.05790970101952553\n",
            "i 386\n",
            "epoch 53\n",
            " batch Loss train: 0.05682231858372688\n",
            "i 387\n",
            "epoch 53\n",
            " batch Loss train: 0.05085504427552223\n",
            "i 388\n",
            "epoch 53\n",
            " batch Loss train: 0.04812593758106232\n",
            "i 389\n",
            "epoch 53\n",
            " batch Loss train: 0.048675574362277985\n",
            "i 390\n",
            "epoch 53\n",
            " batch Loss train: 0.0425560399889946\n",
            "i 391\n",
            "epoch 53\n",
            " batch Loss train: 0.05086451396346092\n",
            "i 392\n",
            "epoch 53\n",
            " batch Loss train: 0.052125848829746246\n",
            "i 393\n",
            "epoch 53\n",
            " batch Loss train: 0.07038504630327225\n",
            "i 394\n",
            "epoch 53\n",
            " batch Loss train: 0.0582071878015995\n",
            "i 395\n",
            "epoch 53\n",
            " batch Loss train: 0.06218073144555092\n",
            "i 396\n",
            "epoch 53\n",
            " batch Loss train: 0.04953163489699364\n",
            "i 397\n",
            "epoch 53\n",
            " batch Loss train: 0.05487753823399544\n",
            "i 398\n",
            "epoch 53\n",
            " batch Loss train: 0.04321780428290367\n",
            "i 399\n",
            "epoch 53\n",
            " batch Loss train: 0.05868346616625786\n",
            "i 400\n",
            "epoch 53\n",
            " batch Loss train: 0.06754061579704285\n",
            "i 401\n",
            "epoch 53\n",
            " batch Loss train: 0.08016633987426758\n",
            "i 402\n",
            "epoch 53\n",
            " batch Loss train: 0.05950629338622093\n",
            "i 403\n",
            "epoch 53\n",
            " batch Loss train: 0.04380455240607262\n",
            "i 404\n",
            "epoch 53\n",
            " batch Loss train: 0.05512147396802902\n",
            "i 405\n",
            "epoch 53\n",
            " batch Loss train: 0.0516170933842659\n",
            "i 406\n",
            "epoch 53\n",
            " batch Loss train: 0.050639647990465164\n",
            "i 407\n",
            "epoch 53\n",
            " batch Loss train: 0.04656444117426872\n",
            "i 408\n",
            "epoch 53\n",
            " batch Loss train: 0.050288792699575424\n",
            "i 409\n",
            "epoch 53\n",
            " batch Loss train: 0.045909009873867035\n",
            "i 410\n",
            "epoch 53\n",
            " batch Loss train: 0.053933728486299515\n",
            "i 411\n",
            "epoch 53\n",
            " batch Loss train: 0.05213398486375809\n",
            "i 412\n",
            "epoch 53\n",
            " batch Loss train: 0.0520797073841095\n",
            "i 413\n",
            "epoch 53\n",
            " batch Loss train: 0.046505119651556015\n",
            "i 414\n",
            "epoch 53\n",
            " batch Loss train: 0.049114976078271866\n",
            "i 415\n",
            "epoch 53\n",
            " batch Loss train: 0.050864629447460175\n",
            "i 416\n",
            "epoch 53\n",
            " batch Loss train: 0.05243746563792229\n",
            "i 417\n",
            "epoch 53\n",
            " batch Loss train: 0.05330120399594307\n",
            "i 418\n",
            "epoch 53\n",
            " batch Loss train: 0.04478777199983597\n",
            "i 419\n",
            "epoch 53\n",
            " batch Loss train: 0.05024368315935135\n",
            "i 420\n",
            "epoch 53\n",
            " batch Loss train: 0.05710046365857124\n",
            "i 421\n",
            "epoch 53\n",
            " batch Loss train: 0.061229027807712555\n",
            "i 422\n",
            "epoch 53\n",
            " batch Loss train: 0.05312353000044823\n",
            "i 423\n",
            "epoch 53\n",
            " batch Loss train: 0.06405594944953918\n",
            "i 424\n",
            "epoch 53\n",
            " batch Loss train: 0.042016882449388504\n",
            "i 425\n",
            "epoch 53\n",
            " batch Loss train: 0.055572330951690674\n",
            "i 426\n",
            "epoch 53\n",
            " batch Loss train: 0.045255228877067566\n",
            "i 427\n",
            "epoch 53\n",
            " batch Loss train: 0.058218155056238174\n",
            "i 428\n",
            "epoch 53\n",
            " batch Loss train: 0.054588865488767624\n",
            "i 429\n",
            "epoch 53\n",
            " batch Loss train: 0.04781166836619377\n",
            "i 430\n",
            "epoch 53\n",
            " batch Loss train: 0.07390322536230087\n",
            "i 431\n",
            "epoch 53\n",
            " batch Loss train: 0.07232781499624252\n",
            "i 432\n",
            "epoch 53\n",
            " batch Loss train: 0.06211191043257713\n",
            "i 433\n",
            "epoch 53\n",
            " batch Loss train: 0.05908312276005745\n",
            "i 434\n",
            "epoch 53\n",
            " batch Loss train: 0.05168100818991661\n",
            "i 435\n",
            "epoch 53\n",
            " batch Loss train: 0.0585535392165184\n",
            "i 436\n",
            "epoch 53\n",
            " batch Loss train: 0.06623142957687378\n",
            "i 437\n",
            "epoch 53\n",
            " batch Loss train: 0.0527212880551815\n",
            "i 438\n",
            "epoch 53\n",
            " batch Loss train: 0.058889731764793396\n",
            "i 439\n",
            "epoch 53\n",
            " batch Loss train: 0.06210264191031456\n",
            "i 440\n",
            "epoch 53\n",
            " batch Loss train: 0.05235903710126877\n",
            "i 441\n",
            "epoch 53\n",
            " batch Loss train: 0.04945901036262512\n",
            "i 442\n",
            "epoch 53\n",
            " batch Loss train: 0.04358575493097305\n",
            "i 443\n",
            "epoch 53\n",
            " batch Loss train: 0.04698291793465614\n",
            "i 444\n",
            "epoch 53\n",
            " batch Loss train: 0.07981765270233154\n",
            "i 445\n",
            "epoch 53\n",
            " batch Loss train: 0.03878801316022873\n",
            "total epoch Loss train: tensor(0.0388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 54\n",
            " batch Loss train: 0.04851331561803818\n",
            "i 1\n",
            "epoch 54\n",
            " batch Loss train: 0.041790418326854706\n",
            "i 2\n",
            "epoch 54\n",
            " batch Loss train: 0.044203124940395355\n",
            "i 3\n",
            "epoch 54\n",
            " batch Loss train: 0.0426744744181633\n",
            "i 4\n",
            "epoch 54\n",
            " batch Loss train: 0.043908990919589996\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 54\n",
            " batch Loss train: 0.042750272899866104\n",
            "i 6\n",
            "epoch 54\n",
            " batch Loss train: 0.054298125207424164\n",
            "i 7\n",
            "epoch 54\n",
            " batch Loss train: 0.055233996361494064\n",
            "i 8\n",
            "epoch 54\n",
            " batch Loss train: 0.052591025829315186\n",
            "i 9\n",
            "epoch 54\n",
            " batch Loss train: 0.054688189178705215\n",
            "i 10\n",
            "epoch 54\n",
            " batch Loss train: 0.043427787721157074\n",
            "i 11\n",
            "epoch 54\n",
            " batch Loss train: 0.0417814739048481\n",
            "i 12\n",
            "epoch 54\n",
            " batch Loss train: 0.0569637194275856\n",
            "i 13\n",
            "epoch 54\n",
            " batch Loss train: 0.04775550961494446\n",
            "i 14\n",
            "epoch 54\n",
            " batch Loss train: 0.036620013415813446\n",
            "i 15\n",
            "epoch 54\n",
            " batch Loss train: 0.055892884731292725\n",
            "i 16\n",
            "epoch 54\n",
            " batch Loss train: 0.03386606648564339\n",
            "i 17\n",
            "epoch 54\n",
            " batch Loss train: 0.033018808811903\n",
            "i 18\n",
            "epoch 54\n",
            " batch Loss train: 0.05449729785323143\n",
            "i 19\n",
            "epoch 54\n",
            " batch Loss train: 0.042510878294706345\n",
            "i 20\n",
            "epoch 54\n",
            " batch Loss train: 0.050912775099277496\n",
            "i 21\n",
            "epoch 54\n",
            " batch Loss train: 0.058999884873628616\n",
            "i 22\n",
            "epoch 54\n",
            " batch Loss train: 0.04940309748053551\n",
            "i 23\n",
            "epoch 54\n",
            " batch Loss train: 0.026900606229901314\n",
            "i 24\n",
            "epoch 54\n",
            " batch Loss train: 0.044007234275341034\n",
            "i 25\n",
            "epoch 54\n",
            " batch Loss train: 0.04147748649120331\n",
            "i 26\n",
            "epoch 54\n",
            " batch Loss train: 0.052515171468257904\n",
            "i 27\n",
            "epoch 54\n",
            " batch Loss train: 0.02879435196518898\n",
            "i 28\n",
            "epoch 54\n",
            " batch Loss train: 0.0783476009964943\n",
            "i 29\n",
            "epoch 54\n",
            " batch Loss train: 0.047312185168266296\n",
            "i 30\n",
            "epoch 54\n",
            " batch Loss train: 0.055174022912979126\n",
            "i 31\n",
            "epoch 54\n",
            " batch Loss train: 0.037787776440382004\n",
            "i 32\n",
            "epoch 54\n",
            " batch Loss train: 0.05574778467416763\n",
            "i 33\n",
            "epoch 54\n",
            " batch Loss train: 0.034224409610033035\n",
            "i 34\n",
            "epoch 54\n",
            " batch Loss train: 0.037406668066978455\n",
            "i 35\n",
            "epoch 54\n",
            " batch Loss train: 0.04115493968129158\n",
            "i 36\n",
            "epoch 54\n",
            " batch Loss train: 0.0440581850707531\n",
            "i 37\n",
            "epoch 54\n",
            " batch Loss train: 0.06419846415519714\n",
            "i 38\n",
            "epoch 54\n",
            " batch Loss train: 0.05449924245476723\n",
            "i 39\n",
            "epoch 54\n",
            " batch Loss train: 0.04131113365292549\n",
            "i 40\n",
            "epoch 54\n",
            " batch Loss train: 0.07983146607875824\n",
            "i 41\n",
            "epoch 54\n",
            " batch Loss train: 0.03601807355880737\n",
            "i 42\n",
            "epoch 54\n",
            " batch Loss train: 0.058068227022886276\n",
            "i 43\n",
            "epoch 54\n",
            " batch Loss train: 0.03550872579216957\n",
            "i 44\n",
            "epoch 54\n",
            " batch Loss train: 0.05862707644701004\n",
            "i 45\n",
            "epoch 54\n",
            " batch Loss train: 0.04618607833981514\n",
            "i 46\n",
            "epoch 54\n",
            " batch Loss train: 0.03862390294671059\n",
            "i 47\n",
            "epoch 54\n",
            " batch Loss train: 0.045986149460077286\n",
            "i 48\n",
            "epoch 54\n",
            " batch Loss train: 0.04910393804311752\n",
            "i 49\n",
            "epoch 54\n",
            " batch Loss train: 0.03505164384841919\n",
            "i 50\n",
            "epoch 54\n",
            " batch Loss train: 0.055520329624414444\n",
            "i 51\n",
            "epoch 54\n",
            " batch Loss train: 0.03148670867085457\n",
            "i 52\n",
            "epoch 54\n",
            " batch Loss train: 0.051328592002391815\n",
            "i 53\n",
            "epoch 54\n",
            " batch Loss train: 0.054109398275613785\n",
            "i 54\n",
            "epoch 54\n",
            " batch Loss train: 0.048709575086832047\n",
            "i 55\n",
            "epoch 54\n",
            " batch Loss train: 0.033180709928274155\n",
            "i 56\n",
            "epoch 54\n",
            " batch Loss train: 0.04164774343371391\n",
            "i 57\n",
            "epoch 54\n",
            " batch Loss train: 0.041708946228027344\n",
            "i 58\n",
            "epoch 54\n",
            " batch Loss train: 0.04166177287697792\n",
            "i 59\n",
            "epoch 54\n",
            " batch Loss train: 0.04295489937067032\n",
            "i 60\n",
            "epoch 54\n",
            " batch Loss train: 0.042146261781454086\n",
            "i 61\n",
            "epoch 54\n",
            " batch Loss train: 0.04092453792691231\n",
            "i 62\n",
            "epoch 54\n",
            " batch Loss train: 0.031109623610973358\n",
            "i 63\n",
            "epoch 54\n",
            " batch Loss train: 0.05474644899368286\n",
            "i 64\n",
            "epoch 54\n",
            " batch Loss train: 0.03500615805387497\n",
            "i 65\n",
            "epoch 54\n",
            " batch Loss train: 0.0464351549744606\n",
            "i 66\n",
            "epoch 54\n",
            " batch Loss train: 0.03805571049451828\n",
            "i 67\n",
            "epoch 54\n",
            " batch Loss train: 0.04720860719680786\n",
            "i 68\n",
            "epoch 54\n",
            " batch Loss train: 0.03961503878235817\n",
            "i 69\n",
            "epoch 54\n",
            " batch Loss train: 0.049170125275850296\n",
            "i 70\n",
            "epoch 54\n",
            " batch Loss train: 0.054258693009614944\n",
            "i 71\n",
            "epoch 54\n",
            " batch Loss train: 0.05788612365722656\n",
            "i 72\n",
            "epoch 54\n",
            " batch Loss train: 0.05491269379854202\n",
            "i 73\n",
            "epoch 54\n",
            " batch Loss train: 0.052802324295043945\n",
            "i 74\n",
            "epoch 54\n",
            " batch Loss train: 0.058976106345653534\n",
            "i 75\n",
            "epoch 54\n",
            " batch Loss train: 0.05821320414543152\n",
            "i 76\n",
            "epoch 54\n",
            " batch Loss train: 0.04400812089443207\n",
            "i 77\n",
            "epoch 54\n",
            " batch Loss train: 0.0620756521821022\n",
            "i 78\n",
            "epoch 54\n",
            " batch Loss train: 0.0499504990875721\n",
            "i 79\n",
            "epoch 54\n",
            " batch Loss train: 0.035623978823423386\n",
            "i 80\n",
            "epoch 54\n",
            " batch Loss train: 0.05561209097504616\n",
            "i 81\n",
            "epoch 54\n",
            " batch Loss train: 0.04560615122318268\n",
            "i 82\n",
            "epoch 54\n",
            " batch Loss train: 0.03775009885430336\n",
            "i 83\n",
            "epoch 54\n",
            " batch Loss train: 0.08042135089635849\n",
            "i 84\n",
            "epoch 54\n",
            " batch Loss train: 0.04285844415426254\n",
            "i 85\n",
            "epoch 54\n",
            " batch Loss train: 0.035660337656736374\n",
            "i 86\n",
            "epoch 54\n",
            " batch Loss train: 0.06071144714951515\n",
            "i 87\n",
            "epoch 54\n",
            " batch Loss train: 0.03892546892166138\n",
            "i 88\n",
            "epoch 54\n",
            " batch Loss train: 0.03913889825344086\n",
            "i 89\n",
            "epoch 54\n",
            " batch Loss train: 0.04732635244727135\n",
            "i 90\n",
            "epoch 54\n",
            " batch Loss train: 0.062155138701200485\n",
            "i 91\n",
            "epoch 54\n",
            " batch Loss train: 0.04651184007525444\n",
            "i 92\n",
            "epoch 54\n",
            " batch Loss train: 0.056720562279224396\n",
            "i 93\n",
            "epoch 54\n",
            " batch Loss train: 0.045306649059057236\n",
            "i 94\n",
            "epoch 54\n",
            " batch Loss train: 0.04439595341682434\n",
            "i 95\n",
            "epoch 54\n",
            " batch Loss train: 0.048358142375946045\n",
            "i 96\n",
            "epoch 54\n",
            " batch Loss train: 0.04663834348320961\n",
            "i 97\n",
            "epoch 54\n",
            " batch Loss train: 0.03331771492958069\n",
            "i 98\n",
            "epoch 54\n",
            " batch Loss train: 0.044141191989183426\n",
            "i 99\n",
            "epoch 54\n",
            " batch Loss train: 0.041948333382606506\n",
            "i 100\n",
            "epoch 54\n",
            " batch Loss train: 0.04986286163330078\n",
            "i 101\n",
            "epoch 54\n",
            " batch Loss train: 0.03883028030395508\n",
            "i 102\n",
            "epoch 54\n",
            " batch Loss train: 0.037955500185489655\n",
            "i 103\n",
            "epoch 54\n",
            " batch Loss train: 0.04111862927675247\n",
            "i 104\n",
            "epoch 54\n",
            " batch Loss train: 0.04566563665866852\n",
            "i 105\n",
            "epoch 54\n",
            " batch Loss train: 0.057357266545295715\n",
            "i 106\n",
            "epoch 54\n",
            " batch Loss train: 0.05343965068459511\n",
            "i 107\n",
            "epoch 54\n",
            " batch Loss train: 0.06035657227039337\n",
            "i 108\n",
            "epoch 54\n",
            " batch Loss train: 0.0659598782658577\n",
            "i 109\n",
            "epoch 54\n",
            " batch Loss train: 0.05308195948600769\n",
            "i 110\n",
            "epoch 54\n",
            " batch Loss train: 0.045931342989206314\n",
            "i 111\n",
            "epoch 54\n",
            " batch Loss train: 0.05578012391924858\n",
            "i 112\n",
            "epoch 54\n",
            " batch Loss train: 0.05739564448595047\n",
            "i 113\n",
            "epoch 54\n",
            " batch Loss train: 0.04517621546983719\n",
            "i 114\n",
            "epoch 54\n",
            " batch Loss train: 0.05404031649231911\n",
            "i 115\n",
            "epoch 54\n",
            " batch Loss train: 0.043673258274793625\n",
            "i 116\n",
            "epoch 54\n",
            " batch Loss train: 0.04882879927754402\n",
            "i 117\n",
            "epoch 54\n",
            " batch Loss train: 0.058402884751558304\n",
            "i 118\n",
            "epoch 54\n",
            " batch Loss train: 0.05767904967069626\n",
            "i 119\n",
            "epoch 54\n",
            " batch Loss train: 0.04916318506002426\n",
            "i 120\n",
            "epoch 54\n",
            " batch Loss train: 0.05130892992019653\n",
            "i 121\n",
            "epoch 54\n",
            " batch Loss train: 0.048673417419195175\n",
            "i 122\n",
            "epoch 54\n",
            " batch Loss train: 0.04713078960776329\n",
            "i 123\n",
            "epoch 54\n",
            " batch Loss train: 0.03892889991402626\n",
            "i 124\n",
            "epoch 54\n",
            " batch Loss train: 0.042699117213487625\n",
            "i 125\n",
            "epoch 54\n",
            " batch Loss train: 0.06761990487575531\n",
            "i 126\n",
            "epoch 54\n",
            " batch Loss train: 0.05166129395365715\n",
            "i 127\n",
            "epoch 54\n",
            " batch Loss train: 0.053339023143053055\n",
            "i 128\n",
            "epoch 54\n",
            " batch Loss train: 0.05846988782286644\n",
            "i 129\n",
            "epoch 54\n",
            " batch Loss train: 0.043867528438568115\n",
            "i 130\n",
            "epoch 54\n",
            " batch Loss train: 0.05356619134545326\n",
            "i 131\n",
            "epoch 54\n",
            " batch Loss train: 0.03790634498000145\n",
            "i 132\n",
            "epoch 54\n",
            " batch Loss train: 0.05625853314995766\n",
            "i 133\n",
            "epoch 54\n",
            " batch Loss train: 0.05323043093085289\n",
            "i 134\n",
            "epoch 54\n",
            " batch Loss train: 0.05237353593111038\n",
            "i 135\n",
            "epoch 54\n",
            " batch Loss train: 0.04836463928222656\n",
            "i 136\n",
            "epoch 54\n",
            " batch Loss train: 0.04809708148241043\n",
            "i 137\n",
            "epoch 54\n",
            " batch Loss train: 0.057975441217422485\n",
            "i 138\n",
            "epoch 54\n",
            " batch Loss train: 0.05928296595811844\n",
            "i 139\n",
            "epoch 54\n",
            " batch Loss train: 0.04818660393357277\n",
            "i 140\n",
            "epoch 54\n",
            " batch Loss train: 0.046929508447647095\n",
            "i 141\n",
            "epoch 54\n",
            " batch Loss train: 0.05542802810668945\n",
            "i 142\n",
            "epoch 54\n",
            " batch Loss train: 0.056491706520318985\n",
            "i 143\n",
            "epoch 54\n",
            " batch Loss train: 0.05402439460158348\n",
            "i 144\n",
            "epoch 54\n",
            " batch Loss train: 0.038924675434827805\n",
            "i 145\n",
            "epoch 54\n",
            " batch Loss train: 0.0760895311832428\n",
            "i 146\n",
            "epoch 54\n",
            " batch Loss train: 0.05442337691783905\n",
            "i 147\n",
            "epoch 54\n",
            " batch Loss train: 0.04377605393528938\n",
            "i 148\n",
            "epoch 54\n",
            " batch Loss train: 0.03713877499103546\n",
            "i 149\n",
            "epoch 54\n",
            " batch Loss train: 0.043335042893886566\n",
            "i 150\n",
            "epoch 54\n",
            " batch Loss train: 0.050860196352005005\n",
            "i 151\n",
            "epoch 54\n",
            " batch Loss train: 0.04419785365462303\n",
            "i 152\n",
            "epoch 54\n",
            " batch Loss train: 0.07809560000896454\n",
            "i 153\n",
            "epoch 54\n",
            " batch Loss train: 0.04332911968231201\n",
            "i 154\n",
            "epoch 54\n",
            " batch Loss train: 0.04843362793326378\n",
            "i 155\n",
            "epoch 54\n",
            " batch Loss train: 0.04014153778553009\n",
            "i 156\n",
            "epoch 54\n",
            " batch Loss train: 0.04073581099510193\n",
            "i 157\n",
            "epoch 54\n",
            " batch Loss train: 0.04317111149430275\n",
            "i 158\n",
            "epoch 54\n",
            " batch Loss train: 0.046072494238615036\n",
            "i 159\n",
            "epoch 54\n",
            " batch Loss train: 0.05579424649477005\n",
            "i 160\n",
            "epoch 54\n",
            " batch Loss train: 0.048901911824941635\n",
            "i 161\n",
            "epoch 54\n",
            " batch Loss train: 0.035751476883888245\n",
            "i 162\n",
            "epoch 54\n",
            " batch Loss train: 0.06228357180953026\n",
            "i 163\n",
            "epoch 54\n",
            " batch Loss train: 0.03488742187619209\n",
            "i 164\n",
            "epoch 54\n",
            " batch Loss train: 0.051860660314559937\n",
            "i 165\n",
            "epoch 54\n",
            " batch Loss train: 0.04117535427212715\n",
            "i 166\n",
            "epoch 54\n",
            " batch Loss train: 0.04923493415117264\n",
            "i 167\n",
            "epoch 54\n",
            " batch Loss train: 0.046820975840091705\n",
            "i 168\n",
            "epoch 54\n",
            " batch Loss train: 0.03875157609581947\n",
            "i 169\n",
            "epoch 54\n",
            " batch Loss train: 0.05477532744407654\n",
            "i 170\n",
            "epoch 54\n",
            " batch Loss train: 0.0589355006814003\n",
            "i 171\n",
            "epoch 54\n",
            " batch Loss train: 0.04020432382822037\n",
            "i 172\n",
            "epoch 54\n",
            " batch Loss train: 0.06545500457286835\n",
            "i 173\n",
            "epoch 54\n",
            " batch Loss train: 0.04079236090183258\n",
            "i 174\n",
            "epoch 54\n",
            " batch Loss train: 0.04249707609415054\n",
            "i 175\n",
            "epoch 54\n",
            " batch Loss train: 0.05153471231460571\n",
            "i 176\n",
            "epoch 54\n",
            " batch Loss train: 0.046707164496183395\n",
            "i 177\n",
            "epoch 54\n",
            " batch Loss train: 0.038262609392404556\n",
            "i 178\n",
            "epoch 54\n",
            " batch Loss train: 0.04578619822859764\n",
            "i 179\n",
            "epoch 54\n",
            " batch Loss train: 0.0532589815557003\n",
            "i 180\n",
            "epoch 54\n",
            " batch Loss train: 0.03391658142209053\n",
            "i 181\n",
            "epoch 54\n",
            " batch Loss train: 0.059101976454257965\n",
            "i 182\n",
            "epoch 54\n",
            " batch Loss train: 0.05665190890431404\n",
            "i 183\n",
            "epoch 54\n",
            " batch Loss train: 0.06335604935884476\n",
            "i 184\n",
            "epoch 54\n",
            " batch Loss train: 0.031124623492360115\n",
            "i 185\n",
            "epoch 54\n",
            " batch Loss train: 0.04092957824468613\n",
            "i 186\n",
            "epoch 54\n",
            " batch Loss train: 0.04770652949810028\n",
            "i 187\n",
            "epoch 54\n",
            " batch Loss train: 0.06419575959444046\n",
            "i 188\n",
            "epoch 54\n",
            " batch Loss train: 0.05229276046156883\n",
            "i 189\n",
            "epoch 54\n",
            " batch Loss train: 0.05171530693769455\n",
            "i 190\n",
            "epoch 54\n",
            " batch Loss train: 0.03933637961745262\n",
            "i 191\n",
            "epoch 54\n",
            " batch Loss train: 0.031711824238300323\n",
            "i 192\n",
            "epoch 54\n",
            " batch Loss train: 0.043099645525217056\n",
            "i 193\n",
            "epoch 54\n",
            " batch Loss train: 0.04926817864179611\n",
            "i 194\n",
            "epoch 54\n",
            " batch Loss train: 0.04422878846526146\n",
            "i 195\n",
            "epoch 54\n",
            " batch Loss train: 0.06276898086071014\n",
            "i 196\n",
            "epoch 54\n",
            " batch Loss train: 0.045544881373643875\n",
            "i 197\n",
            "epoch 54\n",
            " batch Loss train: 0.050202421844005585\n",
            "i 198\n",
            "epoch 54\n",
            " batch Loss train: 0.04168815538287163\n",
            "i 199\n",
            "epoch 54\n",
            " batch Loss train: 0.05955042690038681\n",
            "i 200\n",
            "epoch 54\n",
            " batch Loss train: 0.03992930427193642\n",
            "i 201\n",
            "epoch 54\n",
            " batch Loss train: 0.05099787935614586\n",
            "i 202\n",
            "epoch 54\n",
            " batch Loss train: 0.042748309671878815\n",
            "i 203\n",
            "epoch 54\n",
            " batch Loss train: 0.03842451795935631\n",
            "i 204\n",
            "epoch 54\n",
            " batch Loss train: 0.07377056777477264\n",
            "i 205\n",
            "epoch 54\n",
            " batch Loss train: 0.05962255224585533\n",
            "i 206\n",
            "epoch 54\n",
            " batch Loss train: 0.046885304152965546\n",
            "i 207\n",
            "epoch 54\n",
            " batch Loss train: 0.03876588121056557\n",
            "i 208\n",
            "epoch 54\n",
            " batch Loss train: 0.07675338536500931\n",
            "i 209\n",
            "epoch 54\n",
            " batch Loss train: 0.03868556767702103\n",
            "i 210\n",
            "epoch 54\n",
            " batch Loss train: 0.058285173028707504\n",
            "i 211\n",
            "epoch 54\n",
            " batch Loss train: 0.051865749061107635\n",
            "i 212\n",
            "epoch 54\n",
            " batch Loss train: 0.063832588493824\n",
            "i 213\n",
            "epoch 54\n",
            " batch Loss train: 0.041108619421720505\n",
            "i 214\n",
            "epoch 54\n",
            " batch Loss train: 0.06125754863023758\n",
            "i 215\n",
            "epoch 54\n",
            " batch Loss train: 0.048658426851034164\n",
            "i 216\n",
            "epoch 54\n",
            " batch Loss train: 0.05116882920265198\n",
            "i 217\n",
            "epoch 54\n",
            " batch Loss train: 0.05834142863750458\n",
            "i 218\n",
            "epoch 54\n",
            " batch Loss train: 0.048130206763744354\n",
            "i 219\n",
            "epoch 54\n",
            " batch Loss train: 0.04979322478175163\n",
            "i 220\n",
            "epoch 54\n",
            " batch Loss train: 0.05546756833791733\n",
            "i 221\n",
            "epoch 54\n",
            " batch Loss train: 0.06995496153831482\n",
            "i 222\n",
            "epoch 54\n",
            " batch Loss train: 0.05929660052061081\n",
            "i 223\n",
            "epoch 54\n",
            " batch Loss train: 0.03637322410941124\n",
            "i 224\n",
            "epoch 54\n",
            " batch Loss train: 0.04074553772807121\n",
            "i 225\n",
            "epoch 54\n",
            " batch Loss train: 0.05500306561589241\n",
            "i 226\n",
            "epoch 54\n",
            " batch Loss train: 0.058427780866622925\n",
            "i 227\n",
            "epoch 54\n",
            " batch Loss train: 0.04598386213183403\n",
            "i 228\n",
            "epoch 54\n",
            " batch Loss train: 0.04149366915225983\n",
            "i 229\n",
            "epoch 54\n",
            " batch Loss train: 0.050259482115507126\n",
            "i 230\n",
            "epoch 54\n",
            " batch Loss train: 0.06409205496311188\n",
            "i 231\n",
            "epoch 54\n",
            " batch Loss train: 0.06812642514705658\n",
            "i 232\n",
            "epoch 54\n",
            " batch Loss train: 0.042768754065036774\n",
            "i 233\n",
            "epoch 54\n",
            " batch Loss train: 0.03920041769742966\n",
            "i 234\n",
            "epoch 54\n",
            " batch Loss train: 0.050995487719774246\n",
            "i 235\n",
            "epoch 54\n",
            " batch Loss train: 0.054626718163490295\n",
            "i 236\n",
            "epoch 54\n",
            " batch Loss train: 0.04724462702870369\n",
            "i 237\n",
            "epoch 54\n",
            " batch Loss train: 0.04024870693683624\n",
            "i 238\n",
            "epoch 54\n",
            " batch Loss train: 0.046109188348054886\n",
            "i 239\n",
            "epoch 54\n",
            " batch Loss train: 0.05433020740747452\n",
            "i 240\n",
            "epoch 54\n",
            " batch Loss train: 0.046448856592178345\n",
            "i 241\n",
            "epoch 54\n",
            " batch Loss train: 0.05039723217487335\n",
            "i 242\n",
            "epoch 54\n",
            " batch Loss train: 0.06367126852273941\n",
            "i 243\n",
            "epoch 54\n",
            " batch Loss train: 0.04552953690290451\n",
            "i 244\n",
            "epoch 54\n",
            " batch Loss train: 0.04171716421842575\n",
            "i 245\n",
            "epoch 54\n",
            " batch Loss train: 0.04679965600371361\n",
            "i 246\n",
            "epoch 54\n",
            " batch Loss train: 0.05332276225090027\n",
            "i 247\n",
            "epoch 54\n",
            " batch Loss train: 0.04892756789922714\n",
            "i 248\n",
            "epoch 54\n",
            " batch Loss train: 0.048789091408252716\n",
            "i 249\n",
            "epoch 54\n",
            " batch Loss train: 0.05773772671818733\n",
            "i 250\n",
            "epoch 54\n",
            " batch Loss train: 0.05326511338353157\n",
            "i 251\n",
            "epoch 54\n",
            " batch Loss train: 0.06755195558071136\n",
            "i 252\n",
            "epoch 54\n",
            " batch Loss train: 0.043555714190006256\n",
            "i 253\n",
            "epoch 54\n",
            " batch Loss train: 0.05397447571158409\n",
            "i 254\n",
            "epoch 54\n",
            " batch Loss train: 0.047788094729185104\n",
            "i 255\n",
            "epoch 54\n",
            " batch Loss train: 0.047358401119709015\n",
            "i 256\n",
            "epoch 54\n",
            " batch Loss train: 0.044130269438028336\n",
            "i 257\n",
            "epoch 54\n",
            " batch Loss train: 0.03901340067386627\n",
            "i 258\n",
            "epoch 54\n",
            " batch Loss train: 0.038103923201560974\n",
            "i 259\n",
            "epoch 54\n",
            " batch Loss train: 0.04609597846865654\n",
            "i 260\n",
            "epoch 54\n",
            " batch Loss train: 0.04503423720598221\n",
            "i 261\n",
            "epoch 54\n",
            " batch Loss train: 0.04611005634069443\n",
            "i 262\n",
            "epoch 54\n",
            " batch Loss train: 0.05557509511709213\n",
            "i 263\n",
            "epoch 54\n",
            " batch Loss train: 0.06546938419342041\n",
            "i 264\n",
            "epoch 54\n",
            " batch Loss train: 0.03688735514879227\n",
            "i 265\n",
            "epoch 54\n",
            " batch Loss train: 0.054159410297870636\n",
            "i 266\n",
            "epoch 54\n",
            " batch Loss train: 0.04871555045247078\n",
            "i 267\n",
            "epoch 54\n",
            " batch Loss train: 0.08307010680437088\n",
            "i 268\n",
            "epoch 54\n",
            " batch Loss train: 0.04800436273217201\n",
            "i 269\n",
            "epoch 54\n",
            " batch Loss train: 0.07724162191152573\n",
            "i 270\n",
            "epoch 54\n",
            " batch Loss train: 0.04833871126174927\n",
            "i 271\n",
            "epoch 54\n",
            " batch Loss train: 0.04655715823173523\n",
            "i 272\n",
            "epoch 54\n",
            " batch Loss train: 0.0442037470638752\n",
            "i 273\n",
            "epoch 54\n",
            " batch Loss train: 0.04200331121683121\n",
            "i 274\n",
            "epoch 54\n",
            " batch Loss train: 0.04938628152012825\n",
            "i 275\n",
            "epoch 54\n",
            " batch Loss train: 0.05279901996254921\n",
            "i 276\n",
            "epoch 54\n",
            " batch Loss train: 0.05642944574356079\n",
            "i 277\n",
            "epoch 54\n",
            " batch Loss train: 0.06736348569393158\n",
            "i 278\n",
            "epoch 54\n",
            " batch Loss train: 0.0684698224067688\n",
            "i 279\n",
            "epoch 54\n",
            " batch Loss train: 0.05543409287929535\n",
            "i 280\n",
            "epoch 54\n",
            " batch Loss train: 0.04519594460725784\n",
            "i 281\n",
            "epoch 54\n",
            " batch Loss train: 0.04005806893110275\n",
            "i 282\n",
            "epoch 54\n",
            " batch Loss train: 0.06260150671005249\n",
            "i 283\n",
            "epoch 54\n",
            " batch Loss train: 0.04029456898570061\n",
            "i 284\n",
            "epoch 54\n",
            " batch Loss train: 0.05909402295947075\n",
            "i 285\n",
            "epoch 54\n",
            " batch Loss train: 0.06012885645031929\n",
            "i 286\n",
            "epoch 54\n",
            " batch Loss train: 0.050744496285915375\n",
            "i 287\n",
            "epoch 54\n",
            " batch Loss train: 0.04197268560528755\n",
            "i 288\n",
            "epoch 54\n",
            " batch Loss train: 0.04636280983686447\n",
            "i 289\n",
            "epoch 54\n",
            " batch Loss train: 0.03963220492005348\n",
            "i 290\n",
            "epoch 54\n",
            " batch Loss train: 0.04955489933490753\n",
            "i 291\n",
            "epoch 54\n",
            " batch Loss train: 0.056601181626319885\n",
            "i 292\n",
            "epoch 54\n",
            " batch Loss train: 0.05045398697257042\n",
            "i 293\n",
            "epoch 54\n",
            " batch Loss train: 0.06999483704566956\n",
            "i 294\n",
            "epoch 54\n",
            " batch Loss train: 0.04775001108646393\n",
            "i 295\n",
            "epoch 54\n",
            " batch Loss train: 0.044219110161066055\n",
            "i 296\n",
            "epoch 54\n",
            " batch Loss train: 0.04630604013800621\n",
            "i 297\n",
            "epoch 54\n",
            " batch Loss train: 0.065326027572155\n",
            "i 298\n",
            "epoch 54\n",
            " batch Loss train: 0.03956262022256851\n",
            "i 299\n",
            "epoch 54\n",
            " batch Loss train: 0.05058096721768379\n",
            "i 300\n",
            "epoch 54\n",
            " batch Loss train: 0.03742600604891777\n",
            "i 301\n",
            "epoch 54\n",
            " batch Loss train: 0.04109624773263931\n",
            "i 302\n",
            "epoch 54\n",
            " batch Loss train: 0.03546728566288948\n",
            "i 303\n",
            "epoch 54\n",
            " batch Loss train: 0.053250882774591446\n",
            "i 304\n",
            "epoch 54\n",
            " batch Loss train: 0.06209444999694824\n",
            "i 305\n",
            "epoch 54\n",
            " batch Loss train: 0.06463629752397537\n",
            "i 306\n",
            "epoch 54\n",
            " batch Loss train: 0.049749232828617096\n",
            "i 307\n",
            "epoch 54\n",
            " batch Loss train: 0.044581446796655655\n",
            "i 308\n",
            "epoch 54\n",
            " batch Loss train: 0.06018892675638199\n",
            "i 309\n",
            "epoch 54\n",
            " batch Loss train: 0.04868971183896065\n",
            "i 310\n",
            "epoch 54\n",
            " batch Loss train: 0.05482056364417076\n",
            "i 311\n",
            "epoch 54\n",
            " batch Loss train: 0.05409550666809082\n",
            "i 312\n",
            "epoch 54\n",
            " batch Loss train: 0.050770197063684464\n",
            "i 313\n",
            "epoch 54\n",
            " batch Loss train: 0.044653452932834625\n",
            "i 314\n",
            "epoch 54\n",
            " batch Loss train: 0.04396853223443031\n",
            "i 315\n",
            "epoch 54\n",
            " batch Loss train: 0.06095696985721588\n",
            "i 316\n",
            "epoch 54\n",
            " batch Loss train: 0.07209546864032745\n",
            "i 317\n",
            "epoch 54\n",
            " batch Loss train: 0.038271576166152954\n",
            "i 318\n",
            "epoch 54\n",
            " batch Loss train: 0.04896740987896919\n",
            "i 319\n",
            "epoch 54\n",
            " batch Loss train: 0.04303039610385895\n",
            "i 320\n",
            "epoch 54\n",
            " batch Loss train: 0.052429284900426865\n",
            "i 321\n",
            "epoch 54\n",
            " batch Loss train: 0.04400409013032913\n",
            "i 322\n",
            "epoch 54\n",
            " batch Loss train: 0.047135528177022934\n",
            "i 323\n",
            "epoch 54\n",
            " batch Loss train: 0.053369536995887756\n",
            "i 324\n",
            "epoch 54\n",
            " batch Loss train: 0.05651601403951645\n",
            "i 325\n",
            "epoch 54\n",
            " batch Loss train: 0.04766583442687988\n",
            "i 326\n",
            "epoch 54\n",
            " batch Loss train: 0.048814717680215836\n",
            "i 327\n",
            "epoch 54\n",
            " batch Loss train: 0.04308028519153595\n",
            "i 328\n",
            "epoch 54\n",
            " batch Loss train: 0.06351380795240402\n",
            "i 329\n",
            "epoch 54\n",
            " batch Loss train: 0.05362887308001518\n",
            "i 330\n",
            "epoch 54\n",
            " batch Loss train: 0.0501025952398777\n",
            "i 331\n",
            "epoch 54\n",
            " batch Loss train: 0.0538911335170269\n",
            "i 332\n",
            "epoch 54\n",
            " batch Loss train: 0.050177302211523056\n",
            "i 333\n",
            "epoch 54\n",
            " batch Loss train: 0.04338327422738075\n",
            "i 334\n",
            "epoch 54\n",
            " batch Loss train: 0.040265314280986786\n",
            "i 335\n",
            "epoch 54\n",
            " batch Loss train: 0.04381418973207474\n",
            "i 336\n",
            "epoch 54\n",
            " batch Loss train: 0.05780021846294403\n",
            "i 337\n",
            "epoch 54\n",
            " batch Loss train: 0.058925822377204895\n",
            "i 338\n",
            "epoch 54\n",
            " batch Loss train: 0.0477340891957283\n",
            "i 339\n",
            "epoch 54\n",
            " batch Loss train: 0.040052611380815506\n",
            "i 340\n",
            "epoch 54\n",
            " batch Loss train: 0.06850341707468033\n",
            "i 341\n",
            "epoch 54\n",
            " batch Loss train: 0.05335695669054985\n",
            "i 342\n",
            "epoch 54\n",
            " batch Loss train: 0.05458621308207512\n",
            "i 343\n",
            "epoch 54\n",
            " batch Loss train: 0.05855228751897812\n",
            "i 344\n",
            "epoch 54\n",
            " batch Loss train: 0.07392080128192902\n",
            "i 345\n",
            "epoch 54\n",
            " batch Loss train: 0.0407794751226902\n",
            "i 346\n",
            "epoch 54\n",
            " batch Loss train: 0.044579777866601944\n",
            "i 347\n",
            "epoch 54\n",
            " batch Loss train: 0.04697280377149582\n",
            "i 348\n",
            "epoch 54\n",
            " batch Loss train: 0.05070539191365242\n",
            "i 349\n",
            "epoch 54\n",
            " batch Loss train: 0.05337229743599892\n",
            "i 350\n",
            "epoch 54\n",
            " batch Loss train: 0.05457664653658867\n",
            "i 351\n",
            "epoch 54\n",
            " batch Loss train: 0.047340355813503265\n",
            "i 352\n",
            "epoch 54\n",
            " batch Loss train: 0.04428252577781677\n",
            "i 353\n",
            "epoch 54\n",
            " batch Loss train: 0.04361235350370407\n",
            "i 354\n",
            "epoch 54\n",
            " batch Loss train: 0.04325016587972641\n",
            "i 355\n",
            "epoch 54\n",
            " batch Loss train: 0.06496340036392212\n",
            "i 356\n",
            "epoch 54\n",
            " batch Loss train: 0.06219079717993736\n",
            "i 357\n",
            "epoch 54\n",
            " batch Loss train: 0.04157962650060654\n",
            "i 358\n",
            "epoch 54\n",
            " batch Loss train: 0.06136680021882057\n",
            "i 359\n",
            "epoch 54\n",
            " batch Loss train: 0.04716818034648895\n",
            "i 360\n",
            "epoch 54\n",
            " batch Loss train: 0.05353275686502457\n",
            "i 361\n",
            "epoch 54\n",
            " batch Loss train: 0.05994607135653496\n",
            "i 362\n",
            "epoch 54\n",
            " batch Loss train: 0.055924251675605774\n",
            "i 363\n",
            "epoch 54\n",
            " batch Loss train: 0.06693914532661438\n",
            "i 364\n",
            "epoch 54\n",
            " batch Loss train: 0.046274978667497635\n",
            "i 365\n",
            "epoch 54\n",
            " batch Loss train: 0.045729342848062515\n",
            "i 366\n",
            "epoch 54\n",
            " batch Loss train: 0.05567743629217148\n",
            "i 367\n",
            "epoch 54\n",
            " batch Loss train: 0.055509887635707855\n",
            "i 368\n",
            "epoch 54\n",
            " batch Loss train: 0.045783501118421555\n",
            "i 369\n",
            "epoch 54\n",
            " batch Loss train: 0.04874662682414055\n",
            "i 370\n",
            "epoch 54\n",
            " batch Loss train: 0.06109462305903435\n",
            "i 371\n",
            "epoch 54\n",
            " batch Loss train: 0.04992678388953209\n",
            "i 372\n",
            "epoch 54\n",
            " batch Loss train: 0.06961000710725784\n",
            "i 373\n",
            "epoch 54\n",
            " batch Loss train: 0.042568158358335495\n",
            "i 374\n",
            "epoch 54\n",
            " batch Loss train: 0.0588211789727211\n",
            "i 375\n",
            "epoch 54\n",
            " batch Loss train: 0.06763684749603271\n",
            "i 376\n",
            "epoch 54\n",
            " batch Loss train: 0.06107645854353905\n",
            "i 377\n",
            "epoch 54\n",
            " batch Loss train: 0.06540275365114212\n",
            "i 378\n",
            "epoch 54\n",
            " batch Loss train: 0.06459499150514603\n",
            "i 379\n",
            "epoch 54\n",
            " batch Loss train: 0.05577043071389198\n",
            "i 380\n",
            "epoch 54\n",
            " batch Loss train: 0.059024613350629807\n",
            "i 381\n",
            "epoch 54\n",
            " batch Loss train: 0.05527738109230995\n",
            "i 382\n",
            "epoch 54\n",
            " batch Loss train: 0.05972135812044144\n",
            "i 383\n",
            "epoch 54\n",
            " batch Loss train: 0.05882031470537186\n",
            "i 384\n",
            "epoch 54\n",
            " batch Loss train: 0.052525900304317474\n",
            "i 385\n",
            "epoch 54\n",
            " batch Loss train: 0.0493554025888443\n",
            "i 386\n",
            "epoch 54\n",
            " batch Loss train: 0.06335045397281647\n",
            "i 387\n",
            "epoch 54\n",
            " batch Loss train: 0.03970187529921532\n",
            "i 388\n",
            "epoch 54\n",
            " batch Loss train: 0.03156604617834091\n",
            "i 389\n",
            "epoch 54\n",
            " batch Loss train: 0.04578380659222603\n",
            "i 390\n",
            "epoch 54\n",
            " batch Loss train: 0.05626808851957321\n",
            "i 391\n",
            "epoch 54\n",
            " batch Loss train: 0.0664111077785492\n",
            "i 392\n",
            "epoch 54\n",
            " batch Loss train: 0.06375829875469208\n",
            "i 393\n",
            "epoch 54\n",
            " batch Loss train: 0.05857604369521141\n",
            "i 394\n",
            "epoch 54\n",
            " batch Loss train: 0.059566278010606766\n",
            "i 395\n",
            "epoch 54\n",
            " batch Loss train: 0.06896263360977173\n",
            "i 396\n",
            "epoch 54\n",
            " batch Loss train: 0.05727012827992439\n",
            "i 397\n",
            "epoch 54\n",
            " batch Loss train: 0.059707943350076675\n",
            "i 398\n",
            "epoch 54\n",
            " batch Loss train: 0.04684941843152046\n",
            "i 399\n",
            "epoch 54\n",
            " batch Loss train: 0.03953871503472328\n",
            "i 400\n",
            "epoch 54\n",
            " batch Loss train: 0.05090690404176712\n",
            "i 401\n",
            "epoch 54\n",
            " batch Loss train: 0.07296207547187805\n",
            "i 402\n",
            "epoch 54\n",
            " batch Loss train: 0.04476730525493622\n",
            "i 403\n",
            "epoch 54\n",
            " batch Loss train: 0.04621468856930733\n",
            "i 404\n",
            "epoch 54\n",
            " batch Loss train: 0.058223117142915726\n",
            "i 405\n",
            "epoch 54\n",
            " batch Loss train: 0.040795911103487015\n",
            "i 406\n",
            "epoch 54\n",
            " batch Loss train: 0.0496273972094059\n",
            "i 407\n",
            "epoch 54\n",
            " batch Loss train: 0.03655385971069336\n",
            "i 408\n",
            "epoch 54\n",
            " batch Loss train: 0.03903043270111084\n",
            "i 409\n",
            "epoch 54\n",
            " batch Loss train: 0.05590541288256645\n",
            "i 410\n",
            "epoch 54\n",
            " batch Loss train: 0.039465371519327164\n",
            "i 411\n",
            "epoch 54\n",
            " batch Loss train: 0.04974692314863205\n",
            "i 412\n",
            "epoch 54\n",
            " batch Loss train: 0.062530018389225\n",
            "i 413\n",
            "epoch 54\n",
            " batch Loss train: 0.05650988966226578\n",
            "i 414\n",
            "epoch 54\n",
            " batch Loss train: 0.06748725473880768\n",
            "i 415\n",
            "epoch 54\n",
            " batch Loss train: 0.048822324723005295\n",
            "i 416\n",
            "epoch 54\n",
            " batch Loss train: 0.047827910631895065\n",
            "i 417\n",
            "epoch 54\n",
            " batch Loss train: 0.058987993746995926\n",
            "i 418\n",
            "epoch 54\n",
            " batch Loss train: 0.058224037289619446\n",
            "i 419\n",
            "epoch 54\n",
            " batch Loss train: 0.06528285145759583\n",
            "i 420\n",
            "epoch 54\n",
            " batch Loss train: 0.05010760948061943\n",
            "i 421\n",
            "epoch 54\n",
            " batch Loss train: 0.054405875504016876\n",
            "i 422\n",
            "epoch 54\n",
            " batch Loss train: 0.04342927038669586\n",
            "i 423\n",
            "epoch 54\n",
            " batch Loss train: 0.0549350306391716\n",
            "i 424\n",
            "epoch 54\n",
            " batch Loss train: 0.046267226338386536\n",
            "i 425\n",
            "epoch 54\n",
            " batch Loss train: 0.07507596909999847\n",
            "i 426\n",
            "epoch 54\n",
            " batch Loss train: 0.04740171134471893\n",
            "i 427\n",
            "epoch 54\n",
            " batch Loss train: 0.05295331031084061\n",
            "i 428\n",
            "epoch 54\n",
            " batch Loss train: 0.06506307423114777\n",
            "i 429\n",
            "epoch 54\n",
            " batch Loss train: 0.05261387676000595\n",
            "i 430\n",
            "epoch 54\n",
            " batch Loss train: 0.04481130838394165\n",
            "i 431\n",
            "epoch 54\n",
            " batch Loss train: 0.056664444506168365\n",
            "i 432\n",
            "epoch 54\n",
            " batch Loss train: 0.06544008105993271\n",
            "i 433\n",
            "epoch 54\n",
            " batch Loss train: 0.07510405033826828\n",
            "i 434\n",
            "epoch 54\n",
            " batch Loss train: 0.07498294860124588\n",
            "i 435\n",
            "epoch 54\n",
            " batch Loss train: 0.07955079525709152\n",
            "i 436\n",
            "epoch 54\n",
            " batch Loss train: 0.04864001274108887\n",
            "i 437\n",
            "epoch 54\n",
            " batch Loss train: 0.058612409979104996\n",
            "i 438\n",
            "epoch 54\n",
            " batch Loss train: 0.044355358928442\n",
            "i 439\n",
            "epoch 54\n",
            " batch Loss train: 0.060297951102256775\n",
            "i 440\n",
            "epoch 54\n",
            " batch Loss train: 0.054615091532468796\n",
            "i 441\n",
            "epoch 54\n",
            " batch Loss train: 0.05824153497815132\n",
            "i 442\n",
            "epoch 54\n",
            " batch Loss train: 0.056783512234687805\n",
            "i 443\n",
            "epoch 54\n",
            " batch Loss train: 0.05020972341299057\n",
            "i 444\n",
            "epoch 54\n",
            " batch Loss train: 0.06923135370016098\n",
            "i 445\n",
            "epoch 54\n",
            " batch Loss train: 0.05328619107604027\n",
            "total epoch Loss train: tensor(0.0533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "i 0\n",
            "epoch 55\n",
            " batch Loss train: 0.05204865708947182\n",
            "i 1\n",
            "epoch 55\n",
            " batch Loss train: 0.04364290088415146\n",
            "i 2\n",
            "epoch 55\n",
            " batch Loss train: 0.04599810019135475\n",
            "i 3\n",
            "epoch 55\n",
            " batch Loss train: 0.04404215142130852\n",
            "i 4\n",
            "epoch 55\n",
            " batch Loss train: 0.06664518266916275\n",
            "i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 55\n",
            " batch Loss train: 0.03658415377140045\n",
            "i 6\n",
            "epoch 55\n",
            " batch Loss train: 0.04067588970065117\n",
            "i 7\n",
            "epoch 55\n",
            " batch Loss train: 0.039723947644233704\n",
            "i 8\n",
            "epoch 55\n",
            " batch Loss train: 0.04184180498123169\n",
            "i 9\n",
            "epoch 55\n",
            " batch Loss train: 0.043473802506923676\n",
            "i 10\n",
            "epoch 55\n",
            " batch Loss train: 0.04747215285897255\n",
            "i 11\n",
            "epoch 55\n",
            " batch Loss train: 0.03212731331586838\n",
            "i 12\n",
            "epoch 55\n",
            " batch Loss train: 0.05357293784618378\n",
            "i 13\n",
            "epoch 55\n",
            " batch Loss train: 0.03842999413609505\n",
            "i 14\n",
            "epoch 55\n",
            " batch Loss train: 0.04482002928853035\n",
            "i 15\n",
            "epoch 55\n",
            " batch Loss train: 0.04078539460897446\n",
            "i 16\n",
            "epoch 55\n",
            " batch Loss train: 0.03384366258978844\n",
            "i 17\n",
            "epoch 55\n",
            " batch Loss train: 0.05312857776880264\n",
            "i 18\n",
            "epoch 55\n",
            " batch Loss train: 0.048506688326597214\n",
            "i 19\n",
            "epoch 55\n",
            " batch Loss train: 0.05236359313130379\n",
            "i 20\n",
            "epoch 55\n",
            " batch Loss train: 0.037630509585142136\n",
            "i 21\n",
            "epoch 55\n",
            " batch Loss train: 0.04805572330951691\n",
            "i 22\n",
            "epoch 55\n",
            " batch Loss train: 0.05238929018378258\n",
            "i 23\n",
            "epoch 55\n",
            " batch Loss train: 0.035531528294086456\n",
            "i 24\n",
            "epoch 55\n",
            " batch Loss train: 0.04781442508101463\n",
            "i 25\n",
            "epoch 55\n",
            " batch Loss train: 0.039243970066308975\n",
            "i 26\n",
            "epoch 55\n",
            " batch Loss train: 0.02942468784749508\n",
            "i 27\n",
            "epoch 55\n",
            " batch Loss train: 0.05096834525465965\n",
            "i 28\n",
            "epoch 55\n",
            " batch Loss train: 0.04875337705016136\n",
            "i 29\n",
            "epoch 55\n",
            " batch Loss train: 0.05598720163106918\n",
            "i 30\n",
            "epoch 55\n",
            " batch Loss train: 0.03150234743952751\n",
            "i 31\n",
            "epoch 55\n",
            " batch Loss train: 0.04173241928219795\n",
            "i 32\n",
            "epoch 55\n",
            " batch Loss train: 0.03256389498710632\n",
            "i 33\n",
            "epoch 55\n",
            " batch Loss train: 0.0350944884121418\n",
            "i 34\n",
            "epoch 55\n",
            " batch Loss train: 0.03789323568344116\n",
            "i 35\n",
            "epoch 55\n",
            " batch Loss train: 0.05592479556798935\n",
            "i 36\n",
            "epoch 55\n",
            " batch Loss train: 0.048090510070323944\n",
            "i 37\n",
            "epoch 55\n",
            " batch Loss train: 0.04278610646724701\n",
            "i 38\n",
            "epoch 55\n",
            " batch Loss train: 0.04965120553970337\n",
            "i 39\n",
            "epoch 55\n",
            " batch Loss train: 0.046907879412174225\n",
            "i 40\n",
            "epoch 55\n",
            " batch Loss train: 0.03445178642868996\n",
            "i 41\n",
            "epoch 55\n",
            " batch Loss train: 0.05085911974310875\n",
            "i 42\n",
            "epoch 55\n",
            " batch Loss train: 0.05796660855412483\n",
            "i 43\n",
            "epoch 55\n",
            " batch Loss train: 0.04327446222305298\n",
            "i 44\n",
            "epoch 55\n",
            " batch Loss train: 0.03956444561481476\n",
            "i 45\n",
            "epoch 55\n",
            " batch Loss train: 0.04779338836669922\n",
            "i 46\n",
            "epoch 55\n",
            " batch Loss train: 0.0471373051404953\n",
            "i 47\n",
            "epoch 55\n",
            " batch Loss train: 0.026989076286554337\n",
            "i 48\n",
            "epoch 55\n",
            " batch Loss train: 0.04349179565906525\n",
            "i 49\n",
            "epoch 55\n",
            " batch Loss train: 0.03671907261013985\n",
            "i 50\n",
            "epoch 55\n",
            " batch Loss train: 0.04433989152312279\n",
            "i 51\n",
            "epoch 55\n",
            " batch Loss train: 0.05042503401637077\n",
            "i 52\n",
            "epoch 55\n",
            " batch Loss train: 0.03453860804438591\n",
            "i 53\n",
            "epoch 55\n",
            " batch Loss train: 0.03676113858819008\n",
            "i 54\n",
            "epoch 55\n",
            " batch Loss train: 0.04405121132731438\n",
            "i 55\n",
            "epoch 55\n",
            " batch Loss train: 0.03563101589679718\n",
            "i 56\n",
            "epoch 55\n",
            " batch Loss train: 0.050625164061784744\n",
            "i 57\n",
            "epoch 55\n",
            " batch Loss train: 0.041814371943473816\n",
            "i 58\n",
            "epoch 55\n",
            " batch Loss train: 0.046481553465127945\n",
            "i 59\n",
            "epoch 55\n",
            " batch Loss train: 0.03678341209888458\n",
            "i 60\n",
            "epoch 55\n",
            " batch Loss train: 0.038126084953546524\n",
            "i 61\n",
            "epoch 55\n",
            " batch Loss train: 0.043102484196424484\n",
            "i 62\n",
            "epoch 55\n",
            " batch Loss train: 0.027314351871609688\n",
            "i 63\n",
            "epoch 55\n",
            " batch Loss train: 0.050138138234615326\n",
            "i 64\n",
            "epoch 55\n",
            " batch Loss train: 0.028210218995809555\n",
            "i 65\n",
            "epoch 55\n",
            " batch Loss train: 0.04475920647382736\n",
            "i 66\n",
            "epoch 55\n",
            " batch Loss train: 0.03533971682190895\n",
            "i 67\n",
            "epoch 55\n",
            " batch Loss train: 0.04147651419043541\n",
            "i 68\n",
            "epoch 55\n",
            " batch Loss train: 0.042843155562877655\n",
            "i 69\n",
            "epoch 55\n",
            " batch Loss train: 0.046828899532556534\n",
            "i 70\n",
            "epoch 55\n",
            " batch Loss train: 0.03259250149130821\n",
            "i 71\n",
            "epoch 55\n",
            " batch Loss train: 0.058851905167102814\n",
            "i 72\n",
            "epoch 55\n",
            " batch Loss train: 0.03241516649723053\n",
            "i 73\n",
            "epoch 55\n",
            " batch Loss train: 0.044772133231163025\n",
            "i 74\n",
            "epoch 55\n",
            " batch Loss train: 0.035624757409095764\n",
            "i 75\n",
            "epoch 55\n",
            " batch Loss train: 0.055337876081466675\n",
            "i 76\n",
            "epoch 55\n",
            " batch Loss train: 0.04295527935028076\n",
            "i 77\n",
            "epoch 55\n",
            " batch Loss train: 0.035305898636579514\n",
            "i 78\n",
            "epoch 55\n",
            " batch Loss train: 0.04189452528953552\n",
            "i 79\n",
            "epoch 55\n",
            " batch Loss train: 0.06399256736040115\n",
            "i 80\n",
            "epoch 55\n",
            " batch Loss train: 0.03341519460082054\n",
            "i 81\n",
            "epoch 55\n",
            " batch Loss train: 0.021739017218351364\n",
            "i 82\n",
            "epoch 55\n",
            " batch Loss train: 0.033819396048784256\n",
            "i 83\n",
            "epoch 55\n",
            " batch Loss train: 0.0504210889339447\n",
            "i 84\n",
            "epoch 55\n",
            " batch Loss train: 0.04289030656218529\n",
            "i 85\n",
            "epoch 55\n",
            " batch Loss train: 0.042169176042079926\n",
            "i 86\n",
            "epoch 55\n",
            " batch Loss train: 0.04196757823228836\n",
            "i 87\n",
            "epoch 55\n",
            " batch Loss train: 0.032379213720560074\n",
            "i 88\n",
            "epoch 55\n",
            " batch Loss train: 0.06464772671461105\n",
            "i 89\n",
            "epoch 55\n",
            " batch Loss train: 0.04519877955317497\n",
            "i 90\n",
            "epoch 55\n",
            " batch Loss train: 0.043190598487854004\n",
            "i 91\n",
            "epoch 55\n",
            " batch Loss train: 0.03239057958126068\n",
            "i 92\n",
            "epoch 55\n",
            " batch Loss train: 0.035403091460466385\n",
            "i 93\n",
            "epoch 55\n",
            " batch Loss train: 0.03587191179394722\n",
            "i 94\n",
            "epoch 55\n",
            " batch Loss train: 0.037396810948848724\n",
            "i 95\n",
            "epoch 55\n",
            " batch Loss train: 0.04354000464081764\n",
            "i 96\n",
            "epoch 55\n",
            " batch Loss train: 0.03773252293467522\n",
            "i 97\n",
            "epoch 55\n",
            " batch Loss train: 0.05329601839184761\n",
            "i 98\n",
            "epoch 55\n",
            " batch Loss train: 0.0348171591758728\n",
            "i 99\n",
            "epoch 55\n",
            " batch Loss train: 0.05568065121769905\n",
            "i 100\n",
            "epoch 55\n",
            " batch Loss train: 0.04816898703575134\n",
            "i 101\n",
            "epoch 55\n",
            " batch Loss train: 0.04417962580919266\n",
            "i 102\n",
            "epoch 55\n",
            " batch Loss train: 0.03839009627699852\n",
            "i 103\n",
            "epoch 55\n",
            " batch Loss train: 0.04321820288896561\n",
            "i 104\n",
            "epoch 55\n",
            " batch Loss train: 0.04470878466963768\n",
            "i 105\n",
            "epoch 55\n",
            " batch Loss train: 0.03193656727671623\n",
            "i 106\n",
            "epoch 55\n",
            " batch Loss train: 0.045488011091947556\n",
            "i 107\n",
            "epoch 55\n",
            " batch Loss train: 0.04702185094356537\n",
            "i 108\n",
            "epoch 55\n",
            " batch Loss train: 0.04539980739355087\n",
            "i 109\n",
            "epoch 55\n",
            " batch Loss train: 0.042166806757450104\n",
            "i 110\n",
            "epoch 55\n",
            " batch Loss train: 0.056679997593164444\n",
            "i 111\n",
            "epoch 55\n",
            " batch Loss train: 0.04698362946510315\n",
            "i 112\n",
            "epoch 55\n",
            " batch Loss train: 0.04603120684623718\n",
            "i 113\n",
            "epoch 55\n",
            " batch Loss train: 0.039578262716531754\n",
            "i 114\n",
            "epoch 55\n",
            " batch Loss train: 0.06072060763835907\n",
            "i 115\n",
            "epoch 55\n",
            " batch Loss train: 0.05288504436612129\n",
            "i 116\n",
            "epoch 55\n",
            " batch Loss train: 0.03538762405514717\n",
            "i 117\n",
            "epoch 55\n",
            " batch Loss train: 0.039363715797662735\n",
            "i 118\n",
            "epoch 55\n",
            " batch Loss train: 0.05202453210949898\n",
            "i 119\n",
            "epoch 55\n",
            " batch Loss train: 0.05483635887503624\n",
            "i 120\n",
            "epoch 55\n",
            " batch Loss train: 0.03861207515001297\n",
            "i 121\n",
            "epoch 55\n",
            " batch Loss train: 0.057101789861917496\n",
            "i 122\n",
            "epoch 55\n",
            " batch Loss train: 0.04257021099328995\n",
            "i 123\n",
            "epoch 55\n",
            " batch Loss train: 0.03758031502366066\n",
            "i 124\n",
            "epoch 55\n",
            " batch Loss train: 0.0571550577878952\n",
            "i 125\n",
            "epoch 55\n",
            " batch Loss train: 0.03543601557612419\n",
            "i 126\n",
            "epoch 55\n",
            " batch Loss train: 0.04590972512960434\n",
            "i 127\n",
            "epoch 55\n",
            " batch Loss train: 0.046928342431783676\n",
            "i 128\n",
            "epoch 55\n",
            " batch Loss train: 0.05236538127064705\n",
            "i 129\n",
            "epoch 55\n",
            " batch Loss train: 0.02484939992427826\n",
            "i 130\n",
            "epoch 55\n",
            " batch Loss train: 0.04259476065635681\n",
            "i 131\n",
            "epoch 55\n",
            " batch Loss train: 0.03953859210014343\n",
            "i 132\n",
            "epoch 55\n",
            " batch Loss train: 0.0435614250600338\n",
            "i 133\n",
            "epoch 55\n",
            " batch Loss train: 0.03032774105668068\n",
            "i 134\n",
            "epoch 55\n",
            " batch Loss train: 0.050008032470941544\n",
            "i 135\n",
            "epoch 55\n",
            " batch Loss train: 0.048289209604263306\n",
            "i 136\n",
            "epoch 55\n",
            " batch Loss train: 0.03955673426389694\n",
            "i 137\n",
            "epoch 55\n",
            " batch Loss train: 0.03726517781615257\n",
            "i 138\n",
            "epoch 55\n",
            " batch Loss train: 0.04079778492450714\n",
            "i 139\n",
            "epoch 55\n",
            " batch Loss train: 0.04339626431465149\n",
            "i 140\n",
            "epoch 55\n",
            " batch Loss train: 0.05803307890892029\n",
            "i 141\n",
            "epoch 55\n",
            " batch Loss train: 0.04307224228978157\n",
            "i 142\n",
            "epoch 55\n",
            " batch Loss train: 0.03715471178293228\n",
            "i 143\n",
            "epoch 55\n",
            " batch Loss train: 0.03611917048692703\n",
            "i 144\n",
            "epoch 55\n",
            " batch Loss train: 0.0448964424431324\n",
            "i 145\n",
            "epoch 55\n",
            " batch Loss train: 0.04416627064347267\n",
            "i 146\n",
            "epoch 55\n",
            " batch Loss train: 0.06482239067554474\n",
            "i 147\n",
            "epoch 55\n",
            " batch Loss train: 0.0388692244887352\n",
            "i 148\n",
            "epoch 55\n",
            " batch Loss train: 0.06039198487997055\n",
            "i 149\n",
            "epoch 55\n",
            " batch Loss train: 0.04589509963989258\n",
            "i 150\n",
            "epoch 55\n",
            " batch Loss train: 0.04170854762196541\n",
            "i 151\n",
            "epoch 55\n",
            " batch Loss train: 0.046720243990421295\n",
            "i 152\n",
            "epoch 55\n",
            " batch Loss train: 0.03476647287607193\n",
            "i 153\n",
            "epoch 55\n",
            " batch Loss train: 0.03764718770980835\n",
            "i 154\n",
            "epoch 55\n",
            " batch Loss train: 0.052949320524930954\n",
            "i 155\n",
            "epoch 55\n",
            " batch Loss train: 0.04426465183496475\n",
            "i 156\n",
            "epoch 55\n",
            " batch Loss train: 0.0500115230679512\n",
            "i 157\n",
            "epoch 55\n",
            " batch Loss train: 0.0419384203851223\n",
            "i 158\n",
            "epoch 55\n",
            " batch Loss train: 0.03895669803023338\n",
            "i 159\n",
            "epoch 55\n",
            " batch Loss train: 0.04571422562003136\n",
            "i 160\n",
            "epoch 55\n",
            " batch Loss train: 0.03871670365333557\n",
            "i 161\n",
            "epoch 55\n",
            " batch Loss train: 0.06214599311351776\n",
            "i 162\n",
            "epoch 55\n",
            " batch Loss train: 0.0384693406522274\n",
            "i 163\n",
            "epoch 55\n",
            " batch Loss train: 0.0462767519056797\n",
            "i 164\n",
            "epoch 55\n",
            " batch Loss train: 0.035396162420511246\n",
            "i 165\n",
            "epoch 55\n",
            " batch Loss train: 0.04027923196554184\n",
            "i 166\n",
            "epoch 55\n",
            " batch Loss train: 0.05058973282575607\n",
            "i 167\n",
            "epoch 55\n",
            " batch Loss train: 0.045560892671346664\n",
            "i 168\n",
            "epoch 55\n",
            " batch Loss train: 0.032591816037893295\n",
            "i 169\n",
            "epoch 55\n",
            " batch Loss train: 0.05637066811323166\n",
            "i 170\n",
            "epoch 55\n",
            " batch Loss train: 0.04673606902360916\n",
            "i 171\n",
            "epoch 55\n",
            " batch Loss train: 0.04344268515706062\n",
            "i 172\n",
            "epoch 55\n",
            " batch Loss train: 0.049550861120224\n",
            "i 173\n",
            "epoch 55\n",
            " batch Loss train: 0.03731996566057205\n",
            "i 174\n",
            "epoch 55\n",
            " batch Loss train: 0.06484795361757278\n",
            "i 175\n",
            "epoch 55\n",
            " batch Loss train: 0.0422348752617836\n",
            "i 176\n",
            "epoch 55\n",
            " batch Loss train: 0.06102943420410156\n",
            "i 177\n",
            "epoch 55\n",
            " batch Loss train: 0.060183487832546234\n",
            "i 178\n",
            "epoch 55\n",
            " batch Loss train: 0.03946685791015625\n",
            "i 179\n",
            "epoch 55\n",
            " batch Loss train: 0.034501947462558746\n",
            "i 180\n",
            "epoch 55\n",
            " batch Loss train: 0.06433911621570587\n",
            "i 181\n",
            "epoch 55\n",
            " batch Loss train: 0.0499098002910614\n",
            "i 182\n",
            "epoch 55\n",
            " batch Loss train: 0.0612526498734951\n",
            "i 183\n",
            "epoch 55\n",
            " batch Loss train: 0.037812892347574234\n",
            "i 184\n",
            "epoch 55\n",
            " batch Loss train: 0.06452284008264542\n",
            "i 185\n",
            "epoch 55\n",
            " batch Loss train: 0.057293497025966644\n",
            "i 186\n",
            "epoch 55\n",
            " batch Loss train: 0.05684230476617813\n",
            "i 187\n",
            "epoch 55\n",
            " batch Loss train: 0.05406532064080238\n",
            "i 188\n",
            "epoch 55\n",
            " batch Loss train: 0.04228077083826065\n",
            "i 189\n",
            "epoch 55\n",
            " batch Loss train: 0.043075721710920334\n",
            "i 190\n",
            "epoch 55\n",
            " batch Loss train: 0.07191973179578781\n",
            "i 191\n",
            "epoch 55\n",
            " batch Loss train: 0.04359175264835358\n",
            "i 192\n",
            "epoch 55\n",
            " batch Loss train: 0.05161730945110321\n",
            "i 193\n",
            "epoch 55\n",
            " batch Loss train: 0.04376789927482605\n",
            "i 194\n",
            "epoch 55\n",
            " batch Loss train: 0.06034364551305771\n",
            "i 195\n",
            "epoch 55\n",
            " batch Loss train: 0.05289941281080246\n",
            "i 196\n",
            "epoch 55\n",
            " batch Loss train: 0.0521756187081337\n",
            "i 197\n",
            "epoch 55\n",
            " batch Loss train: 0.06237080320715904\n",
            "i 198\n",
            "epoch 55\n",
            " batch Loss train: 0.0424966998398304\n",
            "i 199\n",
            "epoch 55\n",
            " batch Loss train: 0.048418719321489334\n",
            "i 200\n",
            "epoch 55\n",
            " batch Loss train: 0.07473690062761307\n",
            "i 201\n",
            "epoch 55\n",
            " batch Loss train: 0.0446782112121582\n",
            "i 202\n",
            "epoch 55\n",
            " batch Loss train: 0.03857908770442009\n",
            "i 203\n",
            "epoch 55\n",
            " batch Loss train: 0.038959842175245285\n",
            "i 204\n",
            "epoch 55\n",
            " batch Loss train: 0.03938636928796768\n",
            "i 205\n",
            "epoch 55\n",
            " batch Loss train: 0.0590238943696022\n",
            "i 206\n",
            "epoch 55\n",
            " batch Loss train: 0.05817823484539986\n",
            "i 207\n",
            "epoch 55\n",
            " batch Loss train: 0.057584263384342194\n",
            "i 208\n",
            "epoch 55\n",
            " batch Loss train: 0.042839210480451584\n",
            "i 209\n",
            "epoch 55\n",
            " batch Loss train: 0.03876799717545509\n",
            "i 210\n",
            "epoch 55\n",
            " batch Loss train: 0.04465017095208168\n",
            "i 211\n",
            "epoch 55\n",
            " batch Loss train: 0.06387653946876526\n",
            "i 212\n",
            "epoch 55\n",
            " batch Loss train: 0.04091737046837807\n",
            "i 213\n",
            "epoch 55\n",
            " batch Loss train: 0.06186053901910782\n",
            "i 214\n",
            "epoch 55\n",
            " batch Loss train: 0.04909287765622139\n",
            "i 215\n",
            "epoch 55\n",
            " batch Loss train: 0.03680851683020592\n",
            "i 216\n",
            "epoch 55\n",
            " batch Loss train: 0.06020704656839371\n",
            "i 217\n",
            "epoch 55\n",
            " batch Loss train: 0.04984339326620102\n",
            "i 218\n",
            "epoch 55\n",
            " batch Loss train: 0.046365056186914444\n",
            "i 219\n",
            "epoch 55\n",
            " batch Loss train: 0.04362422227859497\n",
            "i 220\n",
            "epoch 55\n",
            " batch Loss train: 0.043181560933589935\n",
            "i 221\n",
            "epoch 55\n",
            " batch Loss train: 0.05342807248234749\n",
            "i 222\n",
            "epoch 55\n",
            " batch Loss train: 0.053454551845788956\n",
            "i 223\n",
            "epoch 55\n",
            " batch Loss train: 0.0509868748486042\n",
            "i 224\n",
            "epoch 55\n",
            " batch Loss train: 0.04655933752655983\n",
            "i 225\n",
            "epoch 55\n",
            " batch Loss train: 0.06016311049461365\n",
            "i 226\n",
            "epoch 55\n",
            " batch Loss train: 0.04948549345135689\n",
            "i 227\n",
            "epoch 55\n",
            " batch Loss train: 0.047782331705093384\n",
            "i 228\n",
            "epoch 55\n",
            " batch Loss train: 0.03568780794739723\n",
            "i 229\n",
            "epoch 55\n",
            " batch Loss train: 0.0505654402077198\n",
            "i 230\n",
            "epoch 55\n",
            " batch Loss train: 0.03808657079935074\n",
            "i 231\n",
            "epoch 55\n",
            " batch Loss train: 0.052490901201963425\n",
            "i 232\n",
            "epoch 55\n",
            " batch Loss train: 0.04049276188015938\n",
            "i 233\n",
            "epoch 55\n",
            " batch Loss train: 0.044763993471860886\n",
            "i 234\n",
            "epoch 55\n",
            " batch Loss train: 0.0551278330385685\n",
            "i 235\n",
            "epoch 55\n",
            " batch Loss train: 0.04623346030712128\n",
            "i 236\n",
            "epoch 55\n",
            " batch Loss train: 0.04421845078468323\n",
            "i 237\n",
            "epoch 55\n",
            " batch Loss train: 0.04348592832684517\n",
            "i 238\n",
            "epoch 55\n",
            " batch Loss train: 0.05744177848100662\n",
            "i 239\n",
            "epoch 55\n",
            " batch Loss train: 0.05148949474096298\n",
            "i 240\n",
            "epoch 55\n",
            " batch Loss train: 0.06688330322504044\n",
            "i 241\n",
            "epoch 55\n",
            " batch Loss train: 0.04821701720356941\n",
            "i 242\n",
            "epoch 55\n",
            " batch Loss train: 0.06056772172451019\n",
            "i 243\n",
            "epoch 55\n",
            " batch Loss train: 0.05962134897708893\n",
            "i 244\n",
            "epoch 55\n",
            " batch Loss train: 0.041636284440755844\n",
            "i 245\n",
            "epoch 55\n",
            " batch Loss train: 0.06447594612836838\n",
            "i 246\n",
            "epoch 55\n",
            " batch Loss train: 0.03975844010710716\n",
            "i 247\n",
            "epoch 55\n",
            " batch Loss train: 0.04052060469985008\n",
            "i 248\n",
            "epoch 55\n",
            " batch Loss train: 0.05223119631409645\n",
            "i 249\n",
            "epoch 55\n",
            " batch Loss train: 0.05505318194627762\n",
            "i 250\n",
            "epoch 55\n",
            " batch Loss train: 0.05016525462269783\n",
            "i 251\n",
            "epoch 55\n",
            " batch Loss train: 0.05213551968336105\n",
            "i 252\n",
            "epoch 55\n",
            " batch Loss train: 0.05067842826247215\n",
            "i 253\n",
            "epoch 55\n",
            " batch Loss train: 0.04986095800995827\n",
            "i 254\n",
            "epoch 55\n",
            " batch Loss train: 0.0527365580201149\n",
            "i 255\n",
            "epoch 55\n",
            " batch Loss train: 0.05495406687259674\n",
            "i 256\n",
            "epoch 55\n",
            " batch Loss train: 0.05444300174713135\n",
            "i 257\n",
            "epoch 55\n",
            " batch Loss train: 0.047504909336566925\n",
            "i 258\n",
            "epoch 55\n",
            " batch Loss train: 0.04162655770778656\n",
            "i 259\n",
            "epoch 55\n",
            " batch Loss train: 0.04047917202115059\n",
            "i 260\n",
            "epoch 55\n",
            " batch Loss train: 0.04091030731797218\n",
            "i 261\n",
            "epoch 55\n",
            " batch Loss train: 0.05581646040081978\n",
            "i 262\n",
            "epoch 55\n",
            " batch Loss train: 0.05003773421049118\n",
            "i 263\n",
            "epoch 55\n",
            " batch Loss train: 0.05311980098485947\n",
            "i 264\n",
            "epoch 55\n",
            " batch Loss train: 0.04125956445932388\n",
            "i 265\n",
            "epoch 55\n",
            " batch Loss train: 0.051035597920417786\n",
            "i 266\n",
            "epoch 55\n",
            " batch Loss train: 0.031024763360619545\n",
            "i 267\n",
            "epoch 55\n",
            " batch Loss train: 0.05096910521388054\n",
            "i 268\n",
            "epoch 55\n",
            " batch Loss train: 0.05577421188354492\n",
            "i 269\n",
            "epoch 55\n",
            " batch Loss train: 0.06420730799436569\n",
            "i 270\n",
            "epoch 55\n",
            " batch Loss train: 0.04365396872162819\n",
            "i 271\n",
            "epoch 55\n",
            " batch Loss train: 0.050694987177848816\n",
            "i 272\n",
            "epoch 55\n",
            " batch Loss train: 0.042531102895736694\n",
            "i 273\n",
            "epoch 55\n",
            " batch Loss train: 0.03362084925174713\n",
            "i 274\n",
            "epoch 55\n",
            " batch Loss train: 0.045218419283628464\n",
            "i 275\n",
            "epoch 55\n",
            " batch Loss train: 0.05038842186331749\n",
            "i 276\n",
            "epoch 55\n",
            " batch Loss train: 0.07740597426891327\n",
            "i 277\n",
            "epoch 55\n",
            " batch Loss train: 0.05797025188803673\n",
            "i 278\n",
            "epoch 55\n",
            " batch Loss train: 0.06776410341262817\n",
            "i 279\n",
            "epoch 55\n",
            " batch Loss train: 0.03906599059700966\n",
            "i 280\n",
            "epoch 55\n",
            " batch Loss train: 0.05226380005478859\n",
            "i 281\n",
            "epoch 55\n",
            " batch Loss train: 0.0431024394929409\n",
            "i 282\n",
            "epoch 55\n",
            " batch Loss train: 0.040212154388427734\n",
            "i 283\n",
            "epoch 55\n",
            " batch Loss train: 0.06133616343140602\n",
            "i 284\n",
            "epoch 55\n",
            " batch Loss train: 0.04144750162959099\n",
            "i 285\n",
            "epoch 55\n",
            " batch Loss train: 0.03210072219371796\n",
            "i 286\n",
            "epoch 55\n",
            " batch Loss train: 0.04742388427257538\n",
            "i 287\n",
            "epoch 55\n",
            " batch Loss train: 0.05859203264117241\n",
            "i 288\n",
            "epoch 55\n",
            " batch Loss train: 0.0508006326854229\n",
            "i 289\n",
            "epoch 55\n",
            " batch Loss train: 0.05599254369735718\n",
            "i 290\n",
            "epoch 55\n",
            " batch Loss train: 0.04880857095122337\n",
            "i 291\n",
            "epoch 55\n",
            " batch Loss train: 0.042993202805519104\n",
            "i 292\n",
            "epoch 55\n",
            " batch Loss train: 0.041200775653123856\n",
            "i 293\n",
            "epoch 55\n",
            " batch Loss train: 0.044609107077121735\n",
            "i 294\n",
            "epoch 55\n",
            " batch Loss train: 0.05410732701420784\n",
            "i 295\n",
            "epoch 55\n",
            " batch Loss train: 0.0486052967607975\n",
            "i 296\n",
            "epoch 55\n",
            " batch Loss train: 0.05242200568318367\n",
            "i 297\n",
            "epoch 55\n",
            " batch Loss train: 0.0363127700984478\n",
            "i 298\n",
            "epoch 55\n",
            " batch Loss train: 0.05769575387239456\n",
            "i 299\n",
            "epoch 55\n",
            " batch Loss train: 0.0443570576608181\n",
            "i 300\n",
            "epoch 55\n",
            " batch Loss train: 0.04028921574354172\n",
            "i 301\n",
            "epoch 55\n",
            " batch Loss train: 0.03925246000289917\n",
            "i 302\n",
            "epoch 55\n",
            " batch Loss train: 0.03981352224946022\n",
            "i 303\n",
            "epoch 55\n",
            " batch Loss train: 0.05816948413848877\n",
            "i 304\n",
            "epoch 55\n",
            " batch Loss train: 0.05007028579711914\n",
            "i 305\n",
            "epoch 55\n",
            " batch Loss train: 0.03586554899811745\n",
            "i 306\n",
            "epoch 55\n",
            " batch Loss train: 0.054389458149671555\n",
            "i 307\n",
            "epoch 55\n",
            " batch Loss train: 0.045436762273311615\n",
            "i 308\n",
            "epoch 55\n",
            " batch Loss train: 0.05030318722128868\n",
            "i 309\n",
            "epoch 55\n",
            " batch Loss train: 0.04644780233502388\n",
            "i 310\n",
            "epoch 55\n",
            " batch Loss train: 0.04006931558251381\n",
            "i 311\n",
            "epoch 55\n",
            " batch Loss train: 0.046716753393411636\n",
            "i 312\n",
            "epoch 55\n",
            " batch Loss train: 0.046484075486660004\n",
            "i 313\n",
            "epoch 55\n",
            " batch Loss train: 0.0469965860247612\n",
            "i 314\n",
            "epoch 55\n",
            " batch Loss train: 0.04730214923620224\n",
            "i 315\n",
            "epoch 55\n",
            " batch Loss train: 0.04429429769515991\n",
            "i 316\n",
            "epoch 55\n",
            " batch Loss train: 0.0371348112821579\n",
            "i 317\n",
            "epoch 55\n",
            " batch Loss train: 0.040306948125362396\n",
            "i 318\n",
            "epoch 55\n",
            " batch Loss train: 0.04781371355056763\n",
            "i 319\n",
            "epoch 55\n",
            " batch Loss train: 0.056991200894117355\n",
            "i 320\n",
            "epoch 55\n",
            " batch Loss train: 0.03422277048230171\n",
            "i 321\n",
            "epoch 55\n",
            " batch Loss train: 0.04113391041755676\n",
            "i 322\n",
            "epoch 55\n",
            " batch Loss train: 0.03990146145224571\n",
            "i 323\n",
            "epoch 55\n",
            " batch Loss train: 0.0357334241271019\n",
            "i 324\n",
            "epoch 55\n",
            " batch Loss train: 0.048041313886642456\n",
            "i 325\n",
            "epoch 55\n",
            " batch Loss train: 0.05789373442530632\n",
            "i 326\n",
            "epoch 55\n",
            " batch Loss train: 0.06056247651576996\n",
            "i 327\n",
            "epoch 55\n",
            " batch Loss train: 0.0668671503663063\n",
            "i 328\n",
            "epoch 55\n",
            " batch Loss train: 0.046621792018413544\n",
            "i 329\n",
            "epoch 55\n",
            " batch Loss train: 0.05430096760392189\n",
            "i 330\n",
            "epoch 55\n",
            " batch Loss train: 0.044910188764333725\n",
            "i 331\n",
            "epoch 55\n",
            " batch Loss train: 0.07280909270048141\n",
            "i 332\n",
            "epoch 55\n",
            " batch Loss train: 0.04629545658826828\n",
            "i 333\n",
            "epoch 55\n",
            " batch Loss train: 0.055165693163871765\n",
            "i 334\n",
            "epoch 55\n",
            " batch Loss train: 0.0738239660859108\n",
            "i 335\n",
            "epoch 55\n",
            " batch Loss train: 0.05717812851071358\n",
            "i 336\n",
            "epoch 55\n",
            " batch Loss train: 0.05392778664827347\n",
            "i 337\n",
            "epoch 55\n",
            " batch Loss train: 0.03944830223917961\n",
            "i 338\n",
            "epoch 55\n",
            " batch Loss train: 0.045053038746118546\n",
            "i 339\n",
            "epoch 55\n",
            " batch Loss train: 0.05221429094672203\n",
            "i 340\n",
            "epoch 55\n",
            " batch Loss train: 0.03303471580147743\n",
            "i 341\n",
            "epoch 55\n",
            " batch Loss train: 0.04246353358030319\n",
            "i 342\n",
            "epoch 55\n",
            " batch Loss train: 0.042031124234199524\n",
            "i 343\n",
            "epoch 55\n",
            " batch Loss train: 0.043924007564783096\n",
            "i 344\n",
            "epoch 55\n",
            " batch Loss train: 0.06423918902873993\n",
            "i 345\n",
            "epoch 55\n",
            " batch Loss train: 0.0615958645939827\n",
            "i 346\n",
            "epoch 55\n",
            " batch Loss train: 0.042939189821481705\n",
            "i 347\n",
            "epoch 55\n",
            " batch Loss train: 0.05348087474703789\n",
            "i 348\n",
            "epoch 55\n",
            " batch Loss train: 0.046405356377363205\n",
            "i 349\n",
            "epoch 55\n",
            " batch Loss train: 0.045417677611112595\n",
            "i 350\n",
            "epoch 55\n",
            " batch Loss train: 0.06995522230863571\n",
            "i 351\n",
            "epoch 55\n",
            " batch Loss train: 0.05763044208288193\n",
            "i 352\n",
            "epoch 55\n",
            " batch Loss train: 0.05203840509057045\n",
            "i 353\n",
            "epoch 55\n",
            " batch Loss train: 0.03647632524371147\n",
            "i 354\n",
            "epoch 55\n",
            " batch Loss train: 0.05522898957133293\n",
            "i 355\n",
            "epoch 55\n",
            " batch Loss train: 0.03130462020635605\n",
            "i 356\n",
            "epoch 55\n",
            " batch Loss train: 0.056070953607559204\n",
            "i 357\n",
            "epoch 55\n",
            " batch Loss train: 0.04701925814151764\n",
            "i 358\n",
            "epoch 55\n",
            " batch Loss train: 0.03543540835380554\n",
            "i 359\n",
            "epoch 55\n",
            " batch Loss train: 0.0522492341697216\n",
            "i 360\n",
            "epoch 55\n",
            " batch Loss train: 0.054525624960660934\n",
            "i 361\n",
            "epoch 55\n",
            " batch Loss train: 0.05452108010649681\n",
            "i 362\n",
            "epoch 55\n",
            " batch Loss train: 0.049399275332689285\n",
            "i 363\n",
            "epoch 55\n",
            " batch Loss train: 0.06248759850859642\n",
            "i 364\n",
            "epoch 55\n",
            " batch Loss train: 0.04049350693821907\n",
            "i 365\n",
            "epoch 55\n",
            " batch Loss train: 0.048793841153383255\n",
            "i 366\n",
            "epoch 55\n",
            " batch Loss train: 0.046919647604227066\n",
            "i 367\n",
            "epoch 55\n",
            " batch Loss train: 0.04818478226661682\n",
            "i 368\n",
            "epoch 55\n",
            " batch Loss train: 0.042691171169281006\n",
            "i 369\n",
            "epoch 55\n",
            " batch Loss train: 0.06591776013374329\n",
            "i 370\n",
            "epoch 55\n",
            " batch Loss train: 0.057358454912900925\n",
            "i 371\n",
            "epoch 55\n",
            " batch Loss train: 0.03535991534590721\n",
            "i 372\n",
            "epoch 55\n",
            " batch Loss train: 0.03701622784137726\n",
            "i 373\n",
            "epoch 55\n",
            " batch Loss train: 0.05178003013134003\n",
            "i 374\n",
            "epoch 55\n",
            " batch Loss train: 0.061084847897291183\n",
            "i 375\n",
            "epoch 55\n",
            " batch Loss train: 0.033131543546915054\n",
            "i 376\n",
            "epoch 55\n",
            " batch Loss train: 0.051331114023923874\n",
            "i 377\n",
            "epoch 55\n",
            " batch Loss train: 0.04236651957035065\n",
            "i 378\n",
            "epoch 55\n",
            " batch Loss train: 0.05183887109160423\n",
            "i 379\n",
            "epoch 55\n",
            " batch Loss train: 0.06674115359783173\n",
            "i 380\n",
            "epoch 55\n",
            " batch Loss train: 0.042062219232320786\n",
            "i 381\n",
            "epoch 55\n",
            " batch Loss train: 0.06873025000095367\n",
            "i 382\n",
            "epoch 55\n",
            " batch Loss train: 0.06027735397219658\n",
            "i 383\n",
            "epoch 55\n",
            " batch Loss train: 0.04687105119228363\n",
            "i 384\n",
            "epoch 55\n",
            " batch Loss train: 0.07319728285074234\n",
            "i 385\n",
            "epoch 55\n",
            " batch Loss train: 0.046556636691093445\n",
            "i 386\n",
            "epoch 55\n",
            " batch Loss train: 0.05896909907460213\n",
            "i 387\n",
            "epoch 55\n",
            " batch Loss train: 0.05861007422208786\n",
            "i 388\n",
            "epoch 55\n",
            " batch Loss train: 0.059920359402894974\n",
            "i 389\n",
            "epoch 55\n",
            " batch Loss train: 0.04728928953409195\n",
            "i 390\n",
            "epoch 55\n",
            " batch Loss train: 0.0481795072555542\n",
            "i 391\n",
            "epoch 55\n",
            " batch Loss train: 0.07093841582536697\n",
            "i 392\n",
            "epoch 55\n",
            " batch Loss train: 0.04163827374577522\n",
            "i 393\n",
            "epoch 55\n",
            " batch Loss train: 0.054943762719631195\n",
            "i 394\n",
            "epoch 55\n",
            " batch Loss train: 0.08009134232997894\n",
            "i 395\n",
            "epoch 55\n",
            " batch Loss train: 0.041587840765714645\n",
            "i 396\n",
            "epoch 55\n",
            " batch Loss train: 0.039705149829387665\n",
            "i 397\n",
            "epoch 55\n",
            " batch Loss train: 0.042575474828481674\n",
            "i 398\n",
            "epoch 55\n",
            " batch Loss train: 0.04805886000394821\n",
            "i 399\n",
            "epoch 55\n",
            " batch Loss train: 0.05245520547032356\n",
            "i 400\n",
            "epoch 55\n",
            " batch Loss train: 0.057768791913986206\n",
            "i 401\n",
            "epoch 55\n",
            " batch Loss train: 0.05610494688153267\n",
            "i 402\n",
            "epoch 55\n",
            " batch Loss train: 0.0459543913602829\n",
            "i 403\n",
            "epoch 55\n",
            " batch Loss train: 0.048572562634944916\n",
            "i 404\n",
            "epoch 55\n",
            " batch Loss train: 0.03265294060111046\n",
            "i 405\n",
            "epoch 55\n",
            " batch Loss train: 0.06886574625968933\n",
            "i 406\n",
            "epoch 55\n",
            " batch Loss train: 0.055035121738910675\n",
            "i 407\n",
            "epoch 55\n",
            " batch Loss train: 0.06117331236600876\n",
            "i 408\n",
            "epoch 55\n",
            " batch Loss train: 0.07084876298904419\n",
            "i 409\n",
            "epoch 55\n",
            " batch Loss train: 0.06781351566314697\n",
            "i 410\n",
            "epoch 55\n",
            " batch Loss train: 0.04297671467065811\n",
            "i 411\n",
            "epoch 55\n",
            " batch Loss train: 0.04122992604970932\n",
            "i 412\n",
            "epoch 55\n",
            " batch Loss train: 0.06904860585927963\n",
            "i 413\n",
            "epoch 55\n",
            " batch Loss train: 0.04594799503684044\n",
            "i 414\n",
            "epoch 55\n",
            " batch Loss train: 0.061252057552337646\n",
            "i 415\n",
            "epoch 55\n",
            " batch Loss train: 0.05450212582945824\n",
            "i 416\n",
            "epoch 55\n",
            " batch Loss train: 0.05137612298130989\n",
            "i 417\n",
            "epoch 55\n",
            " batch Loss train: 0.06810764223337173\n",
            "i 418\n",
            "epoch 55\n",
            " batch Loss train: 0.06165245547890663\n",
            "i 419\n",
            "epoch 55\n",
            " batch Loss train: 0.08900653570890427\n",
            "i 420\n",
            "epoch 55\n",
            " batch Loss train: 0.055382974445819855\n",
            "i 421\n",
            "epoch 55\n",
            " batch Loss train: 0.04023945331573486\n",
            "i 422\n",
            "epoch 55\n",
            " batch Loss train: 0.05872660502791405\n",
            "i 423\n",
            "epoch 55\n",
            " batch Loss train: 0.05204881355166435\n",
            "i 424\n",
            "epoch 55\n",
            " batch Loss train: 0.04748983308672905\n",
            "i 425\n",
            "epoch 55\n",
            " batch Loss train: 0.04239249229431152\n",
            "i 426\n",
            "epoch 55\n",
            " batch Loss train: 0.046719491481781006\n",
            "i 427\n",
            "epoch 55\n",
            " batch Loss train: 0.05042126029729843\n",
            "i 428\n",
            "epoch 55\n",
            " batch Loss train: 0.046352747827768326\n",
            "i 429\n",
            "epoch 55\n",
            " batch Loss train: 0.058095596730709076\n",
            "i 430\n",
            "epoch 55\n",
            " batch Loss train: 0.060614876449108124\n",
            "i 431\n",
            "epoch 55\n",
            " batch Loss train: 0.08737567067146301\n",
            "i 432\n",
            "epoch 55\n",
            " batch Loss train: 0.04805626720190048\n",
            "i 433\n",
            "epoch 55\n",
            " batch Loss train: 0.04894472286105156\n",
            "i 434\n",
            "epoch 55\n",
            " batch Loss train: 0.04876456782221794\n",
            "i 435\n",
            "epoch 55\n",
            " batch Loss train: 0.05804184079170227\n",
            "i 436\n",
            "epoch 55\n",
            " batch Loss train: 0.05093879997730255\n",
            "i 437\n",
            "epoch 55\n",
            " batch Loss train: 0.03609776869416237\n",
            "i 438\n",
            "epoch 55\n",
            " batch Loss train: 0.05522216111421585\n",
            "i 439\n",
            "epoch 55\n",
            " batch Loss train: 0.04430628567934036\n",
            "i 440\n",
            "epoch 55\n",
            " batch Loss train: 0.04188183695077896\n",
            "i 441\n",
            "epoch 55\n",
            " batch Loss train: 0.0945986956357956\n",
            "i 442\n",
            "epoch 55\n",
            " batch Loss train: 0.0638820081949234\n",
            "i 443\n",
            "epoch 55\n",
            " batch Loss train: 0.05335028097033501\n",
            "i 444\n",
            "epoch 55\n",
            " batch Loss train: 0.0728335976600647\n",
            "i 445\n",
            "epoch 55\n",
            " batch Loss train: 0.04987058788537979\n",
            "total epoch Loss train: tensor(0.0499, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRDD8_oY4IGw"
      },
      "source": [
        "# saving model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "DvsRdkHF4Hej",
        "outputId": "5476a8f3-b83f-4219-b5ed-3f16e56da525"
      },
      "source": [
        "torch.save(model.state_dict(),'hw4model_epoch12.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-f05352764bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hw4model_epoch12.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "1kUpoN4Qn2bP",
        "outputId": "8d8b3544-6006-4343-97df-9e31c07a10c4"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  print(main.model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5815c4acdb5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UbL69X2Eom1",
        "outputId": "949c03d3-5eac-42a4-b1db-c93dd45f2488"
      },
      "source": [
        "print(DEVICE)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53E-XMolwvis",
        "outputId": "0722c703-968c-48ee-c079-b0d9daa997b3"
      },
      "source": [
        "import numpy as np\n",
        "transcript_train = np.load('./data/train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "k=transcript_train[0][1].decode()\n",
        "print(transcript_train[0][0])\n",
        "sentence = transcript_train[0]\n",
        "for c in k:\n",
        "  print(LETTER_LIST.index(c))\n",
        "print(transcript_train[0].shape)\n",
        "sent=\"\"\n",
        "for word in sentence:\n",
        "          word = word.decode()\n",
        "          sent+=(word+ ' ')\n",
        "\n",
        "sent = sent[:-1]\n",
        "lst = []\n",
        "lst.append(LETTER_LIST.index('<sos>'))\n",
        "for c in sent:\n",
        "  lst.append(LETTER_LIST.index(c)) \n",
        "lst.append(LETTER_LIST.index('<eos>'))\n",
        "print(len(lst))\n",
        "print(len(sent))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'he'\n",
            "8\n",
            "1\n",
            "4\n",
            "(43,)\n",
            "227\n",
            "225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-Gqep-lTsPK",
        "outputId": "84e16628-65e5-4c25-99e2-22a8807b99b2"
      },
      "source": [
        "letter_to_index = []\n",
        "for sentence in transcript_train:\n",
        "    sent=\"\"\n",
        "    for word in sentence:\n",
        "      word = word.decode()\n",
        "      sent+=(word + ' ')\n",
        "    sent = sent[:-1]\n",
        "    lst = []\n",
        "    for c in sent:\n",
        "      lst.append(LETTER_LIST.index(c))\n",
        "    letter_to_index.append(lst)\n",
        "print(len(letter_to_index[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XN96mk-8q3a",
        "outputId": "59bfaad0-d89b-4b4f-dc1a-5d0dc9bf0b51"
      },
      "source": [
        "s = \" i mmm mc\"\n",
        "s = s[:-4]\n",
        "print(len(s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N67ZojtsRfFN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}